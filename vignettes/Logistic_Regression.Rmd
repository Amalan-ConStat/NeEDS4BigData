---
title: "Logistic Regression"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Logistic Regression}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style>
body {
text-align: justify}
</style>

```{r load packages and theme,message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(NeEDS4BigData)
library(ggplot2)
library(ggpubr)
library(kableExtra)
library(tidyr)
library(psych)
library(gam)

Theme_special<-function(){
  theme(legend.key.width=unit(1,"cm"),
        axis.text.x = element_text(color = "black",size=12, angle = 30, hjust=0.75),
        axis.text.y = element_text(color = "black",size=12),
        strip.text = element_text(colour = "black",size = 12,face="bold"),
        panel.grid.minor.x = element_blank(),
        axis.title= element_text(color = "black",face = "bold",size = 12),
        legend.text = element_text(color = "black", size=11),
        legend.position = "bottom",
        legend.margin=margin(0,0,0,0),
        legend.box.margin=margin(-1,-2,-1,-2)) 
}
```

# Understanding the skin segmentation data

The data contains $4$ columns and has $245,057$ observations, first column is the response variable and the rest are covariates, however we only use $25\%$ of the data for this explanation.
Aim of the logistic regression model is to classify if images are skin or not based on the a) Red, b) Green and c) Blue colour data.
Skin presence is denoted as one and skin absence is denoted as zero.
Each colour vector is scaled to have a mean of zero and a variance of one (initial range was between 0âˆ’255).
Given data is analysed under three different scenarios,

1. subsampling methods assuming the main effects model can describe the data.
2. model robust or averaging subsampling methods assuming that a set of models can equally describe the data.
3. subsampling method assuming the main effects model is potentially misspecified.

```{r Exploring data}
indexes<-sample(1:nrow(Skin_segmentation),nrow(Skin_segmentation)*0.25)
Original_Data<-cbind(Skin_segmentation[indexes,1],1,Skin_segmentation[indexes,-1])
colnames(Original_Data)<-c("Y",paste0("X",0:ncol(Original_Data[,-c(1,2)])))

# Scaling the covariate data
for (j in 3:5) {
  Original_Data[,j]<-scale(Original_Data[,j])
}
head(Original_Data) %>% 
  kable(format = "html",
        caption = "First five observations of the skin segmentation data.")

# Setting the subsample sizes
N<-nrow(Original_Data); M<-200; k<-c(6:15)*100; rep_k<-rep(k,each=M)
```

## First scenario 

First, we focus on assuming the main effects model is true and that it can describe the big data. 
Based on this model the methods random sampling, leverage sampling, local case control sampling, $A$- and $L$-optimality subsampling and $A$-optimality subsampling with response constraint are used to obtain subsamples and their respective are implemented on the big data.
The obtained subsamples and their respective model parameter estimates are over $M=100$ simulations across different subsample sizes $k=(600,\ldots,1500)$.
We set the initial subsample size of $r1=300$ for the methods that requires a random sample. 
From the final subsamples their respective model parameters are estimated. 
These estimated model parameters are compared with the estimated model parameters of the full big data through the mean squared error $MSE(\tilde{\beta}_k,\hat{\beta})=\sum_{i=1}^M (\tilde{\beta}_k - \hat{\beta})^2/M$. 
Here, $\tilde{\beta}_k$ is the estimated model parameters from the subsample of size $k$ and $\hat{\beta}$ is the estimated model parameters from the full big data. 
Below is the code for implementing the first scenario.

```{r First scenario of random subsampling}
# define colors, shapes, line types for the methods
Method_Names<-c("A-Optimality","L-Optimality","A-Optimality MC","Local case control sampling",
                "Shrinkage Leverage","Basic Leverage","Unweighted Leverage","Random sampling")
Method_Color<-c("green","green4","palegreen",
                "maroon","red","darkred","firebrick","black")
Method_Shape_Types<-c(rep(8,3),rep(4,4),16)
Method_Line_Types<-c(rep("twodash",3),rep("dotted",4),"solid")

# Random sampling
Final_Beta_RS<-matrix(nrow = length(k)*M,ncol = ncol(Original_Data[,-1])+1)
for (i in 1:length(rep_k)) {
  Temp_Data<-Original_Data[sample(1:N,rep_k[i]),]
  glm(Y~.-1,data=Temp_Data,family="binomial")->Results
  Final_Beta_RS[i,]<-c(rep_k[i],coefficients(Results))
  if(i==length(rep_k)){print("All simulations completed for random sampling")}
}
Final_Beta_RS<-cbind.data.frame("Random sampling",Final_Beta_RS)
colnames(Final_Beta_RS)<-c("Method","Subsample",paste0("Beta",0:3))

# Leverage sampling
## we set the shrinkage value of alpha=0.9
NeEDS4BigData::LeverageSampling(r=rep_k,Y=as.matrix(Original_Data[,1]),
                                X=as.matrix(Original_Data[,-1]),N=N,
                                alpha = 0.9,family = "logistic")->Results
Final_Beta_LS<-Results$Beta_Estimates
colnames(Final_Beta_LS)<-c("Method","Subsample",paste0("Beta",0:3))

# Local case control sampling
NeEDS4BigData::LCCsampling(r1=300,r2=rep_k,
                           Y=as.matrix(Original_Data[,1]),
                           X=as.matrix(Original_Data[,-1]),
                           N=N)->Results
Final_Beta_LCCS<-Results$Beta_Estimates
Final_Beta_LCCS$Method<-rep("Local case control sampling",nrow(Final_Beta_LCCS))
colnames(Final_Beta_LCCS)<-c("Method","Subsample",paste0("Beta",0:3))

# A- and L-optimality subsampling for GLM 
NeEDS4BigData::ALoptimalGLMSub(r1=300,r2=rep_k,
                               Y=as.matrix(Original_Data[,1]),
                               X=as.matrix(Original_Data[,-1]),
                               N=N,family = "logistic")->Results
Final_Beta_ALoptGLM<-Results$Beta_Estimates
colnames(Final_Beta_ALoptGLM)<-c("Method","Subsample",paste0("Beta",0:3))

# A-optimality subsampling for without response
NeEDS4BigData::AoptimalMCGLMSub(r1=300,r2=rep_k,
                                Y=as.matrix(Original_Data[,1]),
                                X=as.matrix(Original_Data[,-1]),
                                N=N,family="logistic")->Results
Final_Beta_AoptMCGLM<-Results$Beta_Estimates
Final_Beta_AoptMCGLM$Method<-rep("A-Optimality MC",nrow(Final_Beta_AoptMCGLM))
colnames(Final_Beta_AoptMCGLM)<-c("Method","Subsample",paste0("Beta",0:3))

# Summarising Results of Scenario one
Final_Beta<-rbind(Final_Beta_RS,Final_Beta_LS,Final_Beta_LCCS,
                  Final_Beta_ALoptGLM,Final_Beta_AoptMCGLM)

# Obtaining the model parameter estimates for the full big data model
glm(Y~.-1,data = Original_Data,family="binomial")->All_Results
coefficients(All_Results)->All_Beta
matrix(rep(All_Beta,by=nrow(Final_Beta)),nrow = nrow(Final_Beta),
       ncol = ncol(Final_Beta[,-c(1,2)]),byrow = TRUE)->All_Beta

remove(Final_Beta_RS,Final_Beta_LS,Final_Beta_LCCS,Final_Beta_ALoptGLM,
       Final_Beta_AoptMCGLM,Temp_Data,Results,All_Results)
```

The mean squared error is plotted below.

```{r First scenario model parameter estimates,fig.width=7,fig.height=12,fig.align='center',fig.cap="Mean squared error for a) all the subsampling methods and b) without unweighted leverage sampling."}
# Obtain the mean squared error for the model parameter estimates
MSE_Beta<-data.frame("Method"=Final_Beta$Method,
                     "Subsample"=Final_Beta$Subsample,
                     "MSE"=rowSums((All_Beta - Final_Beta[,-c(1,2)])^2)) 

MSE_Beta$Method<-factor(MSE_Beta$Method,levels = Method_Names,labels = Method_Names)

# Plot for the mean squared error with all methods
ggplot(MSE_Beta |> dplyr::group_by(Method,Subsample) |> 
         dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),
       aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+
  geom_point()+geom_line()+xlab("Subsample size")+ylab("MSE")+
  scale_color_manual(values = Method_Color)+
  scale_linetype_manual(values=Method_Line_Types)+
  scale_shape_manual(values = Method_Shape_Types)+
  theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1

# Plot for the mean squared error with all methods except unweighted leverage
ggplot(MSE_Beta[!(MSE_Beta$Method %in% c("Unweighted Leverage",
                                         "Shrinkage Leverage","Basic Leverage")),] |> 
         dplyr::group_by(Method,Subsample) |> 
         dplyr::summarise(MSE=mean(MSE),.groups = 'drop'),
       aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+
  geom_point()+geom_line()+xlab("Subsample size")+ylab("MSE")+
  scale_color_manual(values = Method_Color[-7])+
  scale_linetype_manual(values=Method_Line_Types[-7])+
  scale_shape_manual(values = Method_Shape_Types[-7])+
  theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2

ggarrange(p1,p2,nrow = 2,ncol = 1,labels = "auto")
```

## Second Scenario

For the second scenario $A$- and $L$-optimality model robust or average subsampling is compared against the $A$- and $L$-optimality subsampling method. 
Here five different models are considered 1) main effects model ($\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3$), 2-4) main effects model with the squared term of each covariate ($X^2_1 / X^2_2 / X^2_3$) and 5) main effects model with all the squared terms ($\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\beta_4X^2_1+\beta_5X^2_2+\beta_6X^2_3$). 
All $Q=5$ models have equal a priori probabilities (i.e $\alpha_q=1/5,q=1,\ldots,5$). 
As in scenario one for each model the mean squared error of the model parameters are calculated for the same number simulations and subsample sizes.
Below is the code of implementation for the second scenario.

```{r Second Scneario with model robust method with equal apriori,fig.width=12,fig.height=12,fig.align='center', fig.cap="Mean squared error for all the models in the order a to e for Model 1 to 5 across the subsampling methods under comparison."}
# Define the subsampling methods and their respective colors, shapes and line types
Method_Names<-c("A-Optimality","L-Optimality","A-Optimality MR","L-Optimality MR")
Method_Color<-c("red","darkred","palegreen","springgreen4")
Method_Shape_Types<-c(rep(8,2),rep(4,2))
Method_Line_Types<-c(rep("twodash",2),rep("dotted",2))

# Preparing the data for the model average method with squared terms
No_of_Variables<-ncol(Original_Data[,-c(1,2)])
Squared_Terms<-paste0("X",1:No_of_Variables,"^2")
term_no <- 2
All_Models <- list(c("X0",paste0("X",1:No_of_Variables)))

Original_Data_ModelRobust<-cbind(Original_Data,Original_Data[,-c(1,2)]^2)
colnames(Original_Data_ModelRobust)<-c("Y","X0",paste0("X",1:No_of_Variables),
                                       paste0("X",1:No_of_Variables,"^2"))

for (i in 1:No_of_Variables)
{
  x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))
  for(j in 1:length(x))
  {
    All_Models[[term_no]] <- c("X0",paste0("X",1:No_of_Variables),x[[j]])
    term_no <- term_no+1
  }
}
All_Models<-All_Models[-c(5:7)]
names(All_Models)<-paste0("Model_",1:length(All_Models))

# A- and L-optimality model robust subsampling for logistic regression
NeEDS4BigData::modelRobustLogSub(r1=300,r2=rep_k,
                                 Y=as.matrix(Original_Data_ModelRobust[,1]),
                                 X=as.matrix(Original_Data_ModelRobust[,-1]),
                                 N=N,Alpha=rep(1/length(All_Models),length(All_Models)),
                                 All_Combinations = All_Models,
                                 All_Covariates = colnames(Original_Data_ModelRobust)[-1])->Results
Final_Beta_modelRobust<-Results$Beta_Estimates

# Mean squared error and their respective plots for all five models
MSE_Beta_MR<-list(); plot_list_MR<-list()
for (i in 1:length(All_Models)) {
  glm(Y~.-1,data=Original_Data_ModelRobust[,c("Y",All_Models[[i]])],family="binomial")->All_Results
  All_Beta<-coefficients(All_Results)

  matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),
         nrow = nrow(Final_Beta_modelRobust[[i]]),
         ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta

  MSE_Beta_MR[[i]]<-data.frame("Method"=Final_Beta_modelRobust[[i]]$Method,
                               "Subsample"=Final_Beta_modelRobust[[i]]$r2,
                               "MSE"=rowSums((All_Beta - Final_Beta_modelRobust[[i]][,-c(1,2)])^2))

  ggplot(MSE_Beta_MR[[i]] |> dplyr::group_by(Method,Subsample) |> 
           dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),
         aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+
    geom_point()+geom_line()+xlab("Subsample size")+ylab("MSE")+
    scale_color_manual(values = Method_Color)+
    ggtitle(paste0("Model ",i))+
    scale_linetype_manual(values=Method_Line_Types)+
    scale_shape_manual(values = Method_Shape_Types)+
    theme_bw()+guides(colour = guide_legend(nrow = 2))+Theme_special()->plot_list_MR[[i]]
}

ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = "auto",
          common.legend = TRUE,legend = "bottom")
```

## Third Scenario

The final and third scenario is for comparison of the subsampling method under the assumption that the main effects model is potentially misspecified against the $A$- and $L$-optimality subsampling method. 
Under the subsampling method that accounts for potential model misspecification we take the scaling factor of $\alpha=10$. 
As in scenario one an two the number of simulations and the subsample sizes stay the same. 
We compare the mean squared error of the estimated model parameters, however as we assume the model is potentially misspecified the asymptotic approximation of the mean squared error from the predictions are calculated as well.
Below is the code for this implementation.

```{r Third Scneario with potential model misspecification methods,fig.width=6,fig.height=5,fig.align='center', warning=FALSE,fig.cap="Average loss for the potentially misspecified main effects model across the subsampling methods under comparison." }
# Define the subsampling methods and their respective colors, shapes and line types
Method_Names<-c("A-Optimality","L-Optimality","LmAMSE","LmAMSE Log Odds 10","LmAMSE Power 10")
Method_Color<-c("red","darkred","palegreen","green","springgreen4")
Method_Shape_Types<-c(rep(8,2),rep(4,3))
Method_Line_Types<-c(rep("twodash",2),rep("dotted",3))

# For the big data fit the main effects model and estimate the contamination
interaction_terms <- combn(colnames(Original_Data[,-1])[-1],2,FUN=function(x)paste(x,collapse="*"))
my_formula<-as.formula(paste("Y ~ ",
                             paste(paste0("s(X",1:ncol(Original_Data[,-c(1,2)]),")"),collapse="+"),
                             "+",paste(paste0("s(",interaction_terms,")"),collapse=" + ")))

glm(Y~.-1,data=Original_Data,family="binomial")->Results
beta.prop<-coefficients(Results)
Xbeta_Final<-as.vector(as.matrix(Original_Data[,-1])%*%beta.prop)
fit_GAM<-gam::gam(formula = my_formula,data=Original_Data,family="binomial")
Xbeta_GAM<-gam::predict.Gam(fit_GAM,newdata = Original_Data[,-1])
f_estimate<-Xbeta_GAM - Xbeta_Final

# A- and L-optimality and LmAMSE model misspecified subsampling for logistic regression 
NeEDS4BigData::modelMissLogSub(r1=300,r2=rep_k,
                               Y=as.matrix(Original_Data[,1]),
                               X=as.matrix(Original_Data[,-1]),
                               N=N,Alpha=10, Beta_Estimate_Full = beta.prop,
                               F_Estimate_Full = f_estimate)->Results

Final_Beta_modelMiss<-Results$Beta_Estimates
Final_Loss_modelMiss<-Results$Loss_Estimates

matrix(rep(beta.prop,by=nrow(Final_Beta_modelMiss)),nrow = nrow(Final_Beta_modelMiss),
       ncol = ncol(Final_Beta_modelMiss[,-c(1,2)]),byrow = TRUE)->All_Beta

# Plots for the mean squared error of the model parameter estimates 
# and the loss for the main effects model 
MSE_Beta_modelMiss<-data.frame("Method"=Final_Beta_modelMiss$Method,
                               "Subsample"=Final_Beta_modelMiss$r2,
                               "MSE"=rowSums((All_Beta - Final_Beta_modelMiss[,-c(1,2)])^2)) 

ggplot(MSE_Beta_modelMiss |> dplyr::group_by(Method,Subsample) |> 
         dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),
       aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+
  geom_point()+geom_line()+xlab("Subsample size")+ylab("MSE")+
  scale_color_manual(values = Method_Color)+
  scale_linetype_manual(values=Method_Line_Types)+
  scale_shape_manual(values = Method_Shape_Types)+
  theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1

ggplot(Final_Loss_modelMiss |> dplyr::group_by(r2,Method) |> 
         dplyr::summarise(meanLoss=mean(Loss), .groups = 'drop'),
       aes(x=factor(r2),y=meanLoss,color=Method,group=Method,linetype=Method,shape=Method)) +
  geom_point()+geom_line()+xlab("Subsample size")+ylab("Mean Loss")+
  scale_color_manual(values = Method_Color)+
  scale_linetype_manual(values=Method_Line_Types)+
  scale_shape_manual(values = Method_Shape_Types)+
  theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2

# ggarrange(p1,p2,nrow = 2,ncol = 1,labels = "auto",
#           legend = "bottom",common.legend = TRUE)
p2
```

### THANK YOU 
