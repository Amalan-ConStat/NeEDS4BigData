[{"path":"/articles/Benchmark_model_based.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Benchmarking model-based subsampling functions","text":"lm(), biglm(), glm() bigglm() functions benchmarked model-based subsampling functions. Benchmarking conducted across three regression problems using consistent setup throughout. big data sizes \\(N = \\{10^4,5 \\times 10^4,10^5,5 \\times 10^5,10^6,5 \\times 10^6\\}\\) five different Covariates sizes \\(\\{5,10,25,50,100\\}\\), functions replicated \\(50\\) times. using subsampling functions, initial subsample sizes \\(\\{100,100,250,500,1000\\}\\) selected based covariate sizes, final subsample size fixed \\(2500\\) ensure successful implementation. implementation conducted high-performance computing system; however, code linear regression-related functions shown . Furthermore, can extended logistic Poisson regression incorporating relevant subsampling functions model configurations.","code":"N_size<-c(1,5,10,50,100,500)*10000 N_size_labels<-c(\"10^4\",\"5 x 10^4\",\"10^5\",\"5 x 10^5\",\"10^6\",\"5 x 10^6\") CS_size<-c(5,10,25,50,100) # load the packages library(NeEDS4BigData) library(here) library(biglm)  # indexes for N and covariate sizes indexA <- as.numeric(Sys.getenv(\"indexA\")) indexB <- as.numeric(Sys.getenv(\"indexB\"))  # set N and covariate size, and assign replicates N_size<-c(1,5,10,50,100,500)*10000 Covariate_size<-c(5,10,25,50,100) Replicates<-50  # set the initial and final subsample sizes,  # with the distribution parameters to generate data r1<-c(1,1,2.5,5,10)*100; r2<-2500; Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5);  Family<-\"linear\"  # assign the indexes N_idx<-indexA Covariate_idx<-indexB No_Of_Var<-Covariate_size[Covariate_idx] N<-N_size[N_idx]  # generate the big data based on N and covariate size Beta<-c(-1,rep(0.5,Covariate_size[Covariate_idx])) Generated_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family) Full_Data<-Generated_Data$Complete_Data  cat(\"N size :\",N_size[N_idx],\" and Covariate size :\",Covariate_size[Covariate_idx],\"\\n\")  # formula for the linear regression model lm_formula<-as.formula(paste(\"Y ~\", paste(paste0(\"X\",0:ncol(Full_Data[,-c(1,2)])),                                            collapse = \" + \")))  # benchmarking the stats::lm() function lm<-sapply(1:Replicates,function(i){   start_T<-Sys.time()   suppressMessages(lm(Y~.-1,data=data.frame(Full_Data)))   return(difftime(Sys.time(),start_T,units = \"secs\")) })  cat(\"Benchmarked lm() function.\\n\")  # benchmarking the biglm::biglm() function biglm<-sapply(1:Replicates,function(i){   start_T<-Sys.time()   suppressMessages(biglm(lm_formula,data=data.frame(Full_Data)))   return(difftime(Sys.time(),start_T,units = \"secs\")) })  cat(\"Benchmarked biglm() function.\\n\")  # benchmarking the NeEDS4BigData::AoptimalGauLMSub() function AoptimalGauLMSub<-sapply(1:Replicates,function(i){   start_T<-Sys.time()   suppressMessages(AoptimalGauLMSub(r1[Covariate_idx],r2,                                     Y=as.matrix(Full_Data[,1]),                                     X=as.matrix(Full_Data[,-1]),                                     N=nrow(Full_Data)))   return(difftime(Sys.time(),start_T,units = \"secs\")) })  cat(\"Benchmarked AoptimalGauLMSub() function.\\n\")  # benchmarking the NeEDS4BigData::LeverageSampling() function LeverageSampling<-sapply(1:Replicates,function(i){   start_T<-Sys.time()   suppressMessages(LeverageSampling(r2,                                     Y=as.matrix(Full_Data[,1]),                                     X=as.matrix(Full_Data[,-1]),                                     N=nrow(Full_Data),                                     S_alpha=0.95,                                     family=Family))   return(difftime(Sys.time(),start_T,units = \"secs\")) })  cat(\"Benchmarked LeverageSampling() function.\\n\")  # benchmarking the NeEDS4BigData::ALoptimalGLMSub() function ALoptimalGLMSub<-sapply(1:Replicates,function(i){   start_T<-Sys.time()   suppressMessages(ALoptimalGLMSub(r1[Covariate_idx],r2,                                    Y=as.matrix(Full_Data[,1]),                                    X=as.matrix(Full_Data[,-1]),                                    N=nrow(Full_Data),                                    family=Family))   return(difftime(Sys.time(),start_T,units = \"secs\")) })  cat(\"Benchmarked ALoptimalGLMSub() function.\\n\")  # benchmarking the NeEDS4BigData::AoptimalMCGLMSub() function AoptimalMCGLMSub<-sapply(1:Replicates,function(i){   start_T<-Sys.time()   suppressMessages(AoptimalMCGLMSub(r1[Covariate_idx],r2,                                     Y=as.matrix(Full_Data[,1]),                                     X=as.matrix(Full_Data[,-1]),                                     N=nrow(Full_Data),                                     family=Family))   return(difftime(Sys.time(),start_T,units = \"secs\")) })  cat(\"Benchmarked AoptimalMCGLMSub() function.\\n\")  Plot_Data<-cbind.data.frame(\"N size\"=N_size[N_idx],                             \"Covariate size\"=Covariate_size[Covariate_idx],\"lm()\"=lm,                             \"biglm()\"=biglm,\"AoptimalGauLMSub()\"=AoptimalGauLMSub,                             \"LeverageSampling()\"=LeverageSampling,                             \"ALoptimalGLMSub()\"=ALoptimalGLMSub,                             \"AoptimalMCGLMSub()\"=AoptimalMCGLMSub)  save(Plot_Data,      file=here(\"Results\",paste0(\"Output_NS_\",N_idx,\"_CS_\",Covariate_idx,\".RData\")))"},{"path":"/articles/Benchmark_model_based.html","id":"linear-regression","dir":"Articles","previous_headings":"","what":"Linear Regression","title":"Benchmarking model-based subsampling functions","text":"linear regression, following functions NeEDS4BigData package: (1) AoptimalGauLMSub(), (2) LeverageSampling(), (3) ALoptimalGLMSub(), (4) AoptimalMCGLMSub() compared lm() biglm(). Average time functions, 5% 95% percentile intervals linear regression. general, subsampling functions perform faster size big data number covariates increase, except LeverageSampling(). Even solution linear regression available analytical form, subsampling functions perform well biglm().","code":"Methods_FCT<-c(\"lm()\",\"biglm()\",\"LeverageSampling()\",\"AoptimalMCGLMSub()\",                \"AoptimalGauLMSub()\",\"ALoptimalGLMSub()\") Method_Colors<-c(\"grey\",\"black\",\"#F76D5E\",\"#A50021\",\"#BBFFBB\",\"#50FF50\")  Final_Linear_Regression %>%   pivot_longer(cols = `lm()`:`AoptimalMCGLMSub()`,                names_to = \"Methods\",values_to = \"Time\") %>%   group_by(`N size`,`Covariate size`,Methods) %>%   summarise(Mean=mean(Time),min=quantile(Time,0.05),max=quantile(Time,0.95),             .groups = \"drop\") %>%   mutate(Methods=factor(Methods,levels = Methods_FCT,labels = Methods_FCT),          `N size`=factor(`N size`,levels = N_size,                          labels=paste0(\"N = \",N_size_labels))) %>% ggplot(.,aes(x=factor(`Covariate size`),y=Mean,color=Methods,group=Methods))+   geom_point(position = position_dodge(width = 0.5))+   geom_line(position = position_dodge(width = 0.5))+   geom_errorbar(aes(ymin=min,ymax=max),position = position_dodge(width = 0.5))+   facet_wrap(~factor(`N size`),scales = \"free\",nrow=2)+   xlab(\"Number of Covariates\")+ylab(\"Time in Seconds\")+   scale_color_manual(values = Method_Colors)+   theme_bw()+ggtitle(\"Linear Regression\")+Theme_special()"},{"path":"/articles/Benchmark_model_based.html","id":"logistic-regression","dir":"Articles","previous_headings":"","what":"Logistic Regression","title":"Benchmarking model-based subsampling functions","text":"logistic regression, following functions: (1) LCCSampling(), (2) LeverageSampling(), (3) AoptimalMCGLMSub(), (4) ALoptimalGLMSub() compared glm() bigglm(). Average time functions, 5% 95% percentile intervals logistic regression. seems little difference using glm() bigglm(), subsampling functions perform faster, performance gap increasing size big data number covariates grow.","code":"Methods_FCT<-c(\"glm()\",\"bigglm()\",\"LCCSampling()\",\"LeverageSampling()\",                \"AoptimalMCGLMSub()\",\"ALoptimalGLMSub()\") Method_Colors<-c(\"grey\",\"black\",\"#F76D5E\",\"#A50021\",\"#BBFFBB\",\"#50FF50\")  Final_Logistic_Regression %>%   pivot_longer(cols = `glm()`:`AoptimalMCGLMSub()`,                names_to = \"Methods\",values_to = \"Time\") %>%   group_by(`N size`,`Covariate size`,Methods) %>%   summarise(Mean=mean(Time),min=quantile(Time,0.05),max=quantile(Time,0.95),             .groups = \"drop\") %>%   mutate(Methods=factor(Methods,levels = Methods_FCT,labels = Methods_FCT),          `N size`=factor(`N size`,levels = N_size,                          labels=paste0(\"N = \",N_size_labels))) %>% ggplot(.,aes(x=factor(`Covariate size`),y=Mean,color=Methods,group=Methods))+   geom_point(position = position_dodge(width = 0.5))+   geom_line(position = position_dodge(width = 0.5))+   geom_errorbar(aes(ymin=min,ymax=max),position = position_dodge(width = 0.5))+   facet_wrap(~factor(`N size`),scales = \"free\",nrow=2)+   xlab(\"Number of Covariates\")+ylab(\"Time in Seconds\")+   scale_color_manual(values = Method_Colors)+   theme_bw()+Theme_special()+ggtitle(\"Logistic Regression\")"},{"path":"/articles/Benchmark_model_based.html","id":"poisson-regression","dir":"Articles","previous_headings":"","what":"Poisson Regression","title":"Benchmarking model-based subsampling functions","text":"Poisson regression, following functions: (1) LeverageSampling(), (2) AoptimalMCGLMSub(), (3) ALoptimalGLMSub() compared glm() bigglm(). Average time functions, 5% 95% percentile intervals Poisson regression. Similar logistic regression, subsampling functions perform faster glm() bigglm() functions. summary, subsampling functions available R package perform best high-dimensional data.","code":"Methods_FCT<-c(\"glm()\",\"bigglm()\",\"LeverageSampling()\",                \"AoptimalMCGLMSub()\",\"ALoptimalGLMSub()\") Method_Colors<-c(\"grey\",\"black\",\"#A50021\",\"#BBFFBB\",\"#50FF50\")  Final_Poisson_Regression %>%   pivot_longer(cols = `glm()`:`AoptimalMCGLMSub()`,                names_to = \"Methods\",values_to = \"Time\") %>%   group_by(`N size`,`Covariate size`,Methods) %>%   summarise(Mean=mean(Time),min=quantile(Time,0.05),max=quantile(Time,0.95),             .groups = \"drop\") %>%   mutate(Methods=factor(Methods,levels = Methods_FCT,labels = Methods_FCT),          `N size`=factor(`N size`,levels = N_size,                          labels=paste0(\"N = \",N_size_labels))) %>% ggplot(.,aes(x=factor(`Covariate size`),y=Mean,color=Methods,group=Methods))+   geom_point(position = position_dodge(width = 0.5))+   geom_line(position = position_dodge(width = 0.5))+   geom_errorbar(aes(ymin=min,ymax=max),position = position_dodge(width = 0.5))+   facet_wrap(~factor(`N size`),scales = \"free\",nrow=2)+   xlab(\"Number of Covariates\")+ylab(\"Time in Seconds\")+   scale_color_manual(values = Method_Colors)+   theme_bw()+Theme_special()+ggtitle(\"Poisson Regression\")"},{"path":"/articles/Benchmark_model_misspecification.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Benchmarking potential model-misspecification through subsampling functions","text":"lm(), biglm(), glm(), bigglm() functions benchmarked model-misspecification subsampling functions. benchmarking conducted across three regression problems using consistent setup throughout. large data sizes \\(N = \\{10^4,5 \\times 10^4,10^5,5 \\times 10^5,10^6\\}\\), three different covariate sizes \\(\\{5,10,25\\}\\), two different misspecification types \\(\\{1,2\\}\\), functions replicated 50 times. using model-misspecification subsampling functions, initial subsample sizes \\(\\{100,200,500\\}\\) selected based covariate sizes, final subsample size fixed \\(2500\\). based size big data set different proportion sizes estimate AMSE, \\(\\{0.5,0.1,0.05,0.02,0.005\\}\\), sample size \\(5000\\). implementation conducted high-performance computing system; however, code linear regression-related functions shown . Furthermore, can extended logistic Poisson regression using relevant subsampling functions model configurations.","code":"N_size<-c(1,5,10,50,100)*10000  N_size_labels<-c(\"10^4\",\"5 x 10^4\",\"10^5\",\"5 x 10^5\",\"10^6\") CS_size<-c(5,10,25) MM_size<-c(1,2) N_and_CS_size <- c(t(outer(CS_size, N_size_labels,                             function(cs, n) paste0(\"N = \",n,                                                   \" and \\nNumber of Covariates \",cs)))) # load the packages library(NeEDS4BigData) library(here) library(biglm) library(Rfast)  # indexes for N, covariate sizes and misspecification types indexA <- as.numeric(Sys.getenv(\"indexA\")) indexB <- as.numeric(Sys.getenv(\"indexB\")) indexC <- as.numeric(Sys.getenv(\"indexC\"))  # set N, covariate size and assign replicates N_size<-c(1,5,10,50,100)*10000 Covariate_size<-c(5,10,25) Replicates<-50  # set the initial and final subsample sizes,  # with the proportion values for AMSE calculation r1<-c(1,2,5)*100; r2<-2500; Family<-\"linear\" proportion<-c(0.5,0.1,0.05,0.02,0.005)  # assign the indexes N_idx<-indexA; Covariate_idx<-indexB; Type_idx<-indexC N <- N_size[N_idx] No_Of_Var <- Covariate_size[Covariate_idx]  # generate the big data based on N, covariate size # and misspecification type Beta <- c(-1,rep(0.5,Covariate_size[Covariate_idx]),1)  if(Type_idx == 1){   X_1 <- replicate(No_Of_Var,stats::runif(n=N,min = -1,max = 1))   Temp<-Rfast::rowprods(X_1)   Misspecification <- rep(0,N)   X_Data <- cbind(X0=1,X_1)      Generated_Data<-GenModelMissGLMdata(N,X_Data,Misspecification,Beta,0.5,Family) }  if(Type_idx == 2){   X_1 <- replicate(No_Of_Var,stats::runif(n=N,min = -1,max = 1))   Temp<-Rfast::rowprods(X_1)   Misspecification <- (Temp-mean(Temp))/sqrt(mean(Temp^2)-mean(Temp)^2)   X_Data <- cbind(X0=1,X_1)      Generated_Data<-GenModelMissGLMdata(N,X_Data,Misspecification,Beta,0.5,Family) }  Full_Data<-Generated_Data$Complete_Data Full_Data<-Full_Data[,-ncol(Full_Data)]  cat(\"N size :\",N_size[N_idx],\" and Covariate size :\",Covariate_size[Covariate_idx],     \" and Misspecification Type :\",Type_idx,\"\\n\")  lm_formula<-as.formula(paste(\"Y ~\", paste(paste0(\"X\",0:ncol(Full_Data[,-c(1,2)])), collapse = \" + \")))  # benchmarking the stats::lm() function lm<-sapply(1:Replicates,function(i){   start_T<-Sys.time()   suppressMessages(lm(Y~.-1,data=data.frame(Full_Data)))   return(difftime(Sys.time(),start_T,units = \"secs\")) })  cat(\"Benchmarked lm() function.\\n\")  # benchmarking the biglm::biglm() function biglm<-sapply(1:Replicates,function(i){   start_T<-Sys.time()   suppressMessages(biglm(lm_formula,data=data.frame(Full_Data)))   return(difftime(Sys.time(),start_T,units = \"secs\")) })  cat(\"Benchmarked biglm() function.\\n\")  # benchmarking the NeEDS4BigData::modelMissLinSub() function modelMissLinSub<-sapply(1:Replicates,function(i){   start_T<-Sys.time()   suppressMessages(modelMissLinSub(r1[Covariate_idx],r2,                                    Y=as.matrix(Full_Data[,1]),                                    X=as.matrix(Full_Data[,-c(1,ncol(Full_Data))]),                                    N=nrow(Full_Data),                                    Alpha=10,                                    proportion = proportion[N_idx]))   return(difftime(Sys.time(),start_T,units = \"secs\")) })  cat(\"Benchmarked modelMissLinSub() function.\\n\")  Plot_Data<-cbind.data.frame(\"N size\"=N_size[N_idx],                             \"Covariate size\"=Covariate_size[Covariate_idx],                             \"Misspecification\"=Type_idx,\"lm()\"=lm,                             \"biglm()\"=biglm,                             \"modelMissLinSub()\"=modelMissLinSub)  save(Plot_Data,      file=here(\"Results\",                paste0(\"Output_NS_\",N_idx,\"_CS_\",Covariate_idx,\"Type\",Type_idx,\".RData\")))"},{"path":"/articles/Benchmark_model_misspecification.html","id":"linear-regression","dir":"Articles","previous_headings":"","what":"Linear regression","title":"Benchmarking potential model-misspecification through subsampling functions","text":"linear regression, modelMissLinSub() function NeEDS4BigData package compared lm() biglm(). Average time functions, 5% 95% percentile intervals model misspecified linear regression. general, subsampling functions perform slower size big data, number covariates, misspecification type differs. Calculating reduction loss value data points significant bottleneck.","code":"Methods_FCT<-c(\"lm()\",\"biglm()\",\"modelMissLinSub()\") Method_Colors<-c(\"grey\",\"black\",\"#50FF50\")  Final_Linear_Regression %>%   pivot_longer(cols = `lm()`:`modelMissLinSub()`,                names_to = \"Methods\",values_to = \"Time\") %>%   mutate(Methods=factor(Methods,levels = Methods_FCT,labels = Methods_FCT),          `N size`=factor(`N size`,levels = N_size,labels = N_size_labels),          `N and Covariate size`=paste0(\"N = \",`N size`,                                        \" and \\nNumber of Covariates \",                                        `Covariate size`)) %>%   mutate(`N and Covariate size`=factor(`N and Covariate size`,                                        levels = N_and_CS_size,                                        labels = N_and_CS_size)) %>%   select(`N and Covariate size`,Misspecification,Methods,Time) %>%   group_by(`N and Covariate size`,Misspecification,Methods) %>%   summarise(Mean=mean(Time),min=quantile(Time,0.05),max=quantile(Time,0.95)) %>%   ggplot(.,aes(x=factor(Misspecification),y=Mean,                color=Methods,group=Methods))+   geom_point(position = position_dodge(width = 0.5))+   geom_line(position = position_dodge(width = 0.5))+   geom_errorbar(aes(ymin=min,ymax=max),                 position = position_dodge(width = 0.5))+   facet_wrap(.~factor(`N and Covariate size`),              scales = \"free\",ncol=length(N_size))+   xlab(\"Misspecification Type\")+ylab(\"Time in Seconds\")+   scale_color_manual(values = Method_Colors)+   theme_bw()+Theme_special()+ggtitle(\"Linear Regression\") ## `summarise()` has grouped output by 'N and Covariate size', 'Misspecification'. ## You can override using the `.groups` argument."},{"path":"/articles/Benchmark_model_misspecification.html","id":"logistic-regression","dir":"Articles","previous_headings":"","what":"Logistic regression","title":"Benchmarking potential model-misspecification through subsampling functions","text":"logistic regression, modelMissLogSub() function compared glm() bigglm(). Average time functions, 5% 95% percentile intervals model misspecified logistic regression. seems significant difference using glm() bigglm(), model-misspecified subsampling function performing slower. performance gap increases size big data number covariates grow irrespective misspecification type. model misspecified subsampling linear regression bottleneck occurs calculating reduction loss data points big data.","code":"Methods_FCT<-c(\"glm()\",\"bigglm()\",\"modelMissLogSub()\") Method_Colors<-c(\"grey\",\"black\",\"#50FF50\")  Final_Logistic_Regression %>%   pivot_longer(cols = `glm()`:`modelMissLogSub()`,                names_to = \"Methods\",values_to = \"Time\") %>%   mutate(Methods=factor(Methods,levels = Methods_FCT,labels = Methods_FCT),          `N size`=factor(`N size`,levels = N_size,labels = N_size_labels),          `N and Covariate size`=paste0(\"N = \",`N size`,                                        \" and \\nNumber of Covariates \",                                        `Covariate size`)) %>%   mutate(`N and Covariate size`=factor(`N and Covariate size`,                                        levels = N_and_CS_size,                                        labels = N_and_CS_size)) %>%   select(`N and Covariate size`,Misspecification,Methods,Time) %>%   group_by(`N and Covariate size`,Misspecification,Methods) %>%   summarise(Mean=mean(Time),min=quantile(Time,0.05),             max=quantile(Time,0.95)) %>%   ggplot(.,aes(x=factor(Misspecification),y=Mean,color=Methods,                group=Methods))+   geom_point(position = position_dodge(width = 0.5))+   geom_line(position = position_dodge(width = 0.5))+   geom_errorbar(aes(ymin=min,ymax=max),                 position = position_dodge(width = 0.5))+   facet_wrap(.~factor(`N and Covariate size`),scales = \"free\",              ncol=length(N_size))+   xlab(\"Misspecification Type\")+ylab(\"Time in Seconds\")+   scale_color_manual(values = Method_Colors)+   theme_bw()+Theme_special()+ggtitle(\"Logistic Regression\") ## `summarise()` has grouped output by 'N and Covariate size', 'Misspecification'. ## You can override using the `.groups` argument."},{"path":"/articles/Benchmark_model_misspecification.html","id":"poisson-regression","dir":"Articles","previous_headings":"","what":"Poisson Regression","title":"Benchmarking potential model-misspecification through subsampling functions","text":"Poisson regression, function modelMissPoiSub() compared glm() bigglm(). Average time functions, 5% 95% percentile intervals model misspecified Poisson regression. Similar logistic regression, model misspecified subsampling function performs slower glm() bigglm() functions. summary, model misspecified subsampling functions available R package limited computation time reduction loss calculated data points. potential solution obtain calculation proportion big data continue subsampling.","code":"Methods_FCT<-c(\"glm()\",\"bigglm()\",\"modelMissPoiSub()\") Method_Colors<-c(\"grey\",\"black\",\"#50FF50\")  Final_Poisson_Regression %>%   pivot_longer(cols = `glm()`:`modelMissPoiSub()`,                names_to = \"Methods\",values_to = \"Time\") %>%   mutate(Methods=factor(Methods,levels = Methods_FCT,labels = Methods_FCT),          `N size`=factor(`N size`,levels = N_size,labels = N_size_labels),          `N and Covariate size`=paste0(\"N = \",`N size`,                                        \" and \\nNumber of Covariates \",                                        `Covariate size`)) %>%   mutate(`N and Covariate size`=factor(`N and Covariate size`,                                        levels = N_and_CS_size,                                        labels = N_and_CS_size)) %>%   select(`N and Covariate size`,Misspecification,Methods,Time) %>%   group_by(`N and Covariate size`,Misspecification,Methods) %>%   summarise(Mean=mean(Time),min=quantile(Time,0.05),             max=quantile(Time,0.95)) %>%   ggplot(.,aes(x=factor(Misspecification),y=Mean,color=Methods,                group=Methods))+   geom_point(position = position_dodge(width = 0.5))+   geom_line(position = position_dodge(width = 0.5))+   geom_errorbar(aes(ymin=min,ymax=max),                 position = position_dodge(width = 0.5))+   facet_wrap(.~factor(`N and Covariate size`),scales = \"free\",              ncol=length(N_size))+   xlab(\"Misspecification Type\")+ylab(\"Time in Seconds\")+   scale_color_manual(values = Method_Colors)+   theme_bw()+Theme_special()+ggtitle(\"Poisson Regression\") ## `summarise()` has grouped output by 'N and Covariate size', 'Misspecification'. ## You can override using the `.groups` argument."},{"path":"/articles/Benchmark_model_robust.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Benchmarking Model-robust Subsampling Functions","text":"lm(), biglm(), glm(), bigglm() functions benchmarked model-robust subsampling functions. benchmarking conducted across three regression problems using consistent setup throughout. large data sizes \\(N = \\{10^4,5 \\times 10^4,10^5,5 \\times 10^5,10^6,5 \\times 10^6\\}\\), three different covariate sizes \\(\\{10,25,50\\}\\), three different model sizes \\(\\{3,5,10\\}\\), functions replicated 50 times. using model-robust subsampling functions, initial subsample sizes \\(\\{100,250,500\\}\\) selected based covariate sizes, final subsample size fixed \\(2500\\). , models included model set selected based AIC values, squared term derived main effects. implementation conducted high-performance computing system; however, code linear regression-related functions shown . Furthermore, can extended logistic Poisson regression using relevant subsampling functions model configurations.","code":"N_size<-c(1,5,10,50,100,500)*10000 N_size_labels<-c(\"10^4\",\"5 x 10^4\",\"10^5\",\"5 x 10^5\",\"10^6\",\"5 x 10^6\") CS_size<-c(10,25,50) NOM_size<-c(3,5,10) N_and_CS_size <- c(t(outer(CS_size, N_size_labels,                             function(cs, n) paste0(\"N = \",n,                                                   \" and \\nNumber of Covariates \",cs)))) # load the packages library(NeEDS4BigData) library(here) library(biglm)  # indexes for N, covariate sizes and number of models indexA <- as.numeric(Sys.getenv(\"indexA\")) indexB <- as.numeric(Sys.getenv(\"indexB\")) indexC <- as.numeric(Sys.getenv(\"indexC\"))  # set N, covariate size, number of models and assign replicates N_size<-c(1,5,10,50,100,500)*10000 Covariate_size<-c(10,25,50) No_of_Models<-c(3,5,10) Replicates<-50  # set the initial and final subsample sizes,  # with the distribution parameters to generate data r1<-c(1,2.5,5)*100; r2<-2500; Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5);  Family<-\"linear\"  # assign the indexes N_idx<-indexA; Covariate_idx<-indexB; ModelSet_idx<-indexC No_Of_Var<-Covariate_size[Covariate_idx] N<-N_size[N_idx] Model_Set<-No_of_Models[ModelSet_idx]  # generate the big data based on N and covariate size Beta<-c(-1,rep(0.5,Covariate_size[Covariate_idx])) Generated_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family) Full_Data<-Generated_Data$Complete_Data  # Find the models in the model set for model-robust subsampling # based on AIC values Full_Data<-cbind(Full_Data,Full_Data[,-c(1,2)]^2) colnames(Full_Data)<-c(\"Y\",paste0(\"X\",0:No_Of_Var),paste0(\"X\",1:No_Of_Var,\"^2\")) Temp_Data<-Full_Data[sample(1:N,1000),]  AIC_Values<-NULL for (i in 1:No_Of_Var) {   temp_data<-as.data.frame(Temp_Data[,c(\"Y\",paste0(\"X\",0:No_Of_Var),paste0(\"X\",i,\"^2\"))])   model <- lm(Y~.-1, data = temp_data)    AIC_Values[i]<-AIC(model)  }  # Covariates in model set Best_Indices <- order(AIC_Values)[1:Model_Set]  Model_Apriori<-rep(1/Model_Set,Model_Set) All_Combinations<-lm_formula<-list();  for (NOM_idx in 1:Model_Set) {   All_Combinations[[NOM_idx]]<-c(paste0(\"X\",0:No_Of_Var),                                  paste0(\"X\",Best_Indices[NOM_idx],\"^2\"))   lm_formula[[NOM_idx]]<-as.formula(paste(\"Y ~\",                                            paste(c(paste0(\"X\",0:No_Of_Var),                                                   paste0(\"X\",Best_Indices[NOM_idx],\"^2\")),                                                  collapse = \" + \"))) }  cat(\"N size :\",N_size[N_idx],\" and Covariate size :\",Covariate_size[Covariate_idx],     \" with the model set \",No_of_Models[ModelSet_idx],\"\\n\")  # benchmarking the stats::lm() function lm<-sapply(1:Replicates,function(i){   start_T<-Sys.time()   for(NOM_idx in 1:length(lm_formula)){     suppressMessages(lm(lm_formula[[NOM_idx]],data=data.frame(Full_Data)))   }   return(difftime(Sys.time(),start_T,units = \"secs\")) })  cat(\"Benchmarked lm() function.\\n\")  # benchmarking the biglm::biglm() function biglm<-sapply(1:Replicates,function(i){   start_T<-Sys.time()   for (NOM_idx in 1:length(lm_formula)) {     suppressMessages(biglm(lm_formula[[NOM_idx]],data=data.frame(Full_Data)))   }   return(difftime(Sys.time(),start_T,units = \"secs\")) })  cat(\"Benchmarked biglm() function.\\n\")  # benchmarking the NeEDS4BigData::modelRobustLinSub() function modelRobustLinSub<-sapply(1:Replicates,function(i){   start_T<-Sys.time()   suppressMessages(modelRobustLinSub(r1[Covariate_idx],r2,                                      Y=as.matrix(Full_Data[,1]),                                      X=as.matrix(Full_Data[,-1]),                                      N=nrow(Full_Data),                                      Apriori_probs=Model_Apriori,                                      All_Combinations = All_Combinations,                                      All_Covariates = colnames(Full_Data[,-1])))   return(difftime(Sys.time(),start_T,units = \"secs\")) })  cat(\"Benchmarked modelRobustLinSub() function.\")  Plot_Data<-cbind.data.frame(\"N size\"=N_size[N_idx],                             \"Covariate size\"=Covariate_size[Covariate_idx],                             \"No of Models\"= No_of_Models[ModelSet_idx],                             \"lm()\"=lm,\"biglm()\"=biglm,                             \"modelRobustLinSub()\"=modelRobustLinSub)  save(Plot_Data,      file=here(\"Results\",paste0(\"Output_NS_\",N_idx,\"_CS_\",Covariate_idx,                                 \"_MS_\",ModelSet_idx,\".RData\")))"},{"path":"/articles/Benchmark_model_robust.html","id":"linear-regression","dir":"Articles","previous_headings":"","what":"Linear regression","title":"Benchmarking Model-robust Subsampling Functions","text":"linear regression, modelRobustLinSub() function NeEDS4BigData package compared lm() biglm(). Average time functions, 5% 95% percentile intervals model robust linear regression. general, subsampling functions perform faster size big data, number covariates, number models increase. Even solution linear regression available analytical form, subsampling functions perform better well biglm().","code":"Methods_FCT<-c(\"lm()\",\"biglm()\",\"modelRobustLinSub()\") Method_Colors<-c(\"grey\",\"black\",\"#50FF50\")  Final_Linear_Regression %>%   pivot_longer(cols = `lm()`:`modelRobustLinSub()`,                names_to = \"Methods\",values_to = \"Time\") %>%   mutate(Methods=factor(Methods,levels = Methods_FCT,labels = Methods_FCT),          `N size`=factor(`N size`,levels = N_size,labels = N_size_labels),          `N and Covariate size`=paste0(\"N = \",`N size`,                                        \" and \\nNumber of Covariates \",                                        `Covariate size`)) %>%   mutate(`N and Covariate size`=factor(`N and Covariate size`,                                        levels = N_and_CS_size,                                        labels = N_and_CS_size)) %>%   select(`N and Covariate size`,`No of Models`,Methods,Time) %>%   group_by(`N and Covariate size`,`No of Models`,Methods) %>%   summarise(Mean=mean(Time),min=quantile(Time,0.05),max=quantile(Time,0.95),             .groups = \"drop\") %>%   ggplot(.,aes(x=factor(`No of Models`),y=Mean,color=Methods,group=Methods))+   geom_point(position = position_dodge(width = 0.5))+   geom_line(position = position_dodge(width = 0.5))+   geom_errorbar(aes(ymin=min,ymax=max),position = position_dodge(width = 0.5))+   facet_wrap(.~factor(`N and Covariate size`),scales = \"free\",ncol=length(N_size))+   xlab(\"Number of Models\")+ylab(\"Time in Seconds\")+   scale_color_manual(values = Method_Colors)+   theme_bw()+Theme_special()+ggtitle(\"Linear Regression\")"},{"path":"/articles/Benchmark_model_robust.html","id":"logistic-regression","dir":"Articles","previous_headings":"","what":"Logistic regression","title":"Benchmarking Model-robust Subsampling Functions","text":"logistic regression, modelRobustLogSub() function compared glm() bigglm(). Average time functions, 5% 95% percentile intervals model robust logistic regression. seems significant difference using glm() bigglm(), model-robust subsampling function performing faster. performance gap increases size big data, number covariates, number models grow.","code":"Methods_FCT<-c(\"glm()\",\"bigglm()\",\"modelRobustLogSub()\") Method_Colors<-c(\"grey\",\"black\",\"#50FF50\")  Final_Logistic_Regression %>%   pivot_longer(cols = `glm()`:`modelRobustLogSub()`,                names_to = \"Methods\",values_to = \"Time\") %>%   mutate(Methods=factor(Methods,levels = Methods_FCT,labels = Methods_FCT),          `N size`=factor(`N size`,levels = N_size,labels = N_size_labels),          `N and Covariate size`=paste0(\"N = \",`N size`,                                        \" and \\nNumber of Covariates \",                                        `Covariate size`)) %>%   mutate(`N and Covariate size`=factor(`N and Covariate size`,                                        levels = N_and_CS_size,                                        labels = N_and_CS_size)) %>%   select(`N and Covariate size`,`No of Models`,Methods,Time) %>%   group_by(`N and Covariate size`,`No of Models`,Methods) %>%   summarise(Mean=mean(Time),min=quantile(Time,0.05),max=quantile(Time,0.95),             .groups = \"drop\") %>%   ggplot(.,aes(x=factor(`No of Models`),y=Mean,color=Methods,group=Methods))+   geom_point(position = position_dodge(width = 0.5))+   geom_line(position = position_dodge(width = 0.5))+   geom_errorbar(aes(ymin=min,ymax=max),position = position_dodge(width = 0.5))+   facet_wrap(.~factor(`N and Covariate size`),scales = \"free\",ncol=length(N_size))+   xlab(\"Number of Models\")+ylab(\"Time in Seconds\")+   scale_color_manual(values = Method_Colors)+   theme_bw()+Theme_special()+ggtitle(\"Logistic Regression\")"},{"path":"/articles/Benchmark_model_robust.html","id":"poisson-regression","dir":"Articles","previous_headings":"","what":"Poisson Regression","title":"Benchmarking Model-robust Subsampling Functions","text":"Poisson regression, function modelRobustPoiSub() compared glm() bigglm(). Average time functions, 5% 95% percentile intervals model robust Poisson regression. Similar logistic regression, model-robust subsampling function performs faster glm() bigglm() functions. summary, model-robust subsampling functions available R package perform best high-dimensional data.","code":"Methods_FCT<-c(\"glm()\",\"bigglm()\",\"modelRobustPoiSub()\") Method_Colors<-c(\"grey\",\"black\",\"#50FF50\")  Final_Poisson_Regression %>%   pivot_longer(cols = `glm()`:`modelRobustPoiSub()`,                names_to = \"Methods\",values_to = \"Time\") %>%   mutate(Methods=factor(Methods,levels = Methods_FCT,labels = Methods_FCT),          `N size`=factor(`N size`,levels = N_size,labels = N_size_labels),          `N and Covariate size`=paste0(\"N = \",`N size`,                                        \" and \\nNumber of Covariates \",                                        `Covariate size`)) %>%   mutate(`N and Covariate size`=factor(`N and Covariate size`,                                        levels = N_and_CS_size,                                        labels = N_and_CS_size)) %>%   select(`N and Covariate size`,`No of Models`,Methods,Time) %>%   group_by(`N and Covariate size`,`No of Models`,Methods) %>%   summarise(Mean=mean(Time),min=quantile(Time,0.05),max=quantile(Time,0.95),             .groups = \"drop\") %>%   ggplot(.,aes(x=factor(`No of Models`),y=Mean,color=Methods,group=Methods))+   geom_point(position = position_dodge(width = 0.5))+   geom_line(position = position_dodge(width = 0.5))+   geom_errorbar(aes(ymin=min,ymax=max),position = position_dodge(width = 0.5))+   facet_wrap(.~factor(`N and Covariate size`),scales = \"free\",ncol=length(N_size))+   xlab(\"Number of Models\")+ylab(\"Time in Seconds\")+   scale_color_manual(values = Method_Colors)+   theme_bw()+Theme_special()+ggtitle(\"Poisson Regression\")"},{"path":"/articles/Introduction.html","id":"big-data-analysis","dir":"Articles","previous_headings":"","what":"Big data analysis","title":"Introduction","text":"Big data presents opportunities analysts uncover new knowledge gain new insights real-world problems. However, massive scale complexity presents computational statistical challenges. include scalability issues, storage constraints, noise accumulation, spurious correlations, incidental endogeneity measurement errors. Figure 1, Chen, Mao, Liu (2014) review size big data different sectors business. Addressing challenges demands innovative approaches computation statistics. Traditional methods, effective small moderate sample sizes, often falter confronted massive datasets. Thus, pressing need innovative statistical methodologies computational tools tailored unique demands big data analysis.","code":""},{"path":"/articles/Introduction.html","id":"computational-solutions-for-big-data-analysis","dir":"Articles","previous_headings":"","what":"Computational solutions for big data analysis","title":"Introduction","text":"Computer engineers often seek powerful computing facilities reduce computing time, leading rapid development supercomputers past decade. supercomputers boast speeds storage capacities hundreds even thousands times greater general-purpose PCs. However, significant energy consumption limited accessibility remain major drawbacks. cloud computing offers partial solution providing accessible computing resources, faces challenges related data transfer inefficiency, privacy security concerns. Graphic Processing Units (GPUs) emerged another computational facility, offering powerful parallel computing capabilities. However, recent comparisons shown even high-end GPUs can outperformed general-purpose multi-core processors, primarily due data transfer inefficiencies. summary, neither supercomputers, cloud computing, GPUs efficiently solved big data problem. Instead, growing need efficient statistical solutions can make big data manageable general-purpose PCs.","code":""},{"path":"/articles/Introduction.html","id":"statistical-solutions-for-big-data-analysis","dir":"Articles","previous_headings":"","what":"Statistical solutions for big data analysis","title":"Introduction","text":"realm addressing challenges posed big data, statistical solutions relatively novel compared engineering solutions, new methodologies continually development. Currently available methods can broadly categorized three groups: Sampling: involves selecting representative subset data analysis instead analysing entire dataset. approach can significantly reduce computational requirements still providing valuable insights underlying population. Divide conquer: approach involves breaking large problem smaller, manageable sub problems. sub problem independently analysed, often parallel, combining results obtain final output. Online updating streamed data: statistical inference updated new data arrive sequentially. recent years, growing preference sampling divide recombine methods addressing range regression problems. Meanwhile, online updating primarily utilized streaming data. Furthermore, large dataset unnecessary confidently answer specific question, sampling often favoured, allows analysis using standard methods.","code":""},{"path":"/articles/Introduction.html","id":"sampling-algorithms-for-big-data","dir":"Articles","previous_headings":"","what":"Sampling algorithms for big data","title":"Introduction","text":"literature presents two strategies resolve primary challenge acquire informative subset efficiently addresses specific analytical questions yield results consistent analysing large data set. : Sample randomly large dataset using subsampling probabilities determined via assumed statistical model objective (e.g., prediction /parameter estimation) (Wang, Zhu, Ma 2018; Yao Wang 2019; Ai, Wang, et al. 2021; Ai, Yu, et al. 2021; Lee, Schifano, Wang 2021, 2022; Zhang, Ning, Ruppert 2021) Select samples based experimental design (Drovandi et al. 2017; Wang, Yang, Stufken 2019; Cheng, Wang, Yang 2020; Hou-Liu Browne 2023; Reuter Schwabe 2023; Yu, Liu, Wang 2023). now package focus subsampling methods Leverage sampling Ma, Mahoney, Yu (2014) Ma Sun (2015). Local case control sampling Fithian Hastie (2015). - L-optimality based subsampling methods Generalised Linear Models Wang, Zhu, Ma (2018) Ai, Yu, et al. (2021). -optimality based subsampling Gaussian Linear Model Lee, Schifano, Wang (2021). - L-optimality based subsampling methods Generalised Linear Models response involved probability calculation Zhang, Ning, Ruppert (2021). - L-optimality based model robust/average subsampling methods Generalised Linear Models Mahendran, Thompson, McGree (2023). Subsampling Generalised Linear Models potential model misspecification Adewale Wiens (2009) Adewale Xu (2010).","code":""},{"path":[]},{"path":"/articles/MB_Linear_Reg_Electricity_Con.html","id":"understanding-the-electric-consumption-data","dir":"Articles","previous_headings":"","what":"Understanding the electric consumption data","title":"Model based Subsampling for Electric Consumption Data","text":"``Electric power consumption’’ data (Hebrail Berard 2012), contains \\(2,049,280\\) measurements house located Sceaux, France December 2006 November 2010. data contains \\(4\\) columns, first column response variable rest covariates, however use first \\(25\\%\\) data demonstration. response \\(y\\) log scaled intensity, covariates active electrical energy ) kitchen (\\(X_1\\)), b) laundry room (\\(X_2\\)) c) water-heater air-conditioner (\\(X_3\\)). covariates scaled mean zero variance one. given data subsampling methods implemented assuming main effects model can describe data. First observations electric consumption data. Based model methods random sampling, leverage sampling (Ma, Mahoney, Yu 2014; Ma Sun 2015), \\(\\)-optimality subsampling Gaussian Linear Model (Lee, Schifano, Wang 2022), \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) \\(\\)-optimality subsampling response constraint (Zhang, Ning, Ruppert 2021) implemented big data. obtained samples respective model parameter estimates \\(M=250\\) simulations across different sample sizes \\(k=(800,\\ldots,2000)\\). set initial sample size \\(r1=400\\) methods requires random sample. final samples, model parameters assume model estimated. estimated model parameters compared estimated model parameters full big data mean squared error \\(MSE(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ}\\sum_{=1}^M \\sum_{j=1}^J(\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\). , \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter.","code":"# Selecting 25% of the big data and prepare it indexes<-1:ceiling(nrow(Electric_consumption)*0.25) Original_Data<-cbind(Electric_consumption[indexes,1],1,Electric_consumption[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) }  head(Electric_consumption) %>%    kable(format = \"html\",         caption = \"First few observations of the electric consumption data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(8,20,by=2)*100; rep_k<-rep(k,each=M) # define colours, shapes, line types for the methods Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality GauLM\",\"A-Optimality MC\",                 \"Shrinkage Leverage\",\"Basic Leverage\",\"Unweighted Leverage\",                 \"Random Sampling\") Method_Colour<-c(\"#BBFFBB\",\"#50FF50\",\"#00BB00\",\"#008600\",                  \"#F76D5E\",\"#D82632\",\"#A50021\",\"#000000\") Method_Shape_Types<-c(rep(17,4),rep(4,3),16)"},{"path":"/articles/MB_Linear_Reg_Electricity_Con.html","id":"random-sampling","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Random sampling","title":"Model based Subsampling for Electric Consumption Data","text":"code implementation.","code":"# Random sampling Final_Beta_RS<-matrix(nrow = length(k)*M,ncol = ncol(Original_Data[,-1])+1) for (i in 1:length(rep_k)) {   Temp_Data<-Original_Data[sample(1:N,rep_k[i]),]   lm(Y~.-1,data=Temp_Data)->Results   Final_Beta_RS[i,]<-c(rep_k[i],coefficients(Results))   if(i==length(rep_k)){print(\"All simulations completed for random sampling\")} } ## [1] \"All simulations completed for random sampling\" Final_Beta_RS<-cbind.data.frame(\"Random Sampling\",Final_Beta_RS) colnames(Final_Beta_RS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Linear_Reg_Electricity_Con.html","id":"leverage-sampling","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Leverage sampling","title":"Model based Subsampling for Electric Consumption Data","text":"","code":"# Leverage sampling ## we set the shrinkage value of S_alpha=0.9 NeEDS4BigData::LeverageSampling(r=rep_k,Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),N=N,                                 S_alpha = 0.9,family = \"linear\")->Results ## Basic and shrinkage leverage probabilities calculated. ## Sampling completed. Final_Beta_LS<-Results$Beta_Estimates colnames(Final_Beta_LS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Linear_Reg_Electricity_Con.html","id":"a-optimality-subsampling-for-gaussian-linear-model","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"A-optimality subsampling for Gaussian Linear Model","title":"Model based Subsampling for Electric Consumption Data","text":"","code":"# A-optimality subsampling for Gaussian Linear Model NeEDS4BigData::AoptimalGauLMSub(r1=400,r2=rep_k,                                 Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),N=N)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_AoptGauLM<-Results$Beta_Estimates Final_Beta_AoptGauLM$Method<-rep(\"A-Optimality GauLM\",nrow(Final_Beta_AoptGauLM)) colnames(Final_Beta_AoptGauLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Linear_Reg_Electricity_Con.html","id":"a--and-l-optimality-subsampling","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"A- and L-optimality subsampling","title":"Model based Subsampling for Electric Consumption Data","text":"","code":"# A- and L-optimality subsampling for GLM NeEDS4BigData::ALoptimalGLMSub(r1=400,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,family = \"linear\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_ALoptGLM<-Results$Beta_Estimates colnames(Final_Beta_ALoptGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Linear_Reg_Electricity_Con.html","id":"a-optimality-subsampling-with-measurement-constraints","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"A-optimality subsampling with measurement constraints","title":"Model based Subsampling for Electric Consumption Data","text":"","code":"# A-optimality subsampling for without response NeEDS4BigData::AoptimalMCGLMSub(r1=400,r2=rep_k,                                 Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),                                 N=N,family=\"linear\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_AoptMCGLM<-Results$Beta_Estimates Final_Beta_AoptMCGLM$Method<-rep(\"A-Optimality MC\",nrow(Final_Beta_AoptMCGLM)) colnames(Final_Beta_AoptMCGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Linear_Reg_Electricity_Con.html","id":"summary","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Summary","title":"Model based Subsampling for Electric Consumption Data","text":"mean squared error plotted . Mean squared error ) subsampling methods b) without unweighted leverage sampling, 5% 95% percentile intervals.","code":"# Summarising Results of Scenario one Final_Beta<-rbind(Final_Beta_RS,Final_Beta_LS,                   Final_Beta_AoptGauLM,Final_Beta_ALoptGLM,                   Final_Beta_AoptMCGLM)  # Obtaining the model parameter estimates for the full big data model lm(Y~.-1,data = Original_Data)->All_Results coefficients(All_Results)->All_Beta matrix(rep(All_Beta,by=nrow(Final_Beta)),nrow = nrow(Final_Beta),        ncol = ncol(Final_Beta[,-c(1,2)]),byrow = TRUE)->All_Beta  remove(Final_Beta_RS,Final_Beta_LS,Final_Beta_AoptGauLM,Final_Beta_ALoptGLM,        Final_Beta_AoptMCGLM,Temp_Data,Results,All_Results) # Obtain the mean squared error for the model parameter estimates MSE_Beta<-data.frame(\"Method\"=Final_Beta$Method,                      \"Sample\"=Final_Beta$Sample,                      \"SE\"=rowSums((All_Beta - Final_Beta[,-c(1,2)])^2))   MSE_Beta$Method<-factor(MSE_Beta$Method,levels = Method_Names,labels = Method_Names)  Mean_Data <- MSE_Beta |>     group_by(Method,Sample) |>     dplyr::summarise(Mean = mean(SE),                      min=quantile(SE,0.05),                      max=quantile(SE,0.95), .groups = \"drop\")  # Plot for the mean squared error with all methods ggplot(data=Mean_Data,aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+   xlab(\"Sample Size\")+ylab(\"Squared Error\")+   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_Data,aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_Data,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+   scale_color_manual(values = Method_Colour)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+Theme_special()+   guides(colour = guide_legend(nrow = 3))->p1  # Plot for the mean squared error with all methods except unweighted leverage ggplot(data=Mean_Data[Mean_Data$Method != \"Unweighted Leverage\",],        aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+   xlab(\"Sample Size\")+ylab(\"Squared Error\")+   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_Data[Mean_Data$Method != \"Unweighted Leverage\",],             aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_Data[Mean_Data$Method != \"Unweighted Leverage\",],                 aes(ymin=min,ymax=max),width=0.3,                 position = position_dodge(width = 0.5))+   scale_color_manual(values = Method_Colour[-7])+   scale_shape_manual(values = Method_Shape_Types[-7])+   theme_bw()+Theme_special()+   guides(colour = guide_legend(nrow = 3))->p2  ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":"/articles/MB_Logistic_Reg_Skin_Seg.html","id":"understanding-the-skin-segmentation-data","dir":"Articles","previous_headings":"","what":"Understanding the skin segmentation data","title":"Model based Subsampling for Skin Segmentation Data","text":"Rajen Abhinav (2012) addressed challenge detecting skin-like regions images component intricate process facial recognition. achieve goal, ``Skin segmentation data’’ curated, data contains \\(4\\) columns \\(245,057\\) observations, first column response variable rest covariates. Aim logistic regression model classify images skin based ) Red, b) Green c) Blue colour data. Skin presence denoted one skin absence denoted zero. colour vector scaled mean zero variance one (initial range 0−255). given data subsampling methods implemented assuming main effects model can describe data. First observations skin segmentation data. Based model methods random sampling, leverage sampling (Ma, Mahoney, Yu 2014; Ma Sun 2015), local case control sampling (Fithian Hastie 2015), \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) \\(\\)-optimality subsampling response constraint (Zhang, Ning, Ruppert 2021) implemented big data. obtained samples respective model parameter estimates \\(M=250\\) simulations across different sample sizes \\(k=(800,\\ldots,2000)\\). set initial sample size \\(r1=400\\) methods requires random sample. final samples, model parameters assume model estimated. estimated model parameters compared estimated model parameters full big data mean squared error \\(MSE(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ}\\sum_{=1}^M \\sum_{j=1}^J(\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\). , \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter.","code":"# Selecting 100% of the big data and prepare it indexes<-1:nrow(Skin_segmentation) Original_Data<-cbind(Skin_segmentation[indexes,1],1,Skin_segmentation[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Skin_segmentation) %>%    kable(format = \"html\",         caption = \"First few observations of the skin segmentation data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(8,20,by=2)*100; rep_k<-rep(k,each=M) # define colours, shapes, line types for the methods Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MC\",                 \"Local case control sampling\",\"Shrinkage Leverage\",                 \"Basic Leverage\",\"Unweighted Leverage\",\"Random sampling\") Method_Colour<-c(\"#BBFFBB\",\"#50FF50\",\"#00BB00\",                  \"#FFAD72\",\"#F76D5E\",\"#D82632\",\"#A50021\",\"#000000\") Method_Shape_Types<-c(rep(17,3),rep(4,4),16)"},{"path":"/articles/MB_Logistic_Reg_Skin_Seg.html","id":"random-sampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Random sampling","title":"Model based Subsampling for Skin Segmentation Data","text":"code implementation.","code":"# Random sampling Final_Beta_RS<-matrix(nrow = length(k)*M,ncol = ncol(Original_Data[,-1])+1) for (i in 1:length(rep_k)) {   Temp_Data<-Original_Data[sample(1:N,rep_k[i]),]   glm(Y~.-1,data=Temp_Data,family=\"binomial\")->Results   Final_Beta_RS[i,]<-c(rep_k[i],coefficients(Results))   if(i==length(rep_k)){print(\"All simulations completed for random sampling\")} } ## [1] \"All simulations completed for random sampling\" Final_Beta_RS<-cbind.data.frame(\"Random sampling\",Final_Beta_RS) colnames(Final_Beta_RS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Logistic_Reg_Skin_Seg.html","id":"leverage-sampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Leverage sampling","title":"Model based Subsampling for Skin Segmentation Data","text":"","code":"# Leverage sampling ## we set the shrinkage value of S_alpha=0.9 NeEDS4BigData::LeverageSampling(r=rep_k,Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),N=N,                                 S_alpha = 0.9,family = \"logistic\")->Results ## Basic and shrinkage leverage probabilities calculated. ## Sampling completed. Final_Beta_LS<-Results$Beta_Estimates colnames(Final_Beta_LS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Logistic_Reg_Skin_Seg.html","id":"local-case-control-sampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Local case control sampling","title":"Model based Subsampling for Skin Segmentation Data","text":"","code":"# Local case control sampling NeEDS4BigData::LCCsampling(r1=400,r2=rep_k,                            Y=as.matrix(Original_Data[,1]),                            X=as.matrix(Original_Data[,-1]),                            N=N)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_LCCS<-Results$Beta_Estimates Final_Beta_LCCS$Method<-rep(\"Local case control sampling\",nrow(Final_Beta_LCCS)) colnames(Final_Beta_LCCS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Logistic_Reg_Skin_Seg.html","id":"a--and-l-optimality-subsampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"A- and L-optimality subsampling","title":"Model based Subsampling for Skin Segmentation Data","text":"","code":"# A- and L-optimality subsampling for GLM  NeEDS4BigData::ALoptimalGLMSub(r1=400,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,family = \"logistic\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_ALoptGLM<-Results$Beta_Estimates colnames(Final_Beta_ALoptGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Logistic_Reg_Skin_Seg.html","id":"a-optimality-subsampling-under-measurement-constraints","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"A-optimality subsampling under measurement constraints","title":"Model based Subsampling for Skin Segmentation Data","text":"","code":"# A-optimality subsampling for without response NeEDS4BigData::AoptimalMCGLMSub(r1=400,r2=rep_k,                                 Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),                                 N=N,family=\"logistic\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_AoptMCGLM<-Results$Beta_Estimates Final_Beta_AoptMCGLM$Method<-rep(\"A-Optimality MC\",nrow(Final_Beta_AoptMCGLM)) colnames(Final_Beta_AoptMCGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Logistic_Reg_Skin_Seg.html","id":"summary","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Summary","title":"Model based Subsampling for Skin Segmentation Data","text":"mean squared error plotted . Mean squared error ) subsampling methods b) without leverage local case control sampling, 5% 95% percentile intervals.","code":"# Summarising Results of Scenario one Final_Beta<-rbind(Final_Beta_RS,Final_Beta_LS,Final_Beta_LCCS,                   Final_Beta_ALoptGLM,Final_Beta_AoptMCGLM)  # Obtaining the model parameter estimates for the full big data model glm(Y~.-1,data = Original_Data,family=\"binomial\")->All_Results coefficients(All_Results)->All_Beta matrix(rep(All_Beta,by=nrow(Final_Beta)),nrow = nrow(Final_Beta),        ncol = ncol(Final_Beta[,-c(1,2)]),byrow = TRUE)->All_Beta  remove(Final_Beta_RS,Final_Beta_LS,Final_Beta_LCCS,Final_Beta_ALoptGLM,        Final_Beta_AoptMCGLM,Temp_Data,Results,All_Results) # Obtain the mean squared error for the model parameter estimates MSE_Beta<-data.frame(\"Method\"=Final_Beta$Method,                      \"Sample\"=Final_Beta$Sample,                      \"SE\"=rowSums((All_Beta - Final_Beta[,-c(1,2)])^2))   MSE_Beta$Method<-factor(MSE_Beta$Method,levels = Method_Names,labels = Method_Names)  Mean_Data <- MSE_Beta |>     group_by(Method,Sample) |>     dplyr::summarise(Mean = mean(SE),                      min=quantile(SE,0.05),                      max=quantile(SE,0.95), .groups = \"drop\")  # Plot for the mean squared error with all methods ggplot(data=Mean_Data,aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+   xlab(\"Sample Size\")+ylab(\"Squared Error\")+   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_Data,aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_Data,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+   scale_color_manual(values = Method_Colour)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+Theme_special()+   guides(colour = guide_legend(nrow = 3))->p1  Left_out<-c(paste0(c(\"Basic \",\"Shrinkage \",\"Unweighted \"),\"Leverage\"),             \"Local case control sampling\") # Plot for the mean squared error except leverage and local case control sampling ggplot(data=Mean_Data[!(Mean_Data$Method %in% Left_out),],        aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+   xlab(\"Sample Size\")+ylab(\"Squared Error\")+   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_Data[!(Mean_Data$Method %in% Left_out),],             aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_Data[!(Mean_Data$Method %in% Left_out),],                 aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+   scale_color_manual(values = Method_Colour[-c(4:7)])+   scale_shape_manual(values = Method_Shape_Types[-c(4:7)])+   theme_bw()+Theme_special()+   guides(colour = guide_legend(nrow = 2))->p2  ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":"/articles/MB_Poisson_Reg_Bike_Sharing.html","id":"understanding-the-bike-sharing-data","dir":"Articles","previous_headings":"","what":"Understanding the bike sharing data","title":"Model based Subsampling for Bike Sharing Data","text":"Fanaee-T Gama (2013) collected data understand bike sharing demands rental return process. data contains \\(4\\) columns \\(17,379\\) observations, first column response variable rest covariates. consider covariates ) temperature (\\(x_1\\)), b) humidity (\\(x_2\\)) c) wind speed (\\(x_3\\)) model response, number bikes rented hourly. covariates scaled mean zero variance one. given data subsampling methods implemented assuming main effects model can describe data. First observations bike sharing data. Based model methods random sampling, leverage sampling (Ma, Mahoney, Yu 2014; Ma Sun 2015), \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) \\(\\)-optimality subsampling response constraint (Zhang, Ning, Ruppert 2021) implemented big data. obtained samples respective model parameter estimates \\(M=250\\) simulations across different sample sizes \\(k=(800,\\ldots,2000)\\). set initial sample size \\(r1=400\\) methods requires random sample. final samples, model parameters assume model estimated. estimated model parameters compared estimated model parameters full big data mean squared error \\(MSE(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ}\\sum_{=1}^M \\sum_{j=1}^J(\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\). , \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter.","code":"# Selecting 100% of the big data and prepare it Original_Data<-cbind(Bike_sharing[,1],1,Bike_sharing[,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Bike_sharing) %>%    kable(format = \"html\",         caption = \"First few observations of the bike sharing data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(8,20,by=2)*100; rep_k<-rep(k,each=M) # define colors, shapes, line types for the methods Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MC\",                 \"Shrinkage Leverage\",\"Basic Leverage\",\"Unweighted Leverage\",                 \"Random Sampling\") Method_Colour<-c(\"#BBFFBB\",\"#50FF50\",\"#00BB00\",                  \"#F76D5E\",\"#D82632\",\"#A50021\",\"#000000\") Method_Shape_Types<-c(rep(17,3),rep(4,3),16)"},{"path":"/articles/MB_Poisson_Reg_Bike_Sharing.html","id":"random-sampling","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Random sampling","title":"Model based Subsampling for Bike Sharing Data","text":"code implementation.","code":"# Random sampling Final_Beta_RS<-matrix(nrow = length(k)*M,ncol = ncol(Original_Data[,-1])+1) for (i in 1:length(rep_k)) {   Temp_Data<-Original_Data[sample(1:N,rep_k[i]),]   glm(Y~.-1,data=Temp_Data,family=\"poisson\")->Results   Final_Beta_RS[i,]<-c(rep_k[i],coefficients(Results))   if(i==length(rep_k)){print(\"All simulations completed for random sampling\")} } ## [1] \"All simulations completed for random sampling\" Final_Beta_RS<-cbind.data.frame(\"Random Sampling\",Final_Beta_RS) colnames(Final_Beta_RS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Poisson_Reg_Bike_Sharing.html","id":"leverage-sampling","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Leverage sampling","title":"Model based Subsampling for Bike Sharing Data","text":"","code":"# Leverage sampling  ## we set the shrinkage value of S_alpha=0.9 NeEDS4BigData::LeverageSampling(r=rep_k,Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),N=N,                                 S_alpha = 0.9,family = \"poisson\")->Results ## Basic and shrinkage leverage probabilities calculated. ## Sampling completed. Final_Beta_LS<-Results$Beta_Estimates colnames(Final_Beta_LS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Poisson_Reg_Bike_Sharing.html","id":"a--and-l-optimality-subsampling","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"A- and L-optimality subsampling","title":"Model based Subsampling for Bike Sharing Data","text":"","code":"# A- and L-optimality subsampling for GLM NeEDS4BigData::ALoptimalGLMSub(r1=400,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,family = \"poisson\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_ALoptGLM<-Results$Beta_Estimates colnames(Final_Beta_ALoptGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Poisson_Reg_Bike_Sharing.html","id":"a-optimality-subsampling-with-measurement-constraints","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"A-optimality subsampling with measurement constraints","title":"Model based Subsampling for Bike Sharing Data","text":"","code":"# A-optimality subsampling without response NeEDS4BigData::AoptimalMCGLMSub(r1=400,r2=rep_k,                                 Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),                                 N=N,family=\"poisson\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_AoptMCGLM<-Results$Beta_Estimates Final_Beta_AoptMCGLM$Method<-rep(\"A-Optimality MC\",nrow(Final_Beta_AoptMCGLM)) colnames(Final_Beta_AoptMCGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/MB_Poisson_Reg_Bike_Sharing.html","id":"summary","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Summary","title":"Model based Subsampling for Bike Sharing Data","text":"mean squared error plotted . Mean squared error ) subsampling methods b) without unweighted leverage sampling, 5% 95% percentile intervals.","code":"# Summarising Results of Scenario one Final_Beta<-rbind(Final_Beta_RS,Final_Beta_LS,                   Final_Beta_ALoptGLM,Final_Beta_AoptMCGLM)  # Obtaining the model parameter estimates for the full big data model glm(Y~.-1,data = Original_Data,family=\"poisson\")->All_Results coefficients(All_Results)->All_Beta matrix(rep(All_Beta,by=nrow(Final_Beta)),nrow = nrow(Final_Beta),        ncol = ncol(Final_Beta[,-c(1,2)]),byrow = TRUE)->All_Beta  remove(Final_Beta_RS,Final_Beta_LS,Final_Beta_ALoptGLM,        Final_Beta_AoptMCGLM,Temp_Data,Results,All_Results) # Obtain the mean squared error for the model parameter estimates MSE_Beta<-data.frame(\"Method\"=Final_Beta$Method,                      \"Sample\"=Final_Beta$Sample,                      \"SE\"=rowSums((All_Beta - Final_Beta[,-c(1,2)])^2))   MSE_Beta$Method<-factor(MSE_Beta$Method,levels = Method_Names,labels = Method_Names)  Mean_Data <- MSE_Beta |>     group_by(Method,Sample) |>     dplyr::summarise(Mean = mean(SE),                      min=quantile(SE,0.05),                      max=quantile(SE,0.95), .groups = \"drop\")  # Plot for the mean squared error with all methods ggplot(data=Mean_Data,aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+   xlab(\"Sample Size\")+ylab(\"Squared Error\")+   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_Data,aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_Data,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+   scale_color_manual(values = Method_Colour)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+Theme_special()+   guides(colour = guide_legend(nrow = 3))->p1  # Plot for the mean squared error with all methods except unweighted leverage ggplot(data=Mean_Data[Mean_Data$Method != \"Unweighted Leverage\",],        aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+   xlab(\"Sample Size\")+ylab(\"Squared Error\")+   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_Data[Mean_Data$Method != \"Unweighted Leverage\",],             aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_Data[Mean_Data$Method != \"Unweighted Leverage\",],                 aes(ymin=min,ymax=max),width=0.3,                 position = position_dodge(width = 0.5))+   scale_color_manual(values = Method_Colour[-7])+   scale_shape_manual(values = Method_Shape_Types[-7])+   theme_bw()+Theme_special()+   guides(colour = guide_legend(nrow = 3))->p2  ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":"/articles/MB_Poisson_Reg_One_Million_Songs.html","id":"understanding-the-one-million-songs-data","dir":"Articles","previous_headings":"","what":"Understanding the one million songs data","title":"Model based Subsampling for One Million Songs Data","text":"Echo nest collected data understand songs popularity using paly counts 1 million users based songs basic information, duration, tempo, loudness, etc. data transformed accordingly works Bertin-Mahieux et al. (2011) study number counts songs features. data contains \\(7\\) columns \\(205,032\\) observations, first column response variable rest covariates. consider covariates ) duration (\\(x_1\\)), b) loudness (\\(x_2\\)), c) tempo (\\(x_3\\)), d) artist hotness (\\(x_4\\)), e) song hotness (\\(x_5\\)), f) album hotness (\\(x_6\\)) model response, number play counts song. covariates \\(x_1,x_2,x_3\\) scaled mean zero variance one, remaining covariates \\(0-1\\). given data subsampling methods implemented assuming main effects model can describe data. First observations one million songs data. Based model methods random sampling, leverage sampling (Ma, Mahoney, Yu 2014; Ma Sun 2015), \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) \\(\\)-optimality subsampling response constraint (Zhang, Ning, Ruppert 2021) implemented big data. obtained samples respective model parameter estimates \\(M=250\\) simulations across different sample sizes \\(k=(800,\\ldots,2000)\\). set initial sample size \\(r1=400\\) methods requires random sample. final samples, model parameters assume model estimated. estimated model parameters compared estimated model parameters full big data mean squared error \\(MSE(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ}\\sum_{=1}^M \\sum_{j=1}^J(\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\). , \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter.","code":"# Selecting 100% of the big data and prepare it Original_Data<-One_Million_Songs colnames(Original_Data)<-c(\"Y\",paste0(\"X\",1:ncol(Original_Data[,-1])))  # Scaling the covariate data for (j in 2:4) {   Original_Data[,j]<-scale(Original_Data[,j]) }  head(One_Million_Songs) %>%    kable(format = \"html\",         caption = \"First few observations of the one million songs data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(8,20,by=2)*100; rep_k<-rep(k,each=M) # define colors, shapes, line types for the methods Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MC\",                 \"Shrinkage Leverage\",\"Basic Leverage\",\"Unweighted Leverage\",                 \"Random Sampling\") Method_Colour<-c(\"#BBFFBB\",\"#50FF50\",\"#00BB00\",                  \"#F76D5E\",\"#D82632\",\"#A50021\",\"#000000\") Method_Shape_Types<-c(rep(17,3),rep(4,3),16)"},{"path":"/articles/MB_Poisson_Reg_One_Million_Songs.html","id":"random-sampling","dir":"Articles","previous_headings":"Understanding the one million songs data","what":"Random sampling","title":"Model based Subsampling for One Million Songs Data","text":"code implementation.","code":"# Random sampling Final_Beta_RS<-matrix(nrow = length(k)*M,ncol = ncol(Original_Data[,-1])+1) for (i in 1:length(rep_k)) {   Temp_Data<-Original_Data[sample(1:N,rep_k[i]),]   glm(Y~.-1,data=Temp_Data,family=\"poisson\")->Results   Final_Beta_RS[i,]<-c(rep_k[i],coefficients(Results))   if(i==length(rep_k)){print(\"All simulations completed for random sampling\")} } ## [1] \"All simulations completed for random sampling\" Final_Beta_RS<-cbind.data.frame(\"Random Sampling\",Final_Beta_RS) colnames(Final_Beta_RS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",1:6))"},{"path":"/articles/MB_Poisson_Reg_One_Million_Songs.html","id":"leverage-sampling","dir":"Articles","previous_headings":"Understanding the one million songs data","what":"Leverage sampling","title":"Model based Subsampling for One Million Songs Data","text":"","code":"# Leverage sampling  ## we set the shrinkage value of S_alpha=0.9 NeEDS4BigData::LeverageSampling(r=rep_k,Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),N=N,                                 S_alpha = 0.9,family = \"poisson\")->Results ## Basic and shrinkage leverage probabilities calculated. ## Sampling completed. Final_Beta_LS<-Results$Beta_Estimates colnames(Final_Beta_LS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",1:6))"},{"path":"/articles/MB_Poisson_Reg_One_Million_Songs.html","id":"a--and-l-optimality-subsampling","dir":"Articles","previous_headings":"Understanding the one million songs data","what":"A- and L-optimality subsampling","title":"Model based Subsampling for One Million Songs Data","text":"","code":"# A- and L-optimality subsampling for GLM NeEDS4BigData::ALoptimalGLMSub(r1=400,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,family = \"poisson\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_ALoptGLM<-Results$Beta_Estimates colnames(Final_Beta_ALoptGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",1:6))"},{"path":"/articles/MB_Poisson_Reg_One_Million_Songs.html","id":"a-optimality-subsampling-with-measurement-constraints","dir":"Articles","previous_headings":"Understanding the one million songs data","what":"A-optimality subsampling with measurement constraints","title":"Model based Subsampling for One Million Songs Data","text":"","code":"# A-optimality subsampling without response NeEDS4BigData::AoptimalMCGLMSub(r1=400,r2=rep_k,                                 Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),                                 N=N,family=\"poisson\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_AoptMCGLM<-Results$Beta_Estimates Final_Beta_AoptMCGLM$Method<-rep(\"A-Optimality MC\",nrow(Final_Beta_AoptMCGLM)) colnames(Final_Beta_AoptMCGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",1:6))"},{"path":"/articles/MB_Poisson_Reg_One_Million_Songs.html","id":"summary","dir":"Articles","previous_headings":"Understanding the one million songs data","what":"Summary","title":"Model based Subsampling for One Million Songs Data","text":"mean squared error plotted . Mean squared error ) subsampling methods b) without unweighted leverage sampling, 5% 95% percentile intervals.","code":"# Summarising Results of Scenario one Final_Beta<-rbind(Final_Beta_RS,Final_Beta_LS,                   Final_Beta_ALoptGLM,Final_Beta_AoptMCGLM)  # Obtaining the model parameter estimates for the full big data model glm(Y~.-1,data = Original_Data,family=\"poisson\")->All_Results coefficients(All_Results)->All_Beta matrix(rep(All_Beta,by=nrow(Final_Beta)),nrow = nrow(Final_Beta),        ncol = ncol(Final_Beta[,-c(1,2)]),byrow = TRUE)->All_Beta  remove(Final_Beta_RS,Final_Beta_LS,Final_Beta_ALoptGLM,        Final_Beta_AoptMCGLM,Temp_Data,Results,All_Results) # Obtain the mean squared error for the model parameter estimates MSE_Beta<-data.frame(\"Method\"=Final_Beta$Method,                      \"Sample\"=Final_Beta$Sample,                      \"SE\"=rowSums((All_Beta - Final_Beta[,-c(1,2)])^2))   MSE_Beta$Method<-factor(MSE_Beta$Method,levels = Method_Names,labels = Method_Names)  Mean_Data <- MSE_Beta |>     group_by(Method,Sample) |>     dplyr::summarise(Mean = mean(SE),                      min=quantile(SE,0.05),                      max=quantile(SE,0.95), .groups = \"drop\")  # Plot for the mean squared error with all methods ggplot(data=Mean_Data,aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+   xlab(\"Sample Size\")+ylab(\"Squared Error\")+   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_Data,aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_Data,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+   scale_color_manual(values = Method_Colour)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+Theme_special()+   guides(colour = guide_legend(nrow = 3))->p1  # Plot for the mean squared error with all methods except unweighted leverage ggplot(data=Mean_Data[Mean_Data$Method != \"Unweighted Leverage\",],        aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+   xlab(\"Sample Size\")+ylab(\"Squared Error\")+   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_Data[Mean_Data$Method != \"Unweighted Leverage\",],             aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_Data[Mean_Data$Method != \"Unweighted Leverage\",],                 aes(ymin=min,ymax=max),width=0.3,                 position = position_dodge(width = 0.5))+   scale_color_manual(values = Method_Colour[-7])+   scale_shape_manual(values = Method_Shape_Types[-7])+   theme_bw()+Theme_special()+   guides(colour = guide_legend(nrow = 3))->p2  ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":"/articles/MR_MM_Linear_Reg_Electricity_Con.html","id":"understanding-the-electric-consumption-data","dir":"Articles","previous_headings":"","what":"Understanding the electric consumption data","title":"Model robust and potential model misspecification for Electric Consumption Data","text":"``Electric power consumption’’ data (Hebrail Berard 2012), contains \\(2,049,280\\) measurements house located Sceaux, France December 2006 November 2010. data contains \\(4\\) columns \\(2,049,280\\) observations, first column response variable rest covariates, however use first \\(10\\%\\) data explanation. response \\(y\\) log scaled intensity, covariates active electrical energy ) kitchen (\\(X_1\\)), b) laundry room (\\(X_2\\)) c) water-heater air-conditioner (\\(X_3\\)). covariates scaled mean zero variance one. Given data analysed two different scenarios, model robust average subsampling methods assuming set models can describe data. subsampling method assuming main effects model potentially misspecified. First observations electric consumption data.","code":"# Selecting 25% of the big data and prepare it indexes<-1:ceiling(nrow(Electric_consumption)*0.10) Original_Data<-cbind(Electric_consumption[indexes,1],1,                      Electric_consumption[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Electric_consumption) %>%    kable(format = \"html\",         caption = \"First few observations of the electric consumption data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(8,20,by=2)*100; rep_k<-rep(k,each=M)"},{"path":"/articles/MR_MM_Linear_Reg_Electricity_Con.html","id":"model-robust-or-average-subsampling","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Model robust or average subsampling","title":"Model robust and potential model misspecification for Electric Consumption Data","text":"method \\(\\)- \\(L\\)-optimality model robust average subsampling (Mahendran, Thompson, McGree 2023) compared \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) method. five models selected eight models, 1) main effects model (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3\\)), 2-4) main effects model squared term covariate (\\(X^2_1 / X^2_2 / X^2_3\\)), 5-7) main effects model two squared terms (\\(X^2_1+X^2_2 / X^2_1+X^2_3 / X^2_2+X^2_3\\)) 8) main effects model squared terms. five models selected based smallest AIC values. model \\(l\\) mean squared error model parameters \\(MSE_l(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ} \\sum_{=1}^M \\sum_{j=1}^J (\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\) calculated \\(M=250\\) simulations across sample sizes \\(k=(800,\\ldots,2000)\\) initial sample size \\(r1=400\\). , \\(l\\)-th model \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter. Covariates selected five models X0, X1, X2, X3, X2^2, X3^2 X0, X1, X2, X3, X1^2, X2^2, X3^2 X0, X1, X2, X3, X3^2 X0, X1, X2, X3, X1^2, X3^2 X0, X1, X2, X3, X2^2","code":"# Define the subsampling methods and their respective colours, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MR\",\"L-Optimality MR\") Method_Colour<-c(\"#D82632\",\"#A50021\",\"#50FF50\",\"#00BB00\") Method_Shape_Types<-c(rep(8,2),rep(17,2))  # Preparing the data for the model average method with squared terms No_of_Variables<-ncol(Original_Data[,-c(1,2)]) Squared_Terms<-paste0(\"X\",1:No_of_Variables,\"^2\") term_no <- 2 All_Models <- list(c(\"X0\",paste0(\"X\",1:No_of_Variables)))  Original_Data_ModelRobust<-cbind(Original_Data,Original_Data[,-c(1,2)]^2) colnames(Original_Data_ModelRobust)<-c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables),                                        paste0(\"X\",1:No_of_Variables,\"^2\")) Temp_Data<-Original_Data_ModelRobust[sample(1:N,1000),]  AIC_Values<-NULL model <- lm(Y~.-1, data = Temp_Data[,c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables))]) AIC_Values[1]<-AIC(model) for (i in 1:No_of_Variables) {   x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))   for(j in 1:length(x))   {     All_Models[[term_no]] <- c(\"X0\",paste0(\"X\",1:No_of_Variables),x[[j]])     temp_data<-as.data.frame(Temp_Data[,c(\"Y\",All_Models[[term_no]])])     model <- lm(Y~.-1, data = temp_data)      AIC_Values[term_no]<-AIC(model)      term_no <- term_no+1   } }  Model_Set<-5 Best_Indices <- order(AIC_Values,decreasing = FALSE)[1:Model_Set]  Model_Apriori<-rep(1/Model_Set,Model_Set) All_Models<-All_Models[Best_Indices] names(All_Models)<-paste0(\"Model_\",1:length(All_Models))"},{"path":"/articles/MR_MM_Linear_Reg_Electricity_Con.html","id":"a-priori-probabilities-are-equal","dir":"Articles","previous_headings":"Understanding the electric consumption data > Model robust or average subsampling","what":"A priori probabilities are equal","title":"Model robust and potential model misspecification for Electric Consumption Data","text":"Consider \\(Q=5\\) model equal priori probability (.e \\(\\alpha_q=1/5,q=1,\\ldots,5\\)). code implementation scenario. Mean squared error models equal apriori order e Model 1 5 across subsampling methods comparison.","code":"All_Covariates<-colnames(Original_Data_ModelRobust)[-1] # A- and L-optimality model robust subsampling for linear regression NeEDS4BigData::modelRobustLinSub(r1=400,r2=rep_k,                                  Y=as.matrix(Original_Data_ModelRobust[,1]),                                  X=as.matrix(Original_Data_ModelRobust[,-1]),                                  N=N,Apriori_probs=rep(1/Model_Set,Model_Set),                                  All_Combinations=All_Models,                                  All_Covariates=All_Covariates)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelRobust<-Results$Beta_Estimates  # Mean squared error and their respective plots for all five models plot_list_MR<-list() for (i in 1:length(All_Models)) {   lm(Y~.-1,data=Original_Data_ModelRobust[,c(\"Y\",All_Models[[i]])])->All_Results   All_Beta<-coefficients(All_Results)      matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),          nrow = nrow(Final_Beta_modelRobust[[i]]),          ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta      data.frame(\"Method\"=Final_Beta_modelRobust[[i]]$Method,              \"Sample\"=Final_Beta_modelRobust[[i]]$r2,              \"SE\"=rowSums((All_Beta-                            Final_Beta_modelRobust[[i]][,-c(1,2)])^2))->MSE_Beta_MR      Mean_Data_MR <- MSE_Beta_MR |>     group_by(Method,Sample) |>     dplyr::summarise(Mean = mean(SE),                      min=quantile(SE,0.05),                      max=quantile(SE,0.95), .groups = \"drop\")      ggplot(Mean_Data_MR,aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+     xlab(\"Sample size\")+ylab(\"Squared Error\")+     geom_point(size=3,position = position_dodge(width = 0.5))+     geom_line(data=Mean_Data_MR,aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+     geom_errorbar(data=Mean_Data_MR,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+     ggtitle(paste0(\"Model \",i))+     scale_color_manual(values = Method_Colour)+     scale_shape_manual(values= Method_Shape_Types)+     theme_bw()+guides(colour= guide_legend(nrow = 2))+     Theme_special()->plot_list_MR[[i]] }  ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = \"auto\",           common.legend = TRUE,legend = \"bottom\")"},{"path":"/articles/MR_MM_Linear_Reg_Electricity_Con.html","id":"main-effects-model-is-potentially-misspecified","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Main effects model is potentially misspecified","title":"Model robust and potential model misspecification for Electric Consumption Data","text":"final scenario comparison subsampling method assumption main effects model potentially misspecified \\(\\)- \\(L\\)-optimality subsampling method. subsampling method accounts potential model misspecification take scaling factor \\(\\alpha=10\\). scenarios number simulations sample sizes stay . compare mean squared error estimated model parameters, however assume model potentially misspecified asymptotic approximation mean squared error predictions calculated well. code implementation. MSE model parameters () AMSE (b) potentially misspecified main effects model across subsampling methods comparison, 5% 95% percentile intervals.","code":"# Define the subsampling methods and their respective colours, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"RLmAMSE\",                 \"RLmAMSE Log Odds 10\",\"RLmAMSE Power 10\") Method_Colour<-c(\"#D82632\",\"#A50021\",\"#BBFFBB\",\"#50FF50\",\"#00BB00\") Method_Shape_Types<-c(rep(8,2),rep(17,3))  # A- and L-optimality and RLmAMSE model misspecified subsampling for linear regression  NeEDS4BigData::modelMissLinSub(r1=400,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,Alpha=10,proportion=0.5)->Results ## 50% or >=50% of the big data is used to help find AMSE for the subsamples,  ## this could take some time. ## Warning: executing %dopar% sequentially: no parallel backend registered ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelMiss<-Results$Beta_Estimates Final_AMSE_modelMiss<-Results$AMSE_Estimates  lm(Y~.-1,data = Original_Data)->All_Results coefficients(All_Results)->All_Beta  matrix(rep(All_Beta,by=nrow(Final_Beta_modelMiss)),nrow = nrow(Final_Beta_modelMiss),        ncol = ncol(Final_Beta_modelMiss[,-c(1,2)]),byrow = TRUE)->All_Beta  # Plots for the mean squared error of the model parameter estimates  # and the AMSE for the main effects model  MSE_Beta_modelMiss<-data.frame(\"Method\"=Final_Beta_modelMiss$Method,                                \"Sample\"=Final_Beta_modelMiss$r2,                                \"SE\"=rowSums((All_Beta -                                                 Final_Beta_modelMiss[,-c(1,2)])^2))  Mean_Data_MM <- MSE_Beta_modelMiss |>   group_by(Method,Sample) |>   dplyr::summarise(Mean = mean(SE),                    min=quantile(SE,0.05),                    max=quantile(SE,0.95), .groups = \"drop\")  ggplot(Mean_Data_MM,aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+   xlab(\"Sample size\")+ylab(\"MSE\")+   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_Data_MM,aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_Data_MM,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+   scale_color_manual(values = Method_Colour)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  Mean_AMSE<-Final_AMSE_modelMiss |> dplyr::group_by(r2,Method) |>         dplyr::summarise(Mean=mean(AMSE),                         min=quantile(AMSE,0.05),                         max=quantile(AMSE,0.95),.groups ='drop')  ggplot(Mean_AMSE,aes(x=factor(r2),y=Mean,color=Method,shape=Method)) +   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_AMSE,aes(x=factor(r2),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_AMSE,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+   xlab(\"Sample size\")+ylab(\"Mean AMSE\")+   scale_color_manual(values = Method_Colour)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":"/articles/MR_MM_Logistic_Reg_Skin_Seg.html","id":"understanding-the-skin-segmentation-data","dir":"Articles","previous_headings":"","what":"Understanding the skin segmentation data","title":"Model robust and potential model misspecification for Skin Segmentation Data","text":"Rajen Abhinav (2012) addressed challenge detecting skin-like regions images component intricate process facial recognition. achieve goal, ``Skin segmentation data’’ curated, data contains \\(4\\) columns \\(245,057\\) observations, first column response variable rest covariates. Aim logistic regression model classify images skin based ) Red, b) Green c) Blue colour data. Skin presence denoted one skin absence denoted zero. colour vector scaled mean zero variance one (initial range 0−255). Given data analysed two different scenarios, model robust average subsampling methods assuming set models can describe data. subsampling method assuming main effects model potentially misspecified. First observations skin segmentation data.","code":"# Selecting 100% of the big data and prepare it indexes<-1:nrow(Skin_segmentation) Original_Data<-cbind(Skin_segmentation[indexes,1],1,Skin_segmentation[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Skin_segmentation) %>%    kable(format = \"html\",         caption = \"First few observations of the skin segmentation data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(8,20,by=2)*100; rep_k<-rep(k,each=M)"},{"path":"/articles/MR_MM_Logistic_Reg_Skin_Seg.html","id":"model-robust-or-average-subsampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Model robust or average subsampling","title":"Model robust and potential model misspecification for Skin Segmentation Data","text":"method \\(\\)- \\(L\\)-optimality model robust average subsampling (Mahendran, Thompson, McGree 2023) compared \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) method. five models selected eight models, 1) main effects model (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3\\)), 2-4) main effects model squared term covariate (\\(X^2_1 / X^2_2 / X^2_3\\)), 5-7) main effects model two squared terms (\\(X^2_1+X^2_2 / X^2_1+X^2_3 / X^2_2+X^2_3\\)) 8) main effects model squared terms. five models selected based smallest AIC values. model \\(j\\) mean squared error model parameters \\(MSE_l(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ} \\sum_{=1}^M \\sum_{j=1}^J (\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\) calculated \\(M=250\\) simulations across sample sizes \\(k=(800,\\ldots,2000)\\) initial sample size \\(r1=400\\). , \\(l\\)-th model \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter. Covariates selected five models X0, X1, X2, X3, X2^2, X3^2 X0, X1, X2, X3, X1^2, X2^2, X3^2 X0, X1, X2, X3, X2^2 X0, X1, X2, X3, X1^2, X2^2 X0, X1, X2, X3, X1^2, X3^2","code":"# Define the subsampling methods and their respective colours, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MR\",\"L-Optimality MR\") Method_Colour<-c(\"#D82632\",\"#A50021\",\"#50FF50\",\"#00BB00\") Method_Shape_Types<-c(rep(8,2),rep(17,2))  # Preparing the data for the model average method with squared terms No_of_Variables<-ncol(Original_Data[,-c(1,2)]) Squared_Terms<-paste0(\"X\",1:No_of_Variables,\"^2\") term_no <- 2 All_Models <- list(c(\"X0\",paste0(\"X\",1:No_of_Variables)))  Original_Data_ModelRobust<-cbind(Original_Data,Original_Data[,-c(1,2)]^2) colnames(Original_Data_ModelRobust)<-c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables),                                        paste0(\"X\",1:No_of_Variables,\"^2\")) Temp_Data<-Original_Data_ModelRobust[sample(1:N,1000),]  AIC_Values<-NULL model <- glm(Y~.-1, family = \"binomial\",              data = Temp_Data[,c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables))]) AIC_Values[1]<-AIC(model) for (i in 1:No_of_Variables) {   x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))   for(j in 1:length(x))   {     All_Models[[term_no]] <- c(\"X0\",paste0(\"X\",1:No_of_Variables),x[[j]])     temp_data<-as.data.frame(Temp_Data[,c(\"Y\",All_Models[[term_no]])])     model <- glm(Y~.-1, data = temp_data,family = \"binomial\")      AIC_Values[term_no]<-AIC(model)      term_no <- term_no+1   } }  Model_Set<-5 Best_Indices <- order(AIC_Values,decreasing = FALSE)[1:Model_Set]  Model_Apriori<-rep(1/Model_Set,Model_Set) All_Models<-All_Models[Best_Indices] names(All_Models)<-paste0(\"Model_\",1:length(All_Models))"},{"path":"/articles/MR_MM_Logistic_Reg_Skin_Seg.html","id":"a-priori-probabilities-are-equal","dir":"Articles","previous_headings":"Understanding the skin segmentation data > Model robust or average subsampling","what":"A priori probabilities are equal","title":"Model robust and potential model misspecification for Skin Segmentation Data","text":"Consider \\(Q=5\\) model equal priori probability (.e \\(\\alpha_q=1/5,q=1,\\ldots,5\\)). code implementation scenario. Mean squared error models equal apriori order e Model 1 5 across subsampling methods comparison.","code":"All_Covariates<-colnames(Original_Data_ModelRobust)[-1] # A- and L-optimality model robust subsampling for logistic regression NeEDS4BigData::modelRobustLogSub(r1=400,r2=rep_k,                                  Y=as.matrix(Original_Data_ModelRobust[,1]),                                  X=as.matrix(Original_Data_ModelRobust[,-1]),                                  N=N,Apriori_probs=rep(1/Model_Set,Model_Set),                                  All_Combinations = All_Models,                                  All_Covariates = All_Covariates)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelRobust<-Results$Beta_Estimates  # Mean squared error and their respective plots for all five models MSE_Beta_MR<-list(); plot_list_MR<-list() for (i in 1:length(All_Models)) {   glm(Y~.-1,data=Original_Data_ModelRobust[,c(\"Y\",All_Models[[i]])],       family=\"binomial\")->All_Results   All_Beta<-coefficients(All_Results)    matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),          nrow = nrow(Final_Beta_modelRobust[[i]]),          ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta    MSE_Beta_MR<-data.frame(\"Method\"=Final_Beta_modelRobust[[i]]$Method,                           \"Sample\"=Final_Beta_modelRobust[[i]]$r2,                           \"SE\"=rowSums((All_Beta -                                          Final_Beta_modelRobust[[i]][,-c(1,2)])^2))    Mean_Data_MR <- MSE_Beta_MR |>     group_by(Method,Sample) |>     dplyr::summarise(Mean = mean(SE),                      min=quantile(SE,0.05),                      max=quantile(SE,0.95), .groups = \"drop\")      ggplot(Mean_Data_MR,aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+     xlab(\"Sample size\")+ylab(\"Squared Error\")+     geom_point(size=3,position = position_dodge(width = 0.5))+     geom_line(data=Mean_Data_MR,aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+     geom_errorbar(data=Mean_Data_MR,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+     ggtitle(paste0(\"Model \",i))+     scale_color_manual(values = Method_Colour)+     scale_shape_manual(values= Method_Shape_Types)+     theme_bw()+guides(colour= guide_legend(nrow = 2))+     Theme_special()->plot_list_MR[[i]] }  ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = \"auto\",           common.legend = TRUE,legend = \"bottom\")"},{"path":"/articles/MR_MM_Logistic_Reg_Skin_Seg.html","id":"main-effects-model-is-potentially-misspecified","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Main effects model is potentially misspecified","title":"Model robust and potential model misspecification for Skin Segmentation Data","text":"final third scenario comparison subsampling method assumption main effects model potentially misspecified \\(\\)- \\(L\\)-optimality subsampling method. subsampling method accounts potential model misspecification take scaling factor \\(\\alpha=10\\). scenario one two number simulations sample sizes stay . compare mean squared error estimated model parameters, however assume model potentially misspecified asymptotic approximation mean squared error predictions calculated well. code implementation. MSE model parameters () AMSE (b) potentially misspecified main effects model across subsampling methods comparison, 5% 95% percentile intervals.","code":"# Define the subsampling methods and their respective colours, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"RLmAMSE\",                 \"RLmAMSE Log Odds 10\",\"RLmAMSE Power 10\") Method_Colour<-c(\"#D82632\",\"#A50021\",\"#BBFFBB\",\"#50FF50\",\"#00BB00\") Method_Shape_Types<-c(rep(8,2),rep(17,3))  # A- and L-optimality and RLmAMSE model misspecified subsampling for logistic regression  NeEDS4BigData::modelMissLogSub(r1=400,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,Alpha=10, proportion = 1)->Results ## 50% or >=50% of the big data is used to help find AMSE for the subsamples,  ## this could take some time. ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelMiss<-Results$Beta_Estimates Final_AMSE_modelMiss<-Results$AMSE_Estimates  glm(Y~.-1,data=Original_Data,family=\"binomial\")->All_Results All_Beta<-coefficients(All_Results)  matrix(rep(All_Beta,by=nrow(Final_Beta_modelMiss)),nrow = nrow(Final_Beta_modelMiss),        ncol = ncol(Final_Beta_modelMiss[,-c(1,2)]),byrow = TRUE)->All_Beta  # Plots for the mean squared error of the model parameter estimates  # and the AMSE for the main effects model  MSE_Beta_modelMiss<-data.frame(\"Method\"=Final_Beta_modelMiss$Method,                                \"Sample\"=Final_Beta_modelMiss$r2,                                \"SE\"=rowSums((All_Beta -                                                 Final_Beta_modelMiss[,-c(1,2)])^2))  Mean_Data_MM <- MSE_Beta_modelMiss |>   group_by(Method,Sample) |>   dplyr::summarise(Mean = mean(SE),                    min=quantile(SE,0.05),                    max=quantile(SE,0.95), .groups = \"drop\")  ggplot(Mean_Data_MM,aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+   xlab(\"Sample size\")+ylab(\"MSE\")+   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_Data_MM,aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_Data_MM,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+   scale_color_manual(values = Method_Colour)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  Mean_AMSE<-Final_AMSE_modelMiss |> dplyr::group_by(r2,Method) |>         dplyr::summarise(Mean=mean(AMSE),                         min=quantile(AMSE,0.05),                         max=quantile(AMSE,0.95),.groups ='drop')  ggplot(Mean_AMSE,aes(x=factor(r2),y=Mean,color=Method,shape=Method)) +   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_AMSE,aes(x=factor(r2),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_AMSE,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+   xlab(\"Sample size\")+ylab(\"Mean AMSE\")+   scale_color_manual(values = Method_Colour)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":"/articles/MR_MM_Poisson_Reg_Bike_Sharing.html","id":"understanding-the-bike-sharing-data","dir":"Articles","previous_headings":"","what":"Understanding the bike sharing data","title":"Model robust and potential model misspecification for Bike Sharing Data","text":"Fanaee-T Gama (2013) collected data understand bike sharing demands rental return process. data contains \\(4\\) columns \\(17,379\\) observations, first column response variable rest covariates. consider covariates ) temperature (\\(x_1\\)), b) humidity (\\(x_2\\)) c) windspeed (\\(x_3\\)) model response, number bikes rented hourly. covariates scaled mean zero variance one. Given data analysed two different scenarios, model robust average subsampling methods assuming set models can describe data. subsampling method assuming main effects model potentially misspecified. First observations bike sharing data.","code":"# Selecting 100% of the big data and prepare it Original_Data<-cbind(Bike_sharing[,1],1,Bike_sharing[,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Bike_sharing) %>%    kable(format = \"html\",         caption = \"First few observations of the bike sharing data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(8,20,by=2)*100; rep_k<-rep(k,each=M)"},{"path":"/articles/MR_MM_Poisson_Reg_Bike_Sharing.html","id":"model-robust-or-average-subsampling","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Model robust or average subsampling","title":"Model robust and potential model misspecification for Bike Sharing Data","text":"method \\(\\)- \\(L\\)-optimality model robust average subsampling (Mahendran, Thompson, McGree 2023) compared \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) method. five models selected eight models, 1) main effects model (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3\\)), 2-4) main effects model squared term covariate (\\(X^2_1 / X^2_2 / X^2_3\\)), 5-7) main effects model two squared terms (\\(X^2_1+X^2_2 / X^2_1+X^2_3 / X^2_2+X^2_3\\)) 8) main effects model squared terms. five models selected based smallest AIC values. model \\(j\\) mean squared error model parameters \\(MSE_l(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ} \\sum_{=1}^M \\sum_{j=1}^J (\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\) calculated \\(M=250\\) simulations across sample sizes \\(k=(800,\\ldots,2000)\\) initial sample size \\(r1=400\\). , \\(l\\)-th model \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter. Covariates selected five models X0, X1, X2, X3, X1^2, X2^2, X3^2 X0, X1, X2, X3, X1^2, X2^2 X0, X1, X2, X3, X1^2, X3^2 X0, X1, X2, X3, X1^2 X0, X1, X2, X3, X2^2, X3^2","code":"# Define the subsampling methods and their respective colours, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MR\",\"L-Optimality MR\") Method_Colour<-c(\"#D82632\",\"#A50021\",\"#50FF50\",\"#00BB00\") Method_Shape_Types<-c(rep(8,2),rep(17,2))  # Preparing the data for the model average method with squared terms No_of_Variables<-ncol(Original_Data[,-c(1,2)]) Squared_Terms<-paste0(\"X\",1:No_of_Variables,\"^2\") term_no <- 2 All_Models <- list(c(\"X0\",paste0(\"X\",1:No_of_Variables)))  Original_Data_ModelRobust<-cbind(Original_Data,Original_Data[,-c(1,2)]^2) colnames(Original_Data_ModelRobust)<-c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables),                                        paste0(\"X\",1:No_of_Variables,\"^2\")) Temp_Data<-Original_Data_ModelRobust[sample(1:N,1000),]  AIC_Values<-NULL model <- glm(Y~.-1, family = \"poisson\",              data = Temp_Data[,c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables))]) AIC_Values[1]<-AIC(model) for (i in 1:No_of_Variables) {   x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))   for(j in 1:length(x))   {     All_Models[[term_no]] <- c(\"X0\",paste0(\"X\",1:No_of_Variables),x[[j]])     temp_data<-as.data.frame(Temp_Data[,c(\"Y\",All_Models[[term_no]])])     model <- glm(Y~.-1, data = temp_data,family = \"poisson\")      AIC_Values[term_no]<-AIC(model)      term_no <- term_no+1   } }  Model_Set<-5 Best_Indices <- order(AIC_Values,decreasing = FALSE)[1:Model_Set]  Model_Apriori<-rep(1/Model_Set,Model_Set) All_Models<-All_Models[Best_Indices] names(All_Models)<-paste0(\"Model_\",1:length(All_Models))"},{"path":"/articles/MR_MM_Poisson_Reg_Bike_Sharing.html","id":"a-priori-probabilities-are-equal","dir":"Articles","previous_headings":"Understanding the bike sharing data > Model robust or average subsampling","what":"A priori probabilities are equal","title":"Model robust and potential model misspecification for Bike Sharing Data","text":"Consider \\(Q=5\\) model equal priori probability (.e \\(\\alpha_q=1/5,q=1,\\ldots,5\\)). code implementation scenario. Mean squared error models equal apriori order e Model 1 5 across subsampling methods comparison.","code":"All_Covariates<-colnames(Original_Data_ModelRobust)[-1] # A- and L-optimality model robust subsampling for poisson regression NeEDS4BigData::modelRobustPoiSub(r1=400,r2=rep_k,                                  Y=as.matrix(Original_Data_ModelRobust[,1]),                                  X=as.matrix(Original_Data_ModelRobust[,-1]),                                  N=N,Apriori_probs=rep(1/Model_Set,Model_Set),                                  All_Combinations = All_Models,                                  All_Covariates = All_Covariates)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelRobust<-Results$Beta_Estimates  # Mean squared error and their respective plots for all five models plot_list_MR<-list() for (i in 1:length(All_Models)) {   glm(Y~.-1,data=Original_Data_ModelRobust[,c(\"Y\",All_Models[[i]])],       family=\"poisson\")->All_Results   All_Beta<-coefficients(All_Results)    matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),          nrow = nrow(Final_Beta_modelRobust[[i]]),          ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta    MSE_Beta_MR<-data.frame(\"Method\"=Final_Beta_modelRobust[[i]]$Method,                           \"Sample\"=Final_Beta_modelRobust[[i]]$r2,                           \"SE\"=rowSums((All_Beta -                                          Final_Beta_modelRobust[[i]][,-c(1,2)])^2))    Mean_Data_MR <- MSE_Beta_MR |>     group_by(Method,Sample) |>     dplyr::summarise(Mean = mean(SE),                      min=quantile(SE,0.05),                      max=quantile(SE,0.95), .groups = \"drop\")      ggplot(Mean_Data_MR,aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+     xlab(\"Sample size\")+ylab(\"Squared Error\")+     geom_point(size=3,position = position_dodge(width = 0.5))+     geom_line(data=Mean_Data_MR,aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+     geom_errorbar(data=Mean_Data_MR,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+     ggtitle(paste0(\"Model \",i))+     scale_color_manual(values = Method_Colour)+     scale_shape_manual(values= Method_Shape_Types)+     theme_bw()+guides(colour= guide_legend(nrow = 2))+     Theme_special()->plot_list_MR[[i]] }  ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = \"auto\",           legend = \"bottom\",common.legend = TRUE)"},{"path":"/articles/MR_MM_Poisson_Reg_Bike_Sharing.html","id":"main-effects-model-is-potentially-misspecified","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Main effects model is potentially misspecified","title":"Model robust and potential model misspecification for Bike Sharing Data","text":"final third scenario comparison subsampling method assumption main effects model potentially misspecified \\(\\)- \\(L\\)-optimality subsampling method. subsampling method accounts potential model misspecification take scaling factor \\(\\alpha=10\\). scenario one two number simulations sample sizes stay . compare mean squared error estimated model parameters, however assume model potentially misspecified asymptotic approximation mean squared error predictions calculated well. code implementation. MSE model parameters () AMSE (b) potentially misspecified main effects model across subsampling methods comparison, 5% 95% percentile intervals.","code":"# Define the subsampling methods and their respective colors, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"RLmAMSE\",                 \"RLmAMSE Log Odds 10\",\"RLmAMSE Power 10\") Method_Colour<-c(\"#D82632\",\"#A50021\",\"#BBFFBB\",\"#50FF50\",\"#00BB00\") Method_Shape_Types<-c(rep(8,2),rep(17,3))  # A- and L-optimality and RLmAMSE model misspecified subsampling for poisson regression  NeEDS4BigData::modelMissPoiSub(r1=400,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,Alpha=10, proportion = 1)->Results ## 50% or >=50% of the big data is used to help find AMSE for the subsamples,  ## this could take some time. ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelMiss<-Results$Beta_Estimates Final_AMSE_modelMiss<-Results$AMSE_Estimates  glm(Y~.-1,data=Original_Data,family=\"poisson\")->All_Results All_Beta<-coefficients(All_Results)  matrix(rep(All_Beta,by=nrow(Final_Beta_modelMiss)),nrow = nrow(Final_Beta_modelMiss),        ncol = ncol(Final_Beta_modelMiss[,-c(1,2)]),byrow = TRUE)->All_Beta  # Plots for the mean squared error of the model parameter estimates  # and the AMSE for the main effects model  MSE_Beta_modelMiss<-data.frame(\"Method\"=Final_Beta_modelMiss$Method,                                \"Sample\"=Final_Beta_modelMiss$r2,                                \"SE\"=rowSums((All_Beta -                                                Final_Beta_modelMiss[,-c(1,2)])^2))   Mean_Data_MM <- MSE_Beta_modelMiss |>   group_by(Method,Sample) |>   dplyr::summarise(Mean = mean(SE),                    min=quantile(SE,0.05),                    max=quantile(SE,0.95), .groups = \"drop\")  ggplot(Mean_Data_MM,aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+   xlab(\"Sample size\")+ylab(\"MSE\")+   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_Data_MM,aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_Data_MM,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+   scale_color_manual(values = Method_Colour)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  Mean_AMSE<-Final_AMSE_modelMiss |> dplyr::group_by(r2,Method) |>         dplyr::summarise(Mean=mean(AMSE),                         min=quantile(AMSE,0.05),                         max=quantile(AMSE,0.95),.groups ='drop')  ggplot(Mean_AMSE,aes(x=factor(r2),y=Mean,color=Method,shape=Method)) +   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_AMSE,aes(x=factor(r2),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_AMSE,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+   xlab(\"Sample size\")+ylab(\"Mean AMSE\")+   scale_color_manual(values = Method_Colour)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":"/articles/MR_MM_Poisson_Reg_One_Million_Songs.html","id":"understanding-the-bike-sharing-data","dir":"Articles","previous_headings":"","what":"Understanding the bike sharing data","title":"Model robust and potential model misspecification for One Million Songs Data","text":"Echo nest collected data understand songs popularity using paly counts 1 million users based songs basic information, duration, tempo, loudness, etc. data transformed accordingly works Bertin-Mahieux et al. (2011) study number counts songs features. data contains \\(7\\) columns \\(205,032\\) observations, first column response variable rest covariates. consider covariates ) duration (\\(x_1\\)), b) loudness (\\(x_2\\)), c) tempo (\\(x_3\\)), d) artist hotness (\\(x_4\\)), e) song hotness (\\(x_5\\)), f) album hotness (\\(x_6\\)) model response, number play counts song. covariates \\(x_1,x_2,x_3\\) scaled mean zero variance one, remaining covariates \\(0-1\\). given data subsampling methods implemented assuming main effects model can describe data. Given data analysed two different scenarios, model robust average subsampling methods assuming set models can describe data. subsampling method assuming main effects model potentially misspecified. First observations one million songs data.","code":"# Selecting 100% of the big data and prepare it Original_Data<-One_Million_Songs colnames(Original_Data)<-c(\"Y\",paste0(\"X\",1:ncol(Original_Data[,-1])))  # Scaling the covariate data for (j in 2:4) {   Original_Data[,j]<-scale(Original_Data[,j]) }  head(One_Million_Songs) %>%    kable(format = \"html\",         caption = \"First few observations of the one million songs data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(8,20,by=2)*100; rep_k<-rep(k,each=M)"},{"path":"/articles/MR_MM_Poisson_Reg_One_Million_Songs.html","id":"model-robust-or-average-subsampling","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Model robust or average subsampling","title":"Model robust and potential model misspecification for One Million Songs Data","text":"method \\(\\)- \\(L\\)-optimality model robust average subsampling (Mahendran, Thompson, McGree 2023) compared \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) method. five models selected eight models, 1) main effects model (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3+\\beta_4X_4+\\beta_5X_5+\\beta_6X_6\\)), 2-7) main effects model squared term covariate (\\(X^2_1 / X^2_2 / X^2_3 / X^2_4 / X^2_5 / X^2_6\\)), 8-22) main effects model pair squared terms, 23-42) main effects model three kind squared terms, 43-57) main effects model four kind squared terms, 58-63) main effects model five kind squared terms 64) main effects model squared terms. five models selected based smallest AIC values. model \\(j\\) mean squared error model parameters \\(MSE_l(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ} \\sum_{=1}^M \\sum_{j=1}^J (\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\) calculated \\(M=250\\) simulations across sample sizes \\(k=(800,\\ldots,2000)\\) initial sample size \\(r1=400\\). , \\(l\\)-th model \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter. Covariates selected five models X1, X2, X3, X4, X5, X6, X1^2, X4^2, X6^2 X1, X2, X3, X4, X5, X6, X1^2, X4^2 X1, X2, X3, X4, X5, X6, X1^2, X3^2, X4^2, X6^2 X1, X2, X3, X4, X5, X6, X1^2, X4^2, X5^2 X1, X2, X3, X4, X5, X6, X1^2, X2^2, X4^2, X6^2","code":"# Define the subsampling methods and their respective colours, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MR\",\"L-Optimality MR\") Method_Colour<-c(\"#D82632\",\"#A50021\",\"#50FF50\",\"#00BB00\") Method_Shape_Types<-c(rep(8,2),rep(17,2))  # Preparing the data for the model average method with squared terms No_of_Variables<-ncol(Original_Data[,-1]) Squared_Terms<-paste0(\"X\",1:No_of_Variables,\"^2\") term_no <- 2 All_Models <- list(c(paste0(\"X\",1:No_of_Variables)))  Original_Data_ModelRobust<-cbind(Original_Data,Original_Data[,-1]^2) colnames(Original_Data_ModelRobust)<-c(\"Y\",paste0(\"X\",1:No_of_Variables),                                        paste0(\"X\",1:No_of_Variables,\"^2\")) Temp_Data<-Original_Data_ModelRobust[sample(1:N,1000),]  AIC_Values<-NULL model <- glm(Y~.-1, family = \"poisson\",              data = Temp_Data[,c(\"Y\",paste0(\"X\",1:No_of_Variables))]) AIC_Values[1]<-AIC(model) for (i in 1:No_of_Variables) {   x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))   for(j in 1:length(x))   {     All_Models[[term_no]] <- c(paste0(\"X\",1:No_of_Variables),x[[j]])     temp_data<-as.data.frame(Temp_Data[,c(\"Y\",All_Models[[term_no]])])     model <- glm(Y~.-1, data = temp_data,family = \"poisson\")      AIC_Values[term_no]<-AIC(model)      term_no <- term_no+1   } }  Model_Set<-5 Best_Indices <- order(AIC_Values,decreasing = FALSE)[1:Model_Set]  Model_Apriori<-rep(1/Model_Set,Model_Set) All_Models<-All_Models[Best_Indices] names(All_Models)<-paste0(\"Model_\",1:length(All_Models))"},{"path":"/articles/MR_MM_Poisson_Reg_One_Million_Songs.html","id":"a-priori-probabilities-are-equal","dir":"Articles","previous_headings":"Understanding the bike sharing data > Model robust or average subsampling","what":"A priori probabilities are equal","title":"Model robust and potential model misspecification for One Million Songs Data","text":"Consider \\(Q=5\\) model equal priori probability (.e \\(\\alpha_q=1/5,q=1,\\ldots,5\\)). code implementation scenario. Mean squared error models equal apriori order e Model 1 5 across subsampling methods comparison.","code":"All_Covariates<-colnames(Original_Data_ModelRobust)[-1] # A- and L-optimality model robust subsampling for poisson regression NeEDS4BigData::modelRobustPoiSub(r1=400,r2=rep_k,                                  Y=as.matrix(Original_Data_ModelRobust[,1]),                                  X=as.matrix(Original_Data_ModelRobust[,-1]),                                  N=N,Apriori_probs=rep(1/Model_Set,Model_Set),                                  All_Combinations = All_Models,                                  All_Covariates = All_Covariates)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelRobust<-Results$Beta_Estimates  # Mean squared error and their respective plots for all five models plot_list_MR<-list() for (i in 1:length(All_Models)) {   glm(Y~.-1,data=Original_Data_ModelRobust[,c(\"Y\",All_Models[[i]])],       family=\"poisson\")->All_Results   All_Beta<-coefficients(All_Results)    matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),          nrow = nrow(Final_Beta_modelRobust[[i]]),          ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta    MSE_Beta_MR<-data.frame(\"Method\"=Final_Beta_modelRobust[[i]]$Method,                           \"Sample\"=Final_Beta_modelRobust[[i]]$r2,                           \"SE\"=rowSums((All_Beta -                                          Final_Beta_modelRobust[[i]][,-c(1,2)])^2))    Mean_Data_MR <- MSE_Beta_MR |>     group_by(Method,Sample) |>     dplyr::summarise(Mean = mean(SE),                      min=quantile(SE,0.05),                      max=quantile(SE,0.95), .groups = \"drop\")      ggplot(Mean_Data_MR,aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+     xlab(\"Sample size\")+ylab(\"Squared Error\")+     geom_point(size=3,position = position_dodge(width = 0.5))+     geom_line(data=Mean_Data_MR,aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+     geom_errorbar(data=Mean_Data_MR,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+     ggtitle(paste0(\"Model \",i))+     scale_color_manual(values = Method_Colour)+     scale_shape_manual(values= Method_Shape_Types)+     theme_bw()+guides(colour= guide_legend(nrow = 2))+     Theme_special()->plot_list_MR[[i]] }  ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = \"auto\",           legend = \"bottom\",common.legend = TRUE)"},{"path":"/articles/MR_MM_Poisson_Reg_One_Million_Songs.html","id":"main-effects-model-is-potentially-misspecified","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Main effects model is potentially misspecified","title":"Model robust and potential model misspecification for One Million Songs Data","text":"final third scenario comparison subsampling method assumption main effects model potentially misspecified \\(\\)- \\(L\\)-optimality subsampling method. subsampling method accounts potential model misspecification take scaling factor \\(\\alpha=10\\). scenario one two number simulations sample sizes stay . compare mean squared error estimated model parameters, however assume model potentially misspecified asymptotic approximation mean squared error predictions calculated well. code implementation. MSE model parameters () AMSE (b) potentially misspecified main effects model across subsampling methods comparison, 5% 95% percentile intervals.","code":"# Define the subsampling methods and their respective colors, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"RLmAMSE\",                 \"RLmAMSE Log Odds 10\",\"RLmAMSE Power 10\") Method_Colour<-c(\"#D82632\",\"#A50021\",\"#BBFFBB\",\"#50FF50\",\"#00BB00\") Method_Shape_Types<-c(rep(8,2),rep(17,3))  # A- and L-optimality and RLmAMSE model misspecified subsampling for poisson regression  NeEDS4BigData::modelMissPoiSub(r1=400,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,Alpha=10, proportion = 1)->Results ## 50% or >=50% of the big data is used to help find AMSE for the subsamples,  ## this could take some time. ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelMiss<-Results$Beta_Estimates Final_AMSE_modelMiss<-Results$AMSE_Estimates  glm(Y~.-1,data=Original_Data,family=\"poisson\")->All_Results All_Beta<-coefficients(All_Results)  matrix(rep(All_Beta,by=nrow(Final_Beta_modelMiss)),nrow = nrow(Final_Beta_modelMiss),        ncol = ncol(Final_Beta_modelMiss[,-c(1,2)]),byrow = TRUE)->All_Beta  # Plots for the mean squared error of the model parameter estimates  # and the AMSE for the main effects model  MSE_Beta_modelMiss<-data.frame(\"Method\"=Final_Beta_modelMiss$Method,                                \"Sample\"=Final_Beta_modelMiss$r2,                                \"SE\"=rowSums((All_Beta -                                                Final_Beta_modelMiss[,-c(1,2)])^2))   Mean_Data_MM <- MSE_Beta_modelMiss |>   group_by(Method,Sample) |>   dplyr::summarise(Mean = mean(SE),                    min=quantile(SE,0.05),                    max=quantile(SE,0.95), .groups = \"drop\")  ggplot(Mean_Data_MM,aes(x=factor(Sample),y=Mean,color=Method,shape=Method))+   xlab(\"Sample size\")+ylab(\"MSE\")+   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_Data_MM,aes(x=factor(Sample),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_Data_MM,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+   scale_color_manual(values = Method_Colour)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  Mean_AMSE<-Final_AMSE_modelMiss |> dplyr::group_by(r2,Method) |>         dplyr::summarise(Mean=mean(AMSE),                         min=quantile(AMSE,0.05),                         max=quantile(AMSE,0.95),.groups ='drop')  ggplot(Mean_AMSE,aes(x=factor(r2),y=Mean,color=Method,shape=Method)) +   geom_point(size=3,position = position_dodge(width = 0.5))+   geom_line(data=Mean_AMSE,aes(x=factor(r2),y=Mean,group=Method,color=Method),             position = position_dodge(width = 0.5))+   geom_errorbar(data=Mean_AMSE,aes(ymin=min,ymax=max),                 width=0.3,position = position_dodge(width = 0.5))+   xlab(\"Sample size\")+ylab(\"Mean AMSE\")+   scale_color_manual(values = Method_Colour)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Amalan Mahendran. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mahendran (2025). NeEDS4BigData: New Experimental Design Based Subsampling Methods Big Data. R package version 1.0.0, https://amalan-constat.github.io/NeEDS4BigData/index.html, https://github.com/Amalan-ConStat/NeEDS4BigData.","code":"@Manual{,   title = {NeEDS4BigData: New Experimental Design Based Subsampling Methods for Big Data},   author = {Amalan Mahendran},   year = {2025},   note = {R package version 1.0.0, https://amalan-constat.github.io/NeEDS4BigData/index.html},   url = {https://github.com/Amalan-ConStat/NeEDS4BigData}, }"},{"path":"/CONDUCT.html","id":null,"dir":"","previous_headings":"","what":"Contributor Code of Conduct","title":"Contributor Code of Conduct","text":"contributors maintainers project, pledge respect people contribute reporting issues, posting feature requests, updating documentation, submitting pull requests patches, activities. committed making participation project harassment-free experience everyone, regardless level experience, gender, gender identity expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion. Examples unacceptable behavior participants include use sexual language imagery, derogatory comments personal attacks, trolling, public private harassment, insults, unprofessional conduct. Project maintainers right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct. Project maintainers follow Code Conduct may removed project team. Instances abusive, harassing, otherwise unacceptable behavior may reported opening issue contacting one project maintainers. Code Conduct adapted Contributor Covenant (http:contributor-covenant.org), version 1.0.0, available http://contributor-covenant.org/version/1/0/0/","code":""},{"path":"/index.html","id":"needs4bigdata-","dir":"","previous_headings":"","what":"NeEDS4BigData - An R package for subsampling methods","title":"NeEDS4BigData - An R package for subsampling methods","text":"R package “NeEDS4BigData” provides approaches implement subsampling methods analyse big data.","code":""},{"path":"/index.html","id":"what-is-needs4bigdata-an-abbreviation-for","dir":"","previous_headings":"","what":"What is “NeEDS4BigData” an abbreviation for?","title":"NeEDS4BigData - An R package for subsampling methods","text":"New Experimental Design based Subsampling methods Big Data.","code":""},{"path":"/index.html","id":"how-to-engage-with-needs4bigdata-the-first-time-","dir":"","previous_headings":"","what":"How to engage with “NeEDS4BigData” the first time ?","title":"NeEDS4BigData - An R package for subsampling methods","text":"","code":"## Installing the package from GitHub devtools::install_github(\"Amalan-ConStat/NeEDS4BigData\")  ## Installing the package from CRAN install.packages(\"NeEDS4BigData\")"},{"path":"/index.html","id":"subsampling-methods","dir":"","previous_headings":"","what":"Subsampling Methods","title":"NeEDS4BigData - An R package for subsampling methods","text":"- L-optimality based subsampling GLMs. -optimality based subsampling Gaussian Linear Models. Leverage sampling GLMs. Local case control sampling logistic regression. -optimality based subsampling measurement constraints GLMs. Model robust subsampling method GLMs. Subsampling method GLMs model potentially misspecified. seven methods described following articles topics Introduction - explains need subsampling methods. Model based subsampling Model robust misspecification Benchmarking Functions 2) assume main effects model can describe data. 3) first consider several models can describe big data, later assume given main effects model misspecified. conditions 2) 3) explore subsampling four given big data sets. , explore computation time ran simulations scenarios 2) 3) compare subsampling functions full data modelling.","code":""},{"path":[]},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 NeEDS4BigData authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":null,"dir":"Reference","previous_headings":"","what":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"Using function sample big data linear, logistic Poisson regression describe data. Subsampling probabilities obtained based - L- optimality criteria.","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"","code":"ALoptimalGLMSub(r1,r2,Y,X,N,family)"},{"path":"/reference/ALoptimalGLMSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data family character value \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"output ALoptimalGLMSub gives list Beta_Estimates estimated model parameters data.frame subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon data.frame subsampling Utility_Estimates estimated D-(log scaled), - L- optimality values obtained subsamples Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Sample_L-Optimality list indexes initial optimal samples obtained based L-Optimality criteria Subsampling_Probability matrix calculated subsampling probabilities - L- optimality criteria","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"Two stage subsampling algorithm big data Generalised Linear Models (linear, logistic Poisson regression). First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters subsampling probabilities evaluated - L-optimality criteria. estimated subsampling probabilities optimal sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. character value provided family three types error message produced.","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"Wang H, Zhu R, Ma P (2018). “Optimal subsampling large sample logistic regression.” Journal American Statistical Association, 113(522), 829--844.  Ai M, Yu J, Zhang H, Wang H (2021). “Optimal subsampling algorithms big data regressions.” Statistica Sinica, 31(2), 749--772.  Yao Y, Wang H (2021). “review optimal subsampling methods massive datasets.” Journal Data Science, 19(1), 151--172.","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"linear\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(600,50); Original_Data<-Full_Data$Complete_Data;  ALoptimalGLMSub(r1 = r1, r2 = r2,Y = as.matrix(Original_Data[,1]),                 X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                 family = \"linear\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> Picking joint bandwidth of 0.00541 #> Picking joint bandwidth of 0.00513 #> Picking joint bandwidth of 0.00702   Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"logistic\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(600,50); Original_Data<-Full_Data$Complete_Data;  ALoptimalGLMSub(r1 = r1, r2 = r2,Y = as.matrix(Original_Data[,1]),                 X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                 family = \"logistic\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> Picking joint bandwidth of 0.0234 #> Picking joint bandwidth of 0.0275 #> Picking joint bandwidth of 0.0227   Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"poisson\" Full_Data<-GenGLMdata(Dist,NULL,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(600,50); Original_Data<-Full_Data$Complete_Data;  ALoptimalGLMSub(r1 = r1, r2 = r2,Y = as.matrix(Original_Data[,1]),                 X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                 family = \"poisson\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> Picking joint bandwidth of 0.00768 #> Picking joint bandwidth of 0.00193 #> Picking joint bandwidth of 0.00266"},{"path":"/reference/AoptimalGauLMSub.html","id":null,"dir":"Reference","previous_headings":"","what":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"Using function sample big data Gaussian linear regression models describe data. Subsampling probabilities obtained based -optimality criteria.","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"","code":"AoptimalGauLMSub(r1,r2,Y,X,N)"},{"path":"/reference/AoptimalGauLMSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"output AoptimalGauLMSub gives list Beta_Estimates estimated model parameters data.frame subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon data.frame subsampling Utility_Estimates estimated D-(log scaled), - L- optimality values obtained subsamples Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Subsampling_Probability matrix calculated subsampling probabilities -optimality criteria","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"Two stage subsampling algorithm big data Gaussian Linear Model. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters subsampling probabilities evaluated -optimality criteria. estimated subsampling probabilities optimal sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced.","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"Lee J, Schifano ED, Wang H (2021). “Fast optimal subsampling probability approximation generalized linear models.” Econometrics Statistics.","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"linear\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,12),50); Original_Data<-Full_Data$Complete_Data;  AoptimalGauLMSub(r1 = r1, r2 = r2,Y = as.matrix(Original_Data[,1]),                  X = as.matrix(Original_Data[,-1]),                  N = nrow(Original_Data))->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> Picking joint bandwidth of 0.00601 #> Picking joint bandwidth of 0.00517 #> Picking joint bandwidth of 0.00529"},{"path":"/reference/AoptimalMCGLMSub.html","id":null,"dir":"Reference","previous_headings":"","what":"A-optimality criteria based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","title":"A-optimality criteria based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"Using function sample big data linear, logistic Poisson regression describe data response \\(y\\) partially unavailable. Subsampling probabilities obtained based -optimality criteria.","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A-optimality criteria based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"","code":"AoptimalMCGLMSub(r1,r2,Y,X,N,family)"},{"path":"/reference/AoptimalMCGLMSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A-optimality criteria based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data family character value \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A-optimality criteria based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"output AoptimalMCGLMSub gives list Beta_Estimates estimated model parameters data.frame subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon data.frame subsampling (valid linear regression) Utility_Estimates estimated D-(log scaled), - L- optimality values obtained subsamples Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Subsampling_Probability matrix calculated subsampling probabilities -optimality criteria","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A-optimality criteria based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"Two stage subsampling algorithm big data Generalised Linear Models (linear, logistic Poisson regression) response available subsampling probability evaluation. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters subsampling probabilities evaluated -optimality criteria. estimated subsampling probabilities optimal sample size \\(r_2 \\ge r_1\\) obtained. Finally, optimal sample used model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. character value provided family three types error message produced.","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"A-optimality criteria based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"Zhang T, Ning Y, Ruppert D (2021). “Optimal sampling generalized linear models measurement constraints.” Journal Computational Graphical Statistics, 30(1), 106--114.","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A-optimality criteria based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"linear\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,12),50); Original_Data<-Full_Data$Complete_Data;  AoptimalMCGLMSub(r1 = r1, r2 = r2,Y = as.matrix(Original_Data[,1]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  family = \"linear\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> Picking joint bandwidth of 0.0102 #> Picking joint bandwidth of 0.00949 #> Picking joint bandwidth of 0.00887   Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"logistic\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,12),50); Original_Data<-Full_Data$Complete_Data;  AoptimalMCGLMSub(r1 = r1, r2 = r2,Y = as.matrix(Original_Data[,1]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  family = \"logistic\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> Picking joint bandwidth of 0.0388 #> Picking joint bandwidth of 0.0435 #> Picking joint bandwidth of 0.0409   Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"poisson\" Full_Data<-GenGLMdata(Dist,NULL,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,12),50); Original_Data<-Full_Data$Complete_Data;  AoptimalMCGLMSub(r1 = r1, r2 = r2,Y = as.matrix(Original_Data[,1]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  family = \"poisson\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> Picking joint bandwidth of 0.0148 #> Picking joint bandwidth of 0.00441 #> Picking joint bandwidth of 0.00455"},{"path":"/reference/Bike_sharing.html","id":null,"dir":"Reference","previous_headings":"","what":"Bike sharing data — Bike_sharing","title":"Bike sharing data — Bike_sharing","text":"Fanaee-T (2013) collected data understand bike sharing demands rental return process. data total contains 17,379 observations consider covariates temperature, humidity windspeed model response, number bikes rented hourly.","code":""},{"path":"/reference/Bike_sharing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bike sharing data — Bike_sharing","text":"","code":"Bike_sharing"},{"path":"/reference/Bike_sharing.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Bike sharing data — Bike_sharing","text":"data frame 4 columns 17,379 rows. Rented_Bikes Number bikes rented hourly Temperature Hourly temperature Humidity Hourly humidity Windspeed Hourly windspeed","code":""},{"path":"/reference/Bike_sharing.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Bike sharing data — Bike_sharing","text":"Extracted Fanaee-T H (2013) Bike Sharing. UCI Machine Learning Repository. Available : doi:10.24432/C5W894","code":""},{"path":"/reference/Bike_sharing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bike sharing data — Bike_sharing","text":"","code":"nrow(Bike_sharing) #> [1] 17379"},{"path":"/reference/Electric_consumption.html","id":null,"dir":"Reference","previous_headings":"","what":"Electric consumption data — Electric_consumption","title":"Electric consumption data — Electric_consumption","text":"Hebrail Berard (2012) described data contains 2,049,280 completed measurements house located Sceaux, France December 2006 November 2010. log scale minute-averaged current intensity selected response covariates active electrical energy (watt-hour) kitchen, laundry room, electric water-heater air-conditioner.","code":""},{"path":"/reference/Electric_consumption.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Electric consumption data — Electric_consumption","text":"","code":"Electric_consumption"},{"path":"/reference/Electric_consumption.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Electric consumption data — Electric_consumption","text":"data frame 4 columns 2,049,280 rows. Intensity Minute-averaged current intensity EE_Kitchen Active electrical energy (watt-hour) kitchen EE_Laundry Active electrical energy (watt-hour) laundry room EE_WH_AC Active electrical energy (watt-hour) electric water-heater air-conditioner","code":""},{"path":"/reference/Electric_consumption.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Electric consumption data — Electric_consumption","text":"Extracted Hebrail G, Berard (2012) Individual Household Electric Power Consumption. UCI Machine Learning Repository. Available : doi:10.24432/C58K54","code":""},{"path":"/reference/Electric_consumption.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Electric consumption data — Electric_consumption","text":"","code":"nrow(Electric_consumption) #> [1] 2049280"},{"path":"/reference/GenGLMdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate data for Generalised Linear Models — GenGLMdata","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"Function simulate big data linear, logistic Poisson regression sampling. Covariate data X Normal, Multivariate Normal Uniform distribution linear regression. Covariate data X Exponential, Normal, Multivariate Normal Uniform distribution logistic regression. Covariate data X Normal Uniform distribution Poisson regression.","code":""},{"path":"/reference/GenGLMdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"","code":"GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,family)"},{"path":"/reference/GenGLMdata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"Dist character value distribution \"Normal\", \"MVNormal\", \"Uniform \"Exponential\" Dist_Par list parameters distribution generate data covariate X No_Of_Var number variables Beta vector model parameters, including intercept N big data size family character vector \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/GenGLMdata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"output GenGLMdata gives list Complete_Data matrix Y X","code":""},{"path":"/reference/GenGLMdata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"Big data Generalised Linear Models generated \"linear\", \"logistic\" \"poisson\" regression types. limited covariate data generation linear regression normal, multivariate normal uniform distribution, logistic regression exponential, normal, multivariate normal uniform distribution Poisson regression normal uniform distribution.","code":""},{"path":"/reference/GenGLMdata.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"Lee Y, Nelder JA (1996). “Hierarchical generalized linear models.” Journal Royal Statistical Society Series B: Statistical Methodology, 58(4), 619--656.","code":""},{"path":"/reference/GenGLMdata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"","code":"No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000;  # Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) Dist<-\"MVNormal\"; Dist_Par<-list(Mean=rep(0,No_Of_Var),Variance=diag(rep(2,No_Of_Var)),Error_Variance=0.5) # Dist<-\"Uniform\"; Dist_Par<-list(Min=0,Max=1)  Family<-\"linear\" Results<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1); # Dist<-\"MVNormal\"; Dist_Par<-list(Mean=rep(0,No_Of_Var),Variance=diag(rep(2,No_Of_Var))) # Dist<-\"Exponential\"; Dist_Par<-list(Rate=3) # Dist<-\"Uniform\"; Dist_Par<-list(Min=0,Max=1)  Family<-\"logistic\" Results<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  # Dist<-\"Normal\"; Dist<-\"Uniform\"; Family<-\"poisson\" Results<-GenGLMdata(Dist,NULL,No_Of_Var,Beta,N,Family)"},{"path":"/reference/GenModelMissGLMdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"Function simulate big data Generalised Linear Models model misspecification scenario misspecification type.","code":""},{"path":"/reference/GenModelMissGLMdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"","code":"GenModelMissGLMdata(N,X_Data,Misspecification,Beta,Var_Epsilon,family)"},{"path":"/reference/GenModelMissGLMdata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"N big data size X_Data matrix covariate data Misspecification vector values misspecification Beta vector model parameters, including intercept misspecification term Var_Epsilon variance value residuals family character vector \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/GenModelMissGLMdata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"output GenModelMissGLMdata gives list Complete_Data matrix Y,X f(x)","code":""},{"path":"/reference/GenModelMissGLMdata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"Big data Generalised Linear Models generated \"linear\", \"logistic\" \"poisson\" regression types model misspecification.","code":""},{"path":"/reference/GenModelMissGLMdata.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"Adewale AJ, Wiens DP (2009). “Robust designs misspecified logistic models.” Journal Statistical Planning Inference, 139(1), 3--15.  Adewale AJ, Xu X (2010). “Robust designs generalized linear models possible overdispersion misspecified link functions.” Computational statistics & data analysis, 54(4), 875--890.","code":""},{"path":"/reference/GenModelMissGLMdata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"","code":"Beta<-c(-1,0.75,0.75,1); Var_Epsilon<-0.5; family <- \"linear\"; N<-10000 X_1 <- replicate(2,stats::runif(n=N,min = -1,max = 1))  Temp<-Rfast::rowprods(X_1) Misspecification <- (Temp-mean(Temp))/sqrt(mean(Temp^2)-mean(Temp)^2) X_Data <- cbind(X0=1,X_1);  Results<-GenModelMissGLMdata(N,X_Data,Misspecification,Beta,Var_Epsilon,family)  Results<-GenModelMissGLMdata(N,X_Data,Misspecification,Beta,Var_Epsilon=NULL,family=\"logistic\")  Results<-GenModelMissGLMdata(N,X_Data,Misspecification,Beta,Var_Epsilon=NULL,family=\"poisson\")"},{"path":"/reference/LCCsampling.html","id":null,"dir":"Reference","previous_headings":"","what":"Local case control sampling for logistic regression — LCCsampling","title":"Local case control sampling for logistic regression — LCCsampling","text":"Using function sample big data logistic regression describe data. Sampling probabilities obtained based local case control method.","code":""},{"path":"/reference/LCCsampling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Local case control sampling for logistic regression — LCCsampling","text":"","code":"LCCsampling(r1,r2,Y,X,N)"},{"path":"/reference/LCCsampling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Local case control sampling for logistic regression — LCCsampling","text":"r1 sample size initial random sampling r2 sample size local case control sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data","code":""},{"path":"/reference/LCCsampling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Local case control sampling for logistic regression — LCCsampling","text":"output LCCsampling gives list Beta_Estimates estimated model parameters data.frame sampling Utility_Estimates estimated D-(log scaled), - L- optimality values obtained subsamples Sample_LCC_Sampling list indexes initial optimal samples obtained based local case control sampling Sampling_Probability vector calculated sampling probabilities local case control sampling","code":""},{"path":"/reference/LCCsampling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Local case control sampling for logistic regression — LCCsampling","text":"Two stage sampling algorithm big data logistic regression. First obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters sampling probabilities evaluated local case control. estimated sampling probabilities optimal sample size \\(r_2 \\ge r_1\\) obtained. Finally, optimal sample used model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced.","code":""},{"path":"/reference/LCCsampling.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Local case control sampling for logistic regression — LCCsampling","text":"Fithian W, Hastie T (2015). “Local case-control sampling: Efficient subsampling imbalanced data sets.” Quality control applied statistics, 60(3), 187--190.","code":""},{"path":"/reference/LCCsampling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Local case control sampling for logistic regression — LCCsampling","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"logistic\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,9,12),50); Original_Data<-Full_Data$Complete_Data;  LCCsampling(r1 = r1, r2 = r2, Y = as.matrix(Original_Data[,1]),             X = as.matrix(Original_Data[,-1]),             N = nrow(Original_Data))->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> Picking joint bandwidth of 0.0298 #> Picking joint bandwidth of 0.0358 #> Picking joint bandwidth of 0.0338"},{"path":"/reference/LeverageSampling.html","id":null,"dir":"Reference","previous_headings":"","what":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"Using function sample big data linear, logistic Poisson regression describe data. Sampling probabilities obtained based basic shrinkage leverage method.","code":""},{"path":"/reference/LeverageSampling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"","code":"LeverageSampling(r,Y,X,N,S_alpha,family)"},{"path":"/reference/LeverageSampling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"r sample size Y response data Y X covariate data X matrix covariates (first column intercept) N size big data S_alpha shrinkage factor 0 1 family character vector \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/LeverageSampling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"output LeverageSampling gives list Beta_Estimates estimated model parameters data.frame sampling Variance_Epsilon_Estimates matrix estimated variance epsilon data.frame sampling (valid linear regression) Utility_Estimates estimated D-(log scaled), - L- optimality values obtained subsamples Sample_Basic_Leverage list indexes optimal samples obtained based basic leverage Sample_Shrinkage_Leverage list indexes optimal samples obtained based shrinkage leverage Sampling_Probability matrix calculated sampling probabilities basic shrinkage leverage","code":""},{"path":"/reference/LeverageSampling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"Leverage sampling algorithm big data Generalised Linear Models (linear, logistic Poisson regression). First obtain random sample size \\(min(r)/2\\) estimate model parameters. Using estimated parameters leverage scores evaluated leverage sampling. estimated leverage scores sample size \\(r\\) obtained. Finally, sample size \\(r\\) used model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. \\(0 < \\alpha_{S} < 1\\) satisfied error message produced. character vector provided family three types error message produced.","code":""},{"path":"/reference/LeverageSampling.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"Ma P, Mahoney M, Yu B (2014). “statistical perspective algorithmic leveraging.” International conference machine learning, 91--99. PMLR.  Ma P, Sun X (2015). “Leveraging big data regression.” Wiley Interdisciplinary Reviews: Computational Statistics, 7(1), 70--76.","code":""},{"path":"/reference/LeverageSampling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"linear\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r<-rep(100*c(6,10),50); Original_Data<-Full_Data$Complete_Data;  LeverageSampling(r = r, Y = as.matrix(Original_Data[,1]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  S_alpha = 0.95,                  family = \"linear\")->Results #> Basic and shrinkage leverage probabilities calculated. #> Sampling completed.  plot_Beta(Results) #> Picking joint bandwidth of 0.0105 #> Picking joint bandwidth of 0.00727 #> Picking joint bandwidth of 0.00867   Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"logistic\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r<-rep(100*c(6,10),50); Original_Data<-Full_Data$Complete_Data;  LeverageSampling(r = r, Y = as.matrix(Original_Data[,1]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  S_alpha = 0.95,                  family = \"logistic\")->Results #> Basic and shrinkage leverage probabilities calculated. #> Sampling completed.  plot_Beta(Results) #> Picking joint bandwidth of 0.0491 #> Picking joint bandwidth of 0.0633 #> Picking joint bandwidth of 0.0393   Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,0.5,0.5); N<-5000; Family<-\"poisson\" Full_Data<-GenGLMdata(Dist,NULL,No_Of_Var,Beta,N,Family)  r<-rep(100*c(6,10),50); Original_Data<-Full_Data$Complete_Data;  LeverageSampling(r = r, Y = as.matrix(Original_Data[,1]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  S_alpha = 0.95,                  family = \"poisson\")->Results #> Basic and shrinkage leverage probabilities calculated. #> Sampling completed.  plot_Beta(Results) #> Picking joint bandwidth of 0.0286 #> Picking joint bandwidth of 0.0188 #> Picking joint bandwidth of 0.0135"},{"path":"/reference/modelMissLinSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Subsampling under linear regression for a potentially misspecified model — modelMissLinSub","title":"Subsampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"Using function sample big data linear regression potentially misspecified model. Subsampling probabilities obtained based - L- optimality criteria RLmAMSE (Reduction Loss minimizing Average Mean Squared Error).","code":""},{"path":"/reference/modelMissLinSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subsampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"","code":"modelMissLinSub(r1,r2,Y,X,N,Alpha,proportion,model=\"Auto\")"},{"path":"/reference/modelMissLinSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subsampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha scaling factor using Log Odds Power functions magnify probabilities proportion proportion big data used help estimate AMSE values subsamples model formula model used GAM main effects (\"s()\"), squared term (\"()\") two-way interaction (\"lo()\") model","code":""},{"path":"/reference/modelMissLinSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subsampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"output modelMissLinSub gives list Beta_Estimates estimated model parameters subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon subsampling AMSE_Estimates matrix estimated AMSE values subsampling Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Sample_L-Optimality list indexes initial optimal samples obtained based L-Optimality criteria Sample_RLmAMSE list indexes optimal samples obtained based obtained based RLmAMSE Sample_RLmAMSE_Log_Odds list indexes optimal samples obtained based RLmAMSE Log Odds function Sample_RLmAMSE_Power list indexes optimal samples obtained based RLmAMSE Power function Subsampling_Probability matrix calculated subsampling probabilities","code":""},{"path":"/reference/modelMissLinSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Subsampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"article function preparation publication. Please patient. Two stage subsampling algorithm big data linear regression potential model misspecification. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters subsampling probabilities evaluated -, L-optimality criteria, RLmAMSE enhanced RLmAMSE (log-odds power) subsampling methods. estimated subsampling probabilities sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated - L-optimality, RLmAMSE enhanced RLmAMSE (log-odds power) optimal sample used. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\),F_estimate_Full aligned error message produced. \\(\\alpha > 1\\) scaling factor satisfied error message produced. proportion region \\((0,1]\\) error message produced. model formula input formed based covariates spline terms (s()), squared term (()), interaction terms (lo()) automatically. model empty NA NAN one defined inputs error message printed, default set model=\"Auto\".","code":""},{"path":"/reference/modelMissLinSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Subsampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"Adewale AJ, Wiens DP (2009). “Robust designs misspecified logistic models.” Journal Statistical Planning Inference, 139(1), 3--15.  Adewale AJ, Xu X (2010). “Robust designs generalized linear models possible overdispersion misspecified link functions.” Computational statistics & data analysis, 54(4), 875--890.","code":""},{"path":"/reference/modelMissLinSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Subsampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"","code":"Beta<-c(-1,0.75,0.75,1); Var_Epsilon<-0.5; family <- \"linear\"; N<-10000 X_1 <- replicate(2,stats::runif(n=N,min = -1,max = 1))  Temp<-Rfast::rowprods(X_1) Misspecification <- (Temp-mean(Temp))/sqrt(mean(Temp^2)-mean(Temp)^2) X_Data <- cbind(X0=1,X_1); Full_Data<-GenModelMissGLMdata(N,X_Data,Misspecification,Beta,Var_Epsilon,family)  r1<-300; r2<-rep(100*c(6,9),50); Original_Data<-Full_Data$Complete_Data[,-ncol(Full_Data$Complete_Data)];  # cl <- parallel::makeCluster(4) # doParallel::registerDoParallel(cl) if (FALSE) { Results<-modelMissLinSub(r1 = r1, r2 = r2,                          Y = as.matrix(Original_Data[,1]),                          X = as.matrix(Original_Data[,-1]),                          N = N, Alpha = 10, proportion = 0.3)  # parallel::stopCluster(cl)  plot_Beta(Results) plot_AMSE(Results) }"},{"path":"/reference/modelMissLogSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Subsampling under logistic regression for a potentially misspecified model — modelMissLogSub","title":"Subsampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"Using function sample big data logistic regression potentially misspecified model. Subsampling probabilities obtained based - L- optimality criteria RLmAMSE (Reduction Loss minimizing Average Mean Squared Error).","code":""},{"path":"/reference/modelMissLogSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subsampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"","code":"modelMissLogSub(r1,r2,Y,X,N,Alpha,proportion,model=\"Auto\")"},{"path":"/reference/modelMissLogSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subsampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha scaling factor using Log Odds Power functions magnify probabilities proportion proportion big data used help estimate AMSE values subsamples model formula model used GAM main effects (\"s()\"), squared term (\"()\") two-way interaction (\"lo()\") model","code":""},{"path":"/reference/modelMissLogSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subsampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"output modelMissLogSub gives list Beta_Estimates estimated model parameters subsampling AMSE_Estimates matrix estimated AMSE values subsampling Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Sample_L-Optimality list indexes initial optimal samples obtained based L-Optimality criteria Sample_RLmAMSE list indexes optimal samples obtained based RLmAMSE Sample_RLmAMSE_Log_Odds list indexes optimal samples obtained based RLmAMSE Log Odds function Sample_RLmAMSE_Power list indexes optimal samples obtained based RLmAMSE Power function Subsampling_Probability matrix calculated subsampling probabilities","code":""},{"path":"/reference/modelMissLogSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Subsampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"article function preparation publication. Please patient. Two stage subsampling algorithm big data logistic regression potential model misspecification. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters subsampling probabilities evaluated -, L-optimality criteria, RLmAMSE enhanced RLmAMSE(log-odds power) subsampling methods. estimated subsampling probabilities sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated - L-optimality, RLmAMSE enhanced RLmAMSE (log-odds power) optimal sample used. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\),F_estimate_Full aligned error message produced. \\(\\alpha > 1\\) scaling vector satisfied error message produced. proportion region \\((0,1]\\) error message produced. model formula input formed based covariates spline terms (s()), squared term (()), interaction terms (lo()) automatically. model empty NA NAN one defined inputs error message printed, default set model=\"Auto\".","code":""},{"path":"/reference/modelMissLogSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Subsampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"Adewale AJ, Wiens DP (2009). “Robust designs misspecified logistic models.” Journal Statistical Planning Inference, 139(1), 3--15.  Adewale AJ, Xu X (2010). “Robust designs generalized linear models possible overdispersion misspecified link functions.” Computational statistics & data analysis, 54(4), 875--890.","code":""},{"path":"/reference/modelMissLogSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Subsampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"","code":"Beta<-c(-1,0.75,0.75,1); family <- \"logistic\"; N<-10000 X_1 <- replicate(2,stats::runif(n=N,min = -1,max = 1))  Temp<-Rfast::rowprods(X_1) Misspecification <- (Temp-mean(Temp))/sqrt(mean(Temp^2)-mean(Temp)^2) X_Data <- cbind(X0=1,X_1); Full_Data<-GenModelMissGLMdata(N,X_Data,Misspecification,Beta,Var_Epsilon=NULL,family)  r1<-300; r2<-rep(100*c(6,9),50); Original_Data<-Full_Data$Complete_Data[,-ncol(Full_Data$Complete_Data)];  # cl <- parallel::makeCluster(4) # doParallel::registerDoParallel(cl) if (FALSE) { Results<-modelMissLogSub(r1 = r1, r2 = r2,                          Y = as.matrix(Original_Data[,1]),                          X = as.matrix(Original_Data[,-1]),                          N = N, Alpha = 10, proportion = 0.3)  # parallel::stopCluster(cl)  plot_Beta(Results) plot_AMSE(Results) }"},{"path":"/reference/modelMissPoiSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Subsampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","title":"Subsampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"Using function sample big data Poisson regression potentially misspecified model. Subsampling probabilities obtained based - L- optimality criteria RLmAMSE (Reduction Loss minimizing Average Mean Squared Error).","code":""},{"path":"/reference/modelMissPoiSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subsampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"","code":"modelMissPoiSub(r1,r2,Y,X,N,Alpha,proportion,model=\"Auto\")"},{"path":"/reference/modelMissPoiSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subsampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha scaling factor using Log Odds Power functions magnify probabilities proportion proportion big data used help estimate AMSE values subsamples model formula model used GAM main effects (\"s()\"), squared term (\"()\") two-way interaction (\"lo()\") model","code":""},{"path":"/reference/modelMissPoiSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subsampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"output modelMissPoiSub gives list Beta_Estimates estimated model parameters subsampling AMSE_Estimates matrix estimated AMSE values subsampling Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Sample_L-Optimality list indexes initial optimal samples obtained based L-Optimality criteria Sample_RLmAMSE list indexes optimal samples obtained based RLmAMSE Sample_RLmAMSE_Log_Odds list indexes optimal samples obtained based RLmAMSE Log Odds function Sample_RLmAMSE_Power list indexes optimal samples obtained based RLmAMSE Power function Subsampling_Probability matrix calculated subsampling probabilities","code":""},{"path":"/reference/modelMissPoiSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Subsampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"article function preparation publication. Please patient. Two stage subsampling algorithm big data Poisson regression potential model misspecification. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters subsampling probabilities evaluated -, L-optimality criteria, RLmAMSE enhanced RLmAMSE (log-odds power) subsampling methods. estimated subsampling probabilities sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated - L-optimality, RLmAMSE enhanced RLmAMSE (log-odds power) optimal sample used. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\),F_estimate_Full aligned error message produced. \\(\\alpha > 1\\) scaling vector satisfied error message produced. proportion region \\((0,1]\\) error message produced. model formula input formed based covariates spline terms (s()), squared term (()), interaction terms (lo()) automatically. model empty NA NAN one defined inputs error message printed, default set model=\"Auto\".","code":""},{"path":"/reference/modelMissPoiSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Subsampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"Adewale AJ, Wiens DP (2009). “Robust designs misspecified logistic models.” Journal Statistical Planning Inference, 139(1), 3--15.  Adewale AJ, Xu X (2010). “Robust designs generalized linear models possible overdispersion misspecified link functions.” Computational statistics & data analysis, 54(4), 875--890.","code":""},{"path":"/reference/modelMissPoiSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Subsampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"","code":"Beta<-c(-1,0.75,0.75,1); family <- \"poisson\"; N<-10000 X_1 <- replicate(2,stats::runif(n=N,min = -1,max = 1))  Temp<-Rfast::rowprods(X_1) Misspecification <- (Temp-mean(Temp))/sqrt(mean(Temp^2)-mean(Temp)^2) X_Data <- cbind(X0=1,X_1);  Full_Data<-GenModelMissGLMdata(N,X_Data,Misspecification,Beta,Var_Epsilon=NULL,family)  r1<-300; r2<-rep(100*c(6,9),50); Original_Data<-Full_Data$Complete_Data[,-ncol(Full_Data$Complete_Data)];  # cl <- parallel::makeCluster(4) # doParallel::registerDoParallel(cl) if (FALSE) { Results<-modelMissPoiSub(r1 = r1, r2 = r2,                          Y = as.matrix(Original_Data[,1]),                          X = as.matrix(Original_Data[,-1]),                          N = N, Alpha = 10, proportion = 0.3)  # parallel::stopCluster(cl)  plot_Beta(Results) plot_AMSE(Results) }"},{"path":"/reference/modelRobustLinSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"Using function sample big data linear regression one model describe data. Subsampling probabilities obtained based - L- optimality criteria.","code":""},{"path":"/reference/modelRobustLinSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"","code":"modelRobustLinSub(r1,r2,Y,X,N,Apriori_probs,All_Combinations,All_Covariates)"},{"path":"/reference/modelRobustLinSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Apriori_probs vector priori model probabilities used obtain model robust subsampling probabilities All_Combinations list possible models can describe data All_Covariates covariates models","code":""},{"path":"/reference/modelRobustLinSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"output modelRobustLinSub gives list Beta_Estimates estimated model parameters model list subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon model subsampling Utility_Estimates estimated D-(log scaled), - L- optimality values obtained subsamples Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Sample_A-Optimality_MR list indexes initial model robust optimal samples obtained based -Optimality criteria Sample_L-Optimality list indexes initial optimal samples obtained based L-Optimality criteria Sample_L-Optimality_MR list indexes initial model robust optimal samples obtained based L-Optimality criteria Subsampling_Probability matrix calculated subsampling probabilities - L- optimality criteria","code":""},{"path":"/reference/modelRobustLinSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"Two stage subsampling algorithm big data linear regression multiple models can describe big data. First stage obtain random sample size \\(r_1\\) estimate model parameters models. Using estimated parameters subsampling probabilities evaluated -, L-optimality criteria model averaging -, L-optimality subsampling methods. estimated subsampling probabilities sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated models. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. \\(0 < \\alpha_{q} < 1\\) priori model probabilities satisfied error message produced, \\(q=1,\\ldots,Q\\) \\(Q\\) number models model set.","code":""},{"path":"/reference/modelRobustLinSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"Mahendran , Thompson H, McGree JM (2023). “model robust subsampling approach Generalised Linear Models big data settings.” Statistical Papers, 64(4), 1137--1157.","code":""},{"path":"/reference/modelRobustLinSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"","code":"indexes<-1:ceiling(nrow(Electric_consumption)*0.005) Original_Data<-cbind(Electric_consumption[indexes,1],1,                      Electric_consumption[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)]))) for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) }  No_of_Variables<-ncol(Original_Data[,-c(1,2)]) Squared_Terms<-paste0(\"X\",1:No_of_Variables,\"^2\") term_no <- 2 All_Models <- list(c(\"X0\",paste0(\"X\",1:No_of_Variables)))  Original_Data<-cbind(Original_Data,Original_Data[,-c(1,2)]^2) colnames(Original_Data)<-c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables),                            paste0(\"X\",1:No_of_Variables,\"^2\"))  for (i in 1:No_of_Variables){   x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))     for(j in 1:length(x)){        All_Models[[term_no]] <- c(\"X0\",paste0(\"X\",1:No_of_Variables),x[[j]])        term_no <- term_no+1      }    }  All_Models<-All_Models[-c(5:7)] names(All_Models)<-paste0(\"Model_\",1:length(All_Models))  r1<-300; r2<-rep(100*c(6,12),25);  modelRobustLinSub(r1 = r1, r2 = r2, Y = as.matrix(Original_Data[,1]),                   X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                   Apriori_probs = rep(1/length(All_Models),length(All_Models)),                   All_Combinations = All_Models,                   All_Covariates = colnames(Original_Data)[-1])->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  Beta_Plots<-plot_Beta(Results)"},{"path":"/reference/modelRobustLogSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"Using function sample big data logistic regression one model describe data. Subsampling probabilities obtained based - L- optimality criteria.","code":""},{"path":"/reference/modelRobustLogSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"","code":"modelRobustLogSub(r1,r2,Y,X,N,Apriori_probs,All_Combinations,All_Covariates)"},{"path":"/reference/modelRobustLogSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Apriori_probs vector priori model probabilities used obtain model robust subsampling probabilities All_Combinations list possible models can describe data All_Covariates covariates models","code":""},{"path":"/reference/modelRobustLogSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"output modelRobustLinSub gives list Beta_Data estimated model parameters model list subsampling Utility_Estimates estimated D-(log scaled), - L- optimality values obtained subsamples Sample_L-optimality list indexes initial optimal samples obtained based L-optimality criteria Sample_L-optimality_MR list indexes initial model robust optimal samples obtained based L-optimality criteria Sample_A-optimality list indexes initial optimal samples obtained based -optimality criteria Sample_A-optimality_MR list indexes initial model robust optimal samples obtained based -optimality criteria Subsampling_Probability matrix calculated subsampling probabilities - L- optimality criteria","code":""},{"path":"/reference/modelRobustLogSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"Two stage subsampling algorithm big data logistic regression multiple models can describe big data. First stage obtain random sample size \\(r_1\\) estimate model parameters models. Using estimated parameters subsampling probabilities evaluated -, L-optimality criteria model averaging -, L-optimality subsampling methods. estimated subsampling probabilities sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated models. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. \\(0 < \\alpha_{q} < 1\\) priori model probabilities satisfied error message produced, \\(q=1,\\ldots,Q\\) \\(Q\\) number models model set.","code":""},{"path":"/reference/modelRobustLogSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"Mahendran , Thompson H, McGree JM (2023). “model robust subsampling approach Generalised Linear Models big data settings.” Statistical Papers, 64(4), 1137--1157.","code":""},{"path":"/reference/modelRobustLogSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"","code":"indexes<-1:ceiling(nrow(Skin_segmentation)*0.25) Original_Data<-cbind(Skin_segmentation[indexes,1],1,Skin_segmentation[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } No_of_Variables<-ncol(Original_Data[,-c(1,2)])  Squared_Terms<-paste0(\"X\",1:No_of_Variables,\"^2\") term_no <- 2 All_Models <- list(c(\"X0\",paste0(\"X\",1:No_of_Variables)))  Original_Data<-cbind(Original_Data,Original_Data[,-c(1,2)]^2) colnames(Original_Data)<-c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables),                            paste0(\"X\",1:No_of_Variables,\"^2\"))  for (i in 1:No_of_Variables){   x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))   for(j in 1:length(x)){     All_Models[[term_no]] <- c(\"X0\",paste0(\"X\",1:No_of_Variables),x[[j]])     term_no <- term_no+1   } } All_Models<-All_Models[-c(5:7)] names(All_Models)<-paste0(\"Model_\",1:length(All_Models))  r1<-300; r2<-rep(100*c(6,12),25);  modelRobustLogSub(r1 = r1, r2 = r2, Y = as.matrix(Original_Data[,1]),                   X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                   Apriori_probs = rep(1/length(All_Models),length(All_Models)),                   All_Combinations = All_Models,                   All_Covariates = colnames(Original_Data)[-1])->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  Beta_Plots<-plot_Beta(Results)"},{"path":"/reference/modelRobustPoiSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"Using function sample big data Poisson regression one model describe data. Subsampling probabilities obtained based - L- optimality criteria.","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"","code":"modelRobustPoiSub(r1,r2,Y,X,N,Apriori_probs,All_Combinations,All_Covariates)"},{"path":"/reference/modelRobustPoiSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Apriori_probs vector priori model probabilities used obtain model robust subsampling probabilities All_Combinations list possible models can describe data All_Covariates covariates models","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"output modelRobustLinSub gives list Beta_Data estimated model parameters model list subsampling Utility_Estimates estimated D-(log scaled), - L- optimality values obtained subsamples Sample_L-optimality list indexes initial optimal samples obtained based L-optimality criteria Sample_L-optimality_MR list indexes initial model robust optimal samples obtained based L-optimality criteria Sample_A-optimality list indexes initial optimal samples obtained based -optimality criteria Sample_A-optimality_MR list indexes initial model robust optimal samples obtained based -optimality criteria Subsampling_Probability matrix calculated subsampling probabilities - L- optimality criteria","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"Two stage subsampling algorithm big data Poisson regression multiple models can describe big data. First stage obtain random sample size \\(r_1\\) estimate model parameters models. Using estimated parameters subsampling probabilities evaluated -, L-optimality criteria model averaging -, L-optimality subsampling methods. estimated subsampling probabilities sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated models. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. \\(0 < \\alpha_{q} < 1\\) priori model probabilities satisfied error message produced, \\(q=1,\\ldots,Q\\) \\(Q\\) number models model set.","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"Mahendran , Thompson H, McGree JM (2023). “model robust subsampling approach Generalised Linear Models big data settings.” Statistical Papers, 64(4), 1137--1157.","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"","code":"indexes<-1:ceiling(nrow(Bike_sharing)*0.5) Original_Data<-cbind(Bike_sharing[indexes,1],1,Bike_sharing[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) }  No_of_Variables<-ncol(Original_Data[,-c(1,2)]) Squared_Terms<-paste0(\"X\",1:No_of_Variables,\"^2\") term_no <- 2 All_Models <- list(c(\"X0\",paste0(\"X\",1:No_of_Variables)))  Original_Data<-cbind(Original_Data,Original_Data[,-c(1,2)]^2) colnames(Original_Data)<-c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables),                             paste0(\"X\",1:No_of_Variables,\"^2\"))  for (i in 1:No_of_Variables) {   x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))   for(j in 1:length(x))   {     All_Models[[term_no]] <- c(\"X0\",paste0(\"X\",1:No_of_Variables),x[[j]])     term_no <- term_no+1   } } All_Models<-All_Models[-c(5:7)] names(All_Models)<-paste0(\"Model_\",1:length(All_Models))  r1<-300; r2<-rep(100*c(6,12),25);  modelRobustPoiSub(r1 = r1, r2 = r2, Y = as.matrix(Original_Data[,1]),                   X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                   Apriori_probs = rep(1/length(All_Models),length(All_Models)),                   All_Combinations = All_Models,                   All_Covariates = colnames(Original_Data)[-1])->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  Beta_Plots<-plot_Beta(Results)"},{"path":"/reference/One_Million_Songs.html","id":null,"dir":"Reference","previous_headings":"","what":"One Million Songs data — One_Million_Songs","title":"One Million Songs data — One_Million_Songs","text":"data set contains 1,019,318 unique users' music play counts Echo Nest, available \"http://millionsongdataset.com/tasteprofile/\". basic step, interesting predict play counts using song information collected Million Song Dataset (Bertin-Mahieux et al. (2011)). cleaning feature engineering data total contains 205,032 observations consider covariates duration, loudness, tempo, artist hotness, song hotness, album hotness model response, number song counts.","code":""},{"path":"/reference/One_Million_Songs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"One Million Songs data — One_Million_Songs","text":"","code":"One_Million_Songs"},{"path":"/reference/One_Million_Songs.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"One Million Songs data — One_Million_Songs","text":"data frame 4 columns 309,685 rows. Counts Number playback counts songs Duration Duration song Loudness Loudness song Tempo Tempo song Artist_Hotness value 0 1 Song_Hotness value 0 1 Album_Hotness value 0 1","code":""},{"path":"/reference/One_Million_Songs.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"One Million Songs data — One_Million_Songs","text":"McFee B, Bertin-Mahieux T, Ellis DP, Lanckriet GR (2012). “million song dataset challenge.” Proceedings 21st International Conference World Wide Web, 909--916.  Ai M, Yu J, Zhang H, Wang H (2021). “Optimal subsampling algorithms big data regressions.” Statistica Sinica, 31(2), 749--772.","code":""},{"path":"/reference/One_Million_Songs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"One Million Songs data — One_Million_Songs","text":"","code":"nrow(One_Million_Songs) #> [1] 205032"},{"path":"/reference/plot_AMSE.html","id":null,"dir":"Reference","previous_headings":"","what":"Plotting AMSE outputs for the samples under model misspecification — plot_AMSE","title":"Plotting AMSE outputs for the samples under model misspecification — plot_AMSE","text":"using subsampling methods potential model misspecification obtain respective AMSE values predictions. summarised plots .","code":""},{"path":"/reference/plot_AMSE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plotting AMSE outputs for the samples under model misspecification — plot_AMSE","text":"","code":"plot_AMSE(object)"},{"path":"/reference/plot_AMSE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plotting AMSE outputs for the samples under model misspecification — plot_AMSE","text":"object object subsampling subsampling function potential model misspecification","code":""},{"path":"/reference/plot_AMSE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plotting AMSE outputs for the samples under model misspecification — plot_AMSE","text":"output faceted ggplot result","code":""},{"path":"/reference/plot_AMSE.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plotting AMSE outputs for the samples under model misspecification — plot_AMSE","text":"- L-optimality criteria RLmAMSE subsampling Generalised Linear Models potential model misspecification facets variance bias^2 AMSE values.","code":""},{"path":"/reference/plot_Beta.html","id":null,"dir":"Reference","previous_headings":"","what":"Plotting model parameter outputs after subsampling — plot_Beta","title":"Plotting model parameter outputs after subsampling — plot_Beta","text":"using subsampling methods mostly obtain estimated model parameter estimates. , summarised histogram plots.","code":""},{"path":"/reference/plot_Beta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plotting model parameter outputs after subsampling — plot_Beta","text":"","code":"plot_Beta(object)"},{"path":"/reference/plot_Beta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plotting model parameter outputs after subsampling — plot_Beta","text":"object object subsampling subsampling functions","code":""},{"path":"/reference/plot_Beta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plotting model parameter outputs after subsampling — plot_Beta","text":"output faceted ggplot result","code":""},{"path":"/reference/plot_Beta.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plotting model parameter outputs after subsampling — plot_Beta","text":"local case control sampling facets sample sizes beta values. leverage sampling facets sample sizes beta values. - L-optimality criteria subsampling Generalised Linear Models facets sample sizes beta values. -optimality criteria subsampling Gaussian Linear Models facets sample sizes beta values. -optimality criteria subsampling Generalised Linear Models response variable inclusive facets sample sizes beta values. - L-optimality criteria subsampling Generalised Linear Models multiple models can describe data facets sample sizes beta values. - L-optimality criteria LmAMSE subsampling Generalised Linear Models potential model misspecification facets sample sizes beta values.","code":""},{"path":"/reference/Skin_segmentation.html","id":null,"dir":"Reference","previous_headings":"","what":"Skin segmentation data — Skin_segmentation","title":"Skin segmentation data — Skin_segmentation","text":"Rajen Abhinav (2012) addressed challenge detecting skin-like regions images component intricate process facial recognition. achieve goal, curated “Skin segmentation” data set, comprising RGB (R-red, G-green, B-blue) values randomly selected pixels N = 245,057 facial images, including 50,859 skin samples 194,198 nonskin samples, spanning diverse age groups, racial backgrounds, genders.","code":""},{"path":"/reference/Skin_segmentation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Skin segmentation data — Skin_segmentation","text":"","code":"Skin_segmentation"},{"path":"/reference/Skin_segmentation.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Skin segmentation data — Skin_segmentation","text":"data frame 4 columns 245,057 rows. Skin_presence Skin presence randomly selected pixels Red Red values randomly selected pixels Green Green values randomly selected pixels Blue Blue values randomly selected pixels","code":""},{"path":"/reference/Skin_segmentation.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Skin segmentation data — Skin_segmentation","text":"Extracted Rajen B, Abhinav D (2012) Skin segmentation. UCI Machine Learning Repository. Available : doi:10.24432/C5T30C","code":""},{"path":"/reference/Skin_segmentation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Skin segmentation data — Skin_segmentation","text":"","code":"nrow(Skin_segmentation) #> [1] 245057"},{"path":"/news/index.html","id":"needs4bigdata-100","dir":"Changelog","previous_headings":"","what":"NeEDS4BigData 1.0.0","title":"NeEDS4BigData 1.0.0","text":"Initial CRAN submission. Subsampling functions softmax quantile regression included coming versions. article subsampling method possible model misspecification updated later.","code":""}]
