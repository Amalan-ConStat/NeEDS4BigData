[{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"understanding-the-electric-consumption-data","dir":"Articles","previous_headings":"","what":"Understanding the electric consumption data","title":"Model based Sampling for Linear Regression","text":"``Electric power consumption’’ data (Hebrail Berard 2012), contains \\(2,049,280\\) measurements house located Sceaux, France December 2006 November 2010. data contains \\(4\\) columns, first column response variable rest covariates, however use first \\(5\\%\\) data demonstration. response \\(y\\) log scaled intensity, covariates active electrical energy ) kitchen (\\(X_1\\)), b) laundry room (\\(X_2\\)) c) water-heater air-conditioner (\\(X_3\\)). covariates scaled mean zero variance one. given data sampling methods implemented assuming main effects model can describe data. First observations electric consumption data. Based model methods random sampling, leverage sampling (Ma, Mahoney, Yu 2014; Ma Sun 2015), \\(\\)-optimality sampling Gaussian Linear Model (Lee, Schifano, Wang 2022), \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) \\(\\)-optimality sampling response constraint (Zhang, Ning, Ruppert 2021) implemented big data. obtained samples respective model parameter estimates \\(M=100\\) simulations across different sample sizes \\(k=(600,\\ldots,1500)\\). set initial sample size \\(r1=300\\) methods requires random sample. final samples, model parameters assume model estimated. estimated model parameters compared estimated model parameters full big data mean squared error \\(MSE(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ}\\sum_{=1}^M \\sum_{j=1}^J(\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\). , \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter.","code":"# Selecting 5% of the big data and prepare it indexes<-1:ceiling(nrow(Electric_consumption)*0.05) Original_Data<-cbind(Electric_consumption[indexes,1],1,Electric_consumption[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Original_Data) %>%    kable(format = \"html\",         caption = \"First few observations of the electric consumption data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(6,18,by=3)*100; rep_k<-rep(k,each=M) # define colours, shapes, line types for the methods Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality GauLM\",\"A-Optimality MC\",                 \"Shrinkage Leverage\",\"Basic Leverage\",\"Unweighted Leverage\",                 \"Random Sampling\") Method_Colour<-c(\"yellowgreen\",\"green4\",\"green\",\"springgreen4\",                  \"red\",\"darkred\",\"maroon\",\"black\") Method_Shape_Types<-c(rep(8,4),rep(4,3),16) Method_Line_Types<-c(rep(\"twodash\",4),rep(\"dotted\",3),\"solid\")"},{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"random-sampling","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Random sampling","title":"Model based Sampling for Linear Regression","text":"code implementing sampling methods.","code":"# Random sampling Final_Beta_RS<-matrix(nrow = length(k)*M,ncol = ncol(Original_Data[,-1])+1) for (i in 1:length(rep_k)) {   Temp_Data<-Original_Data[sample(1:N,rep_k[i]),]   lm(Y~.-1,data=Temp_Data)->Results   Final_Beta_RS[i,]<-c(rep_k[i],coefficients(Results))   if(i==length(rep_k)){print(\"All simulations completed for random sampling\")} } ## [1] \"All simulations completed for random sampling\" Final_Beta_RS<-cbind.data.frame(\"Random Sampling\",Final_Beta_RS) colnames(Final_Beta_RS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"leverage-sampling","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Leverage sampling","title":"Model based Sampling for Linear Regression","text":"","code":"# Leverage sampling ## we set the shrinkage value of alpha=0.9 NeEDS4BigData::LeverageSampling(r=rep_k,Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),N=N,                                 alpha = 0.9,family = \"linear\")->Results ## Basic and shrinkage leverage probabilities calculated. ## Sampling completed. Final_Beta_LS<-Results$Beta_Estimates colnames(Final_Beta_LS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"a-optimality-subsampling-for-gaussian-linear-model","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"A-optimality subsampling for Gaussian Linear Model","title":"Model based Sampling for Linear Regression","text":"","code":"# A-optimality subsampling for Gaussian Linear Model NeEDS4BigData::AoptimalGauLMSub(r1=300,r2=rep_k,                                 Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),N=N)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_AoptGauLM<-Results$Beta_Estimates Final_Beta_AoptGauLM$Method<-rep(\"A-Optimality GauLM\",nrow(Final_Beta_AoptGauLM)) colnames(Final_Beta_AoptGauLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"a--and-l-optimality-subsampling","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"A- and L-optimality subsampling","title":"Model based Sampling for Linear Regression","text":"","code":"# A- and L-optimality subsampling for GLM NeEDS4BigData::ALoptimalGLMSub(r1=300,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,family = \"linear\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_ALoptGLM<-Results$Beta_Estimates colnames(Final_Beta_ALoptGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"a-optimality-sampling-with-measurement-constraints","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"A-optimality sampling with measurement constraints","title":"Model based Sampling for Linear Regression","text":"","code":"# A-optimality sampling for without response NeEDS4BigData::AoptimalMCGLMSub(r1=300,r2=rep_k,                                 Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),                                 N=N,family=\"linear\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_AoptMCGLM<-Results$Beta_Estimates Final_Beta_AoptMCGLM$Method<-rep(\"A-Optimality MC\",nrow(Final_Beta_AoptMCGLM)) colnames(Final_Beta_AoptMCGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"summary","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Summary","title":"Model based Sampling for Linear Regression","text":"mean squared error plotted . Mean squared error ) sampling methods b) without unweighted leverage sampling.","code":"# Summarising Results of Scenario one Final_Beta<-rbind(Final_Beta_RS,Final_Beta_LS,                   Final_Beta_AoptGauLM,Final_Beta_ALoptGLM,                   Final_Beta_AoptMCGLM)  # Obtaining the model parameter estimates for the full big data model lm(Y~.-1,data = Original_Data)->All_Results coefficients(All_Results)->All_Beta matrix(rep(All_Beta,by=nrow(Final_Beta)),nrow = nrow(Final_Beta),        ncol = ncol(Final_Beta[,-c(1,2)]),byrow = TRUE)->All_Beta  remove(Final_Beta_RS,Final_Beta_LS,Final_Beta_AoptGauLM,Final_Beta_ALoptGLM,        Final_Beta_AoptMCGLM,Temp_Data,Results,All_Results) # Obtain the mean squared error for the model parameter estimates MSE_Beta<-data.frame(\"Method\"=Final_Beta$Method,                      \"Sample\"=Final_Beta$Sample,                      \"MSE\"=rowSums((All_Beta - Final_Beta[,-c(1,2)])^2))   MSE_Beta$Method<-factor(MSE_Beta$Method,levels = Method_Names,labels = Method_Names)  # Plot for the mean squared error with all methods ggplot(MSE_Beta |> dplyr::group_by(Method,Sample) |>         dplyr::summarise(MSE=mean(MSE),.groups ='drop'),     aes(x=factor(Sample),y=MSE,color=Method,group=Method,         linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Colour)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  # Plot for the mean squared error with all methods except unweighted leverage ggplot(MSE_Beta[MSE_Beta$Method != \"Unweighted Leverage\",] |>         dplyr::group_by(Method,Sample) |>         dplyr::summarise(MSE=mean(MSE),.groups ='drop'),    aes(x=factor(Sample),y=MSE,color=Method,group=Method,        linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Colour[-7])+   scale_linetype_manual(values=Method_Line_Types[-7])+   scale_shape_manual(values = Method_Shape_Types[-7])+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"understanding-the-skin-segmentation-data","dir":"Articles","previous_headings":"","what":"Understanding the skin segmentation data","title":"Model based Sampling for Logistic Regression","text":"Rajen Abhinav (2012) addressed challenge detecting skin-like regions images component intricate process facial recognition. achieve goal, ``Skin segmentation data’’ curated, data contains \\(4\\) columns \\(245,057\\) observations, first column response variable rest covariates. Aim logistic regression model classify images skin based ) Red, b) Green c) Blue colour data. Skin presence denoted one skin absence denoted zero. colour vector scaled mean zero variance one (initial range 0−255). given data sampling methods implemented assuming main effects model can describe data. First observations skin segmentation data. Based model methods random sampling, leverage sampling (Ma, Mahoney, Yu 2014; Ma Sun 2015), local case control sampling (Fithian Hastie 2015), \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) \\(\\)-optimality sampling response constraint (Zhang, Ning, Ruppert 2021) implemented big data. obtained samples respective model parameter estimates \\(M=100\\) simulations across different sample sizes \\(k=(600,\\ldots,1500)\\). set initial sample size \\(r1=300\\) methods requires random sample. final samples, model parameters assume model estimated. estimated model parameters compared estimated model parameters full big data mean squared error \\(MSE(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ}\\sum_{=1}^M \\sum_{j=1}^J(\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\). , \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter.","code":"# Selecting 100% of the big data and prepare it indexes<-1:nrow(Skin_segmentation) Original_Data<-cbind(Skin_segmentation[indexes,1],1,Skin_segmentation[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Original_Data) %>%    kable(format = \"html\",         caption = \"First few observations of the skin segmentation data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(6,18,by=3)*100; rep_k<-rep(k,each=M) # define colours, shapes, line types for the methods Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MC\",                 \"Local case control sampling\",\"Shrinkage Leverage\",                 \"Basic Leverage\",\"Unweighted Leverage\",\"Random sampling\") Method_Colour<-c(\"green\",\"green4\",\"yellowgreen\",                  \"maroon\",\"red\",\"darkred\",\"firebrick\",\"black\") Method_Shape_Types<-c(rep(8,3),rep(4,4),16) Method_Line_Types<-c(rep(\"twodash\",3),rep(\"dotted\",4),\"solid\")"},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"random-sampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Random sampling","title":"Model based Sampling for Logistic Regression","text":"code implementation.","code":"# Random sampling Final_Beta_RS<-matrix(nrow = length(k)*M,ncol = ncol(Original_Data[,-1])+1) for (i in 1:length(rep_k)) {   Temp_Data<-Original_Data[sample(1:N,rep_k[i]),]   glm(Y~.-1,data=Temp_Data,family=\"binomial\")->Results   Final_Beta_RS[i,]<-c(rep_k[i],coefficients(Results))   if(i==length(rep_k)){print(\"All simulations completed for random sampling\")} } ## [1] \"All simulations completed for random sampling\" Final_Beta_RS<-cbind.data.frame(\"Random sampling\",Final_Beta_RS) colnames(Final_Beta_RS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"leverage-sampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Leverage sampling","title":"Model based Sampling for Logistic Regression","text":"","code":"# Leverage sampling ## we set the shrinkage value of alpha=0.9 NeEDS4BigData::LeverageSampling(r=rep_k,Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),N=N,                                 alpha = 0.9,family = \"logistic\")->Results ## Basic and shrinkage leverage probabilities calculated. ## Sampling completed. Final_Beta_LS<-Results$Beta_Estimates colnames(Final_Beta_LS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"local-case-control-sampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Local case control sampling","title":"Model based Sampling for Logistic Regression","text":"","code":"# Local case control sampling NeEDS4BigData::LCCsampling(r1=300,r2=rep_k,                            Y=as.matrix(Original_Data[,1]),                            X=as.matrix(Original_Data[,-1]),                            N=N)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_LCCS<-Results$Beta_Estimates Final_Beta_LCCS$Method<-rep(\"Local case control sampling\",nrow(Final_Beta_LCCS)) colnames(Final_Beta_LCCS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"a--and-l-optimality-subsampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"A- and L-optimality subsampling","title":"Model based Sampling for Logistic Regression","text":"","code":"# A- and L-optimality subsampling for GLM  NeEDS4BigData::ALoptimalGLMSub(r1=300,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,family = \"logistic\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_ALoptGLM<-Results$Beta_Estimates colnames(Final_Beta_ALoptGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"a-optimality-sampling-under-measurement-constraints","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"A-optimality sampling under measurement constraints","title":"Model based Sampling for Logistic Regression","text":"","code":"# A-optimality sampling for without response NeEDS4BigData::AoptimalMCGLMSub(r1=300,r2=rep_k,                                 Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),                                 N=N,family=\"logistic\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_AoptMCGLM<-Results$Beta_Estimates Final_Beta_AoptMCGLM$Method<-rep(\"A-Optimality MC\",nrow(Final_Beta_AoptMCGLM)) colnames(Final_Beta_AoptMCGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"summary","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Summary","title":"Model based Sampling for Logistic Regression","text":"mean squared error plotted . Mean squared error ) sampling methods b) without unweighted leverage sampling.","code":"# Summarising Results of Scenario one Final_Beta<-rbind(Final_Beta_RS,Final_Beta_LS,Final_Beta_LCCS,                   Final_Beta_ALoptGLM,Final_Beta_AoptMCGLM)  # Obtaining the model parameter estimates for the full big data model glm(Y~.-1,data = Original_Data,family=\"binomial\")->All_Results coefficients(All_Results)->All_Beta matrix(rep(All_Beta,by=nrow(Final_Beta)),nrow = nrow(Final_Beta),        ncol = ncol(Final_Beta[,-c(1,2)]),byrow = TRUE)->All_Beta  remove(Final_Beta_RS,Final_Beta_LS,Final_Beta_LCCS,Final_Beta_ALoptGLM,        Final_Beta_AoptMCGLM,Temp_Data,Results,All_Results) # Obtain the mean squared error for the model parameter estimates MSE_Beta<-data.frame(\"Method\"=Final_Beta$Method,                      \"Sample\"=Final_Beta$Sample,                      \"MSE\"=rowSums((All_Beta - Final_Beta[,-c(1,2)])^2))   MSE_Beta$Method<-factor(MSE_Beta$Method,levels = Method_Names,labels = Method_Names)  # Plot for the mean squared error with all methods ggplot(MSE_Beta |> dplyr::group_by(Method,Sample) |>         dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),     aes(x=factor(Sample),y=MSE,color=Method,group=Method,         linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Colour)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  # Plot for the mean squared error with all methods except unweighted leverage ggplot(MSE_Beta[!(MSE_Beta$Method %in% c(\"Unweighted Leverage\",\"Shrinkage Leverage\",                                          \"Basic Leverage\")),] |>         dplyr::group_by(Method,Sample) |>         dplyr::summarise(MSE=mean(MSE),.groups = 'drop'),     aes(x=factor(Sample),y=MSE,color=Method,group=Method,         linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Colour[-7])+   scale_linetype_manual(values=Method_Line_Types[-7])+   scale_shape_manual(values = Method_Shape_Types[-7])+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":"/articles/Basic_Sampling_Poi_Reg.html","id":"understanding-the-bike-sharing-data","dir":"Articles","previous_headings":"","what":"Understanding the bike sharing data","title":"Model based Sampling for Poisson Regression","text":"Fanaee-T Gama (2013) collected data understand bike sharing demands rental return process. data contains \\(4\\) columns \\(17,379\\) observations, first column response variable rest covariates. consider covariates ) temperature (\\(x_1\\)), b) humidity (\\(x_2\\)) c) wind speed (\\(x_3\\)) model response, number bikes rented hourly. covariates scaled mean zero variance one. given data subsampling methods implemented assuming main effects model can describe data. First observations bike sharing data. Based model methods random sampling, leverage sampling (Ma, Mahoney, Yu 2014; Ma Sun 2015), \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) \\(\\)-optimality sampling response constraint (Zhang, Ning, Ruppert 2021) implemented big data. obtained samples respective model parameter estimates \\(M=100\\) simulations across different sample sizes \\(k=(600,\\ldots,1500)\\). set initial sample size \\(r1=300\\) methods requires random sample. final samples, model parameters assume model estimated. estimated model parameters compared estimated model parameters full big data mean squared error \\(MSE(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ}\\sum_{=1}^M \\sum_{j=1}^J(\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\). , \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter.","code":"# Selecting 100% of the big data and prepare it Original_Data<-cbind(Bike_sharing[,1],1,Bike_sharing[,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Original_Data) %>%    kable(format = \"html\",         caption = \"First few observations of the bike sharing data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(6,18,by=3)*100; rep_k<-rep(k,each=M) # define colors, shapes, line types for the methods Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MC\",                 \"Shrinkage Leverage\",\"Basic Leverage\",\"Unweighted Leverage\",                 \"Random Sampling\") Method_Colour<-c(\"green\",\"green4\",\"yellowgreen\",                 \"red\",\"darkred\",\"maroon\",\"black\") Method_Shape_Types<-c(rep(8,3),rep(4,3),16) Method_Line_Types<-c(rep(\"twodash\",3),rep(\"dotted\",3),\"solid\")"},{"path":"/articles/Basic_Sampling_Poi_Reg.html","id":"random-sampling","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Random sampling","title":"Model based Sampling for Poisson Regression","text":"code implementing sampling methods.","code":"# Random sampling Final_Beta_RS<-matrix(nrow = length(k)*M,ncol = ncol(Original_Data[,-1])+1) for (i in 1:length(rep_k)) {   Temp_Data<-Original_Data[sample(1:N,rep_k[i]),]   glm(Y~.-1,data=Temp_Data,family=\"poisson\")->Results   Final_Beta_RS[i,]<-c(rep_k[i],coefficients(Results))   if(i==length(rep_k)){print(\"All simulations completed for random sampling\")} } ## [1] \"All simulations completed for random sampling\" Final_Beta_RS<-cbind.data.frame(\"Random Sampling\",Final_Beta_RS) colnames(Final_Beta_RS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Poi_Reg.html","id":"leverage-sampling","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Leverage sampling","title":"Model based Sampling for Poisson Regression","text":"","code":"# Leverage sampling  ## we set the shrinkage value of alpha=0.9 NeEDS4BigData::LeverageSampling(r=rep_k,Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),N=N,                                 alpha = 0.9,family = \"poisson\")->Results ## Basic and shrinkage leverage probabilities calculated. ## Sampling completed. Final_Beta_LS<-Results$Beta_Estimates colnames(Final_Beta_LS)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Poi_Reg.html","id":"a--and-l-optimality-subsampling","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"A- and L-optimality subsampling","title":"Model based Sampling for Poisson Regression","text":"","code":"# A- and L-optimality subsampling for GLM NeEDS4BigData::ALoptimalGLMSub(r1=300,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,family = \"poisson\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_ALoptGLM<-Results$Beta_Estimates colnames(Final_Beta_ALoptGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Poi_Reg.html","id":"a-optimality-sampling-with-measurement-constraints","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"A-optimality sampling with measurement constraints","title":"Model based Sampling for Poisson Regression","text":"","code":"# A-optimality sampling without response NeEDS4BigData::AoptimalMCGLMSub(r1=300,r2=rep_k,                                 Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),                                 N=N,family=\"poisson\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_AoptMCGLM<-Results$Beta_Estimates Final_Beta_AoptMCGLM$Method<-rep(\"A-Optimality MC\",nrow(Final_Beta_AoptMCGLM)) colnames(Final_Beta_AoptMCGLM)<-c(\"Method\",\"Sample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Poi_Reg.html","id":"summary","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Summary","title":"Model based Sampling for Poisson Regression","text":"mean squared error plotted . Mean squared error ) sampling methods b) without unweighted leverage sampling.","code":"# Summarising Results of Scenario one Final_Beta<-rbind(Final_Beta_RS,Final_Beta_LS,                   Final_Beta_ALoptGLM,Final_Beta_AoptMCGLM)  # Obtaining the model parameter estimates for the full big data model glm(Y~.-1,data = Original_Data,family=\"poisson\")->All_Results coefficients(All_Results)->All_Beta matrix(rep(All_Beta,by=nrow(Final_Beta)),nrow = nrow(Final_Beta),        ncol = ncol(Final_Beta[,-c(1,2)]),byrow = TRUE)->All_Beta  remove(Final_Beta_RS,Final_Beta_LS,Final_Beta_ALoptGLM,        Final_Beta_AoptMCGLM,Temp_Data,Results,All_Results) # Obtain the mean squared error for the model parameter estimates MSE_Beta<-data.frame(\"Method\"=Final_Beta$Method,                      \"Sample\"=Final_Beta$Sample,                      \"MSE\"=rowSums((All_Beta - Final_Beta[,-c(1,2)])^2))   MSE_Beta$Method<-factor(MSE_Beta$Method,levels = Method_Names,labels = Method_Names)  # Plot for the mean squared error with all methods ggplot(MSE_Beta |> dplyr::group_by(Method,Sample) |>         dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),        aes(x=factor(Sample),y=MSE,color=Method,group=Method,            linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Colour)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  # Plot for the mean squared error with all methods except unweighted leverage ggplot(MSE_Beta[MSE_Beta$Method != \"Unweighted Leverage\",] |>          dplyr::group_by(Method,Sample) |>          dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),        aes(x=factor(Sample),y=MSE,color=Method,group=Method,            linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Colour[-6])+   scale_linetype_manual(values=Method_Line_Types[-6])+   scale_shape_manual(values = Method_Shape_Types[-6])+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  #save(Final_Beta,All_Beta,MSE_Beta,p1,p2,file=\"Poisson_Regression_Scenario_1.RData\") #load(\"Poisson_Regression_Scenario_1.RData\") ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":"/articles/Introduction.html","id":"big-data-analysis","dir":"Articles","previous_headings":"","what":"Big data analysis","title":"Introduction","text":"Big data presents opportunities analysts uncover new knowledge gain new insights real-world problems. However, massive scale complexity presents computational statistical challenges. include scalability issues, storage constraints, noise accumulation, spurious correlations, incidental endogeneity measurement errors. Figure 1, Chen, Mao, Liu (2014) review size big data different sectors business. Addressing challenges demands innovative approaches computation statistics. Traditional methods, effective small moderate sample sizes, often falter confronted massive datasets. Thus, pressing need innovative statistical methodologies computational tools tailored unique demands big data analysis.","code":""},{"path":"/articles/Introduction.html","id":"computational-solutions-for-big-data-analysis","dir":"Articles","previous_headings":"","what":"Computational solutions for big data analysis","title":"Introduction","text":"Computer engineers often seek powerful computing facilities reduce computing time, leading rapid development supercomputers past decade. supercomputers boast speeds storage capacities hundreds even thousands times greater general-purpose PCs. However, significant energy consumption limited accessibility remain major drawbacks. cloud computing offers partial solution providing accessible computing resources, faces challenges related data transfer inefficiency, privacy security concerns. Graphic Processing Units (GPUs) emerged another computational facility, offering powerful parallel computing capabilities. However, recent comparisons shown even high-end GPUs can outperformed general-purpose multi-core processors, primarily due data transfer inefficiencies. summary, neither supercomputers, cloud computing, GPUs efficiently solved big data problem. Instead, growing need efficient statistical solutions can make big data manageable general-purpose PCs.","code":""},{"path":"/articles/Introduction.html","id":"statistical-solutions-for-big-data-analysis","dir":"Articles","previous_headings":"","what":"Statistical solutions for big data analysis","title":"Introduction","text":"realm addressing challenges posed big data, statistical solutions relatively novel compared engineering solutions, new methodologies continually development. Currently available methods can broadly categorized three groups: Sampling: involves selecting representative subset data analysis instead analysing entire dataset. approach can significantly reduce computational requirements still providing valuable insights underlying population. Divide conquer: approach involves breaking large problem smaller, manageable sub problems. sub problem independently analysed, often parallel, combining results obtain final output. Online updating streamed data: statistical inference updated new data arrive sequentially. recent years, growing preference sampling divide recombine methods addressing range regression problems. Meanwhile, online updating primarily utilized streaming data. Furthermore, large dataset unnecessary confidently answer specific question, sampling often favoured, allows analysis using standard methods.","code":""},{"path":"/articles/Introduction.html","id":"sampling-algorithms-for-big-data","dir":"Articles","previous_headings":"","what":"Sampling algorithms for big data","title":"Introduction","text":"literature presents two strategies resolve primary challenge acquire informative subset efficiently addresses specific analytical questions yield results consistent analysing large data set. : Sample randomly large dataset using sampling probabilities determined via assumed statistical model objective (e.g., prediction /parameter estimation) (Wang, Zhu, Ma 2018; Yao Wang 2019; Ai, Wang, et al. 2021; Ai, Yu, et al. 2021; Lee, Schifano, Wang 2021, 2022; Zhang, Ning, Ruppert 2021) Select samples based experimental design (Drovandi et al. 2017; Wang, Yang, Stufken 2019; Cheng, Wang, Yang 2020; Hou-Liu Browne 2023; Reuter Schwabe 2023; Yu, Liu, Wang 2023). now package focus sampling methods Leverage sampling Ma, Mahoney, Yu (2014) Ma Sun (2015). Local case control sampling Fithian Hastie (2015). - L-optimality based subsampling methods Generalised Linear Models Wang, Zhu, Ma (2018) Ai, Yu, et al. (2021). -optimality based subsampling Gaussian Linear Model Lee, Schifano, Wang (2021). - L-optimality based sampling methods Generalised Linear Models response involved probability calculation Zhang, Ning, Ruppert (2021). - L-optimality based model robust/average subsampling methods Generalised Linear Models Mahendran, Thompson, McGree (2023). Sampling Generalised Linear Models potential model misspecification Adewale Wiens (2009) Adewale Xu (2010).","code":""},{"path":[]},{"path":"/articles/Linear_Regression.html","id":"understanding-the-electric-consumption-data","dir":"Articles","previous_headings":"","what":"Understanding the electric consumption data","title":"Linear Regression : Model robust and misspecification","text":"``Electric power consumption’’ data (Hebrail Berard 2012), contains \\(2,049,280\\) measurements house located Sceaux, France December 2006 November 2010. data contains \\(4\\) columns \\(2,049,280\\) observations, first column response variable rest covariates, however use first \\(1\\%\\) data explanation. response \\(y\\) log scaled intensity, covariates active electrical energy ) kitchen (\\(X_1\\)), b) laundry room (\\(X_2\\)) c) water-heater air-conditioner (\\(X_3\\)). covariates scaled mean zero variance one. Given data analysed two different scenarios, model robust average subsampling methods assuming set models can describe data. sampling method assuming main effects model potentially misspecified. First observations electric consumption data.","code":"# Selecting 1% of the big data and prepare it indexes<-1:ceiling(nrow(Electric_consumption)*0.01) Original_Data<-cbind(Electric_consumption[indexes,1],1,                      Electric_consumption[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Original_Data) %>%    kable(format = \"html\",         caption = \"First few observations of the electric consumption data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(6,18,by=3)*100; rep_k<-rep(k,each=M)"},{"path":"/articles/Linear_Regression.html","id":"model-robust-or-average-subsampling","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Model robust or average subsampling","title":"Linear Regression : Model robust and misspecification","text":"method \\(\\)- \\(L\\)-optimality model robust average subsampling (Mahendran, Thompson, McGree 2023) compared \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) method. five different models considered 1) main effects model (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3\\)), 2-4) main effects model squared term covariate (\\(X^2_1 / X^2_2 / X^2_3\\)) 5) main effects model squared terms (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3+\\beta_4X^2_1+\\beta_5X^2_2+\\beta_6X^2_3\\)). model \\(l\\) mean squared error model parameters \\(MSE_l(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ} \\sum_{=1}^M \\sum_{j=1}^J (\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\) calculated \\(M=100\\) simulations across sample sizes \\(k=(600,\\ldots,1500)\\) initial sample size \\(r1=300\\). , \\(l\\)-th model \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter.","code":"# Define the sampling methods and their respective colours, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MR\",\"L-Optimality MR\") Method_Colour<-c(\"red\",\"darkred\",\"green\",\"springgreen4\") Method_Shape_Types<-c(rep(8,2),rep(4,2)) Method_Line_Types<-c(rep(\"twodash\",2),rep(\"dotted\",2))  # Preparing the data for the model average method with squared terms No_of_Variables<-ncol(Original_Data[,-c(1,2)]) Squared_Terms<-paste0(\"X\",1:No_of_Variables,\"^2\") term_no <- 2 All_Models <- list(c(\"X0\",paste0(\"X\",1:No_of_Variables)))  Original_Data_ModelRobust<-cbind(Original_Data,Original_Data[,-c(1,2)]^2) colnames(Original_Data_ModelRobust)<-c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables),                                        paste0(\"X\",1:No_of_Variables,\"^2\"))  for (i in 1:No_of_Variables) {   x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))   for(j in 1:length(x))   {     All_Models[[term_no]] <- c(\"X0\",paste0(\"X\",1:No_of_Variables),x[[j]])     term_no <- term_no+1   } } All_Models<-All_Models[-c(5:7)] names(All_Models)<-paste0(\"Model_\",1:length(All_Models))"},{"path":"/articles/Linear_Regression.html","id":"apriori-probabilities-are-equal","dir":"Articles","previous_headings":"Understanding the electric consumption data > Model robust or average subsampling","what":"Apriori probabilities are equal","title":"Linear Regression : Model robust and misspecification","text":"Consider \\(Q=5\\) model equal priori probability (.e \\(\\alpha_q=1/5,q=1,\\ldots,5\\)). code implementation scenario. Mean squared error models equal apriori order e Model 1 5 across subsampling methods comparison.","code":"All_Covariates<-colnames(Original_Data_ModelRobust)[-1] # A- and L-optimality model robust subsampling for linear regression NeEDS4BigData::modelRobustLinSub(r1=300,r2=rep_k,                                  Y=as.matrix(Original_Data_ModelRobust[,1]),                                  X=as.matrix(Original_Data_ModelRobust[,-1]),N=N,                                  Alpha=rep(1/length(All_Models),length(All_Models)),                                  All_Combinations=All_Models,                                  All_Covariates=All_Covariates)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelRobust<-Results$Beta_Estimates  # Mean squared error and their respective plots for all five models MSE_Beta_MR<-list(); plot_list_MR<-list() for (i in 1:length(All_Models)) {   lm(Y~.-1,data=Original_Data_ModelRobust[,c(\"Y\",All_Models[[i]])])->All_Results   All_Beta<-coefficients(All_Results)      matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),          nrow = nrow(Final_Beta_modelRobust[[i]]),          ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta      data.frame(\"Method\"=Final_Beta_modelRobust[[i]]$Method,              \"Sample\"=Final_Beta_modelRobust[[i]]$r2,              \"MSE\"=rowSums((All_Beta-Final_Beta_modelRobust[[i]][,-c(1,2)])^2))->     MSE_Beta_MR[[i]]      ggplot(MSE_Beta_MR[[i]] |> dplyr::group_by(Method,Sample) |>          dplyr::summarise(MSE=mean(MSE),.groups ='drop'),          aes(x=factor(Sample),y=MSE,color=Method,group=Method,              linetype=Method,shape=Method))+     geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"MSE\")+     scale_color_manual(values = Method_Colour)+     ggtitle(paste0(\"Model \",i))+     scale_linetype_manual(values=Method_Line_Types)+     scale_shape_manual(values= Method_Shape_Types)+     theme_bw()+guides(colour= guide_legend(nrow = 2))+     Theme_special()->plot_list_MR[[i]] }  ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = \"auto\",           common.legend = TRUE,legend = \"bottom\")"},{"path":"/articles/Linear_Regression.html","id":"main-effects-model-is-potentially-misspecified","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Main effects model is potentially misspecified","title":"Linear Regression : Model robust and misspecification","text":"final scenario comparison sampling method assumption main effects model potentially misspecified \\(\\)- \\(L\\)-optimality subsampling method. sampling method accounts potential model misspecification take scaling factor \\(\\alpha=10\\). scenarios number simulations sample sizes stay . compare mean squared error estimated model parameters, however assume model potentially misspecified asymptotic approximation mean squared error predictions calculated well. code implementation. AMSE potentially misspecified main effects model across sampling methods comparison.","code":"# Define the sampling methods and their respective colours, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"RLmAMSE\",                 \"RLmAMSE Log Odds 10\",\"RLmAMSE Power 10\") Method_Colour<-c(\"red\",\"darkred\",\"yellowgreen\",\"green\",\"springgreen4\") Method_Shape_Types<-c(rep(8,2),rep(4,3)) Method_Line_Types<-c(rep(\"twodash\",2),rep(\"dotted\",3))  # For the big data fit the main effects model and estimate the contamination interaction_terms <- combn(colnames(Original_Data[,-1])[-1],2,                            FUN=function(x) paste(x,collapse=\"*\")) as.formula(paste(\"Y ~ \",                  paste(paste0(\"s(X\",1:ncol(Original_Data[,-c(1,2)]),\")\"),                        collapse=\"+\"),\"+\",paste(paste0(\"s(\",interaction_terms,\")\"),                                                collapse=\"+\")))->my_formula  lm(Y~.-1,data=Original_Data)->Results beta.prop<-coefficients(Results) Xbeta_Final<-as.vector(as.matrix(Original_Data[,-1])%*%beta.prop) Var.prop<-sum((Original_Data$Y-Xbeta_Final)^2)/N fit_GAM<-gam::gam(formula = my_formula,data=Original_Data) Xbeta_GAM<-gam::predict.Gam(fit_GAM,newdata = Original_Data[,-1]) f_estimate<-Xbeta_GAM - Xbeta_Final Var_GAM.prop<-sum((Original_Data[,1]-Xbeta_GAM)^2)/N  # A- and L-optimality and RLmAMSE model misspecified sampling for linear regression  NeEDS4BigData::modelMissLinSub(r1=300,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,Alpha=10,Var_GAM_Full = Var_GAM.prop,                                Var_Full=Var.prop,                                F_Estimate_Full = f_estimate)->Results ## Warning: executing %dopar% sequentially: no parallel backend registered ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelMiss<-Results$Beta_Estimates Final_AMSE_modelMiss<-Results$AMSE_Estimates  lm(Y~.-1,data = Original_Data)->All_Results coefficients(All_Results)->All_Beta matrix(rep(All_Beta,by=nrow(Final_Beta_modelMiss)),nrow = nrow(Final_Beta_modelMiss),        ncol = ncol(Final_Beta_modelMiss[,-c(1,2)]),byrow = TRUE)->All_Beta  # Plots for the mean squared error of the model parameter estimates  # and the AMSE for the main effects model  MSE_Beta_modelMiss<-data.frame(\"Method\"=Final_Beta_modelMiss$Method,                                \"Sample\"=Final_Beta_modelMiss$r2,                                \"MSE\"=rowSums((All_Beta -                                                Final_Beta_modelMiss[,-c(1,2)])^2))  ggplot(MSE_Beta_modelMiss |> dplyr::group_by(Method,Sample) |>         dplyr::summarise(MSE=mean(MSE),.groups ='drop'),        aes(x=factor(Sample),y=MSE,color=Method,group=Method,            linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Colour)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  ggplot(Final_AMSE_modelMiss |> dplyr::group_by(r2,Method) |>         dplyr::summarise(meanAMSE=mean(AMSE),.groups ='drop'),        aes(x=factor(r2),y=meanAMSE,color=Method,group=Method,            linetype=Method,shape=Method)) +   geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"Mean AMSE\")+   scale_color_manual(values = Method_Colour)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  p2 #ggarrange(p1,p2,nrow = 1,ncol = 2,labels = \"auto\")"},{"path":[]},{"path":"/articles/Logistic_Regression.html","id":"understanding-the-skin-segmentation-data","dir":"Articles","previous_headings":"","what":"Understanding the skin segmentation data","title":"Logistic Regression : Model robust and misspecification","text":"Rajen Abhinav (2012) addressed challenge detecting skin-like regions images component intricate process facial recognition. achieve goal, ``Skin segmentation data’’ curated, data contains \\(4\\) columns \\(245,057\\) observations, first column response variable rest covariates. Aim logistic regression model classify images skin based ) Red, b) Green c) Blue colour data. Skin presence denoted one skin absence denoted zero. colour vector scaled mean zero variance one (initial range 0−255). Given data analysed two different scenarios, model robust average subsampling methods assuming set models can describe data. sampling method assuming main effects model potentially misspecified. First observations skin segmentation data.","code":"# Selecting 100% of the big data and prepare it indexes<-1:nrow(Skin_segmentation) Original_Data<-cbind(Skin_segmentation[indexes,1],1,Skin_segmentation[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Original_Data) %>%    kable(format = \"html\",         caption = \"First few observations of the skin segmentation data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(6,18,by=3)*100; rep_k<-rep(k,each=M)"},{"path":"/articles/Logistic_Regression.html","id":"model-robust-or-average-subsampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Model robust or average subsampling","title":"Logistic Regression : Model robust and misspecification","text":"method \\(\\)- \\(L\\)-optimality model robust average subsampling (Mahendran, Thompson, McGree 2023) compared \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) method. five different models considered 1) main effects model (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3\\)), 2-4) main effects model squared term covariate (\\(X^2_1 / X^2_2 / X^2_3\\)) 5) main effects model squared terms (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3+\\beta_4X^2_1+\\beta_5X^2_2+\\beta_6X^2_3\\)). model \\(j\\) mean squared error model parameters \\(MSE_l(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ} \\sum_{=1}^M \\sum_{j=1}^J (\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\) calculated \\(M=100\\) simulations across sample sizes \\(k=(600,\\ldots,1500)\\) initial sample size \\(r1=300\\). , \\(l\\)-th model \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter.","code":"# Define the subsampling methods and their respective colours, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MR\",\"L-Optimality MR\") Method_Colour<-c(\"red\",\"darkred\",\"green\",\"springgreen4\") Method_Shape_Types<-c(rep(8,2),rep(4,2)) Method_Line_Types<-c(rep(\"twodash\",2),rep(\"dotted\",2))  # Preparing the data for the model average method with squared terms No_of_Variables<-ncol(Original_Data[,-c(1,2)]) Squared_Terms<-paste0(\"X\",1:No_of_Variables,\"^2\") term_no <- 2 All_Models <- list(c(\"X0\",paste0(\"X\",1:No_of_Variables)))  Original_Data_ModelRobust<-cbind(Original_Data,Original_Data[,-c(1,2)]^2) colnames(Original_Data_ModelRobust)<-c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables),                                        paste0(\"X\",1:No_of_Variables,\"^2\"))  for (i in 1:No_of_Variables) {   x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))   for(j in 1:length(x))   {     All_Models[[term_no]] <- c(\"X0\",paste0(\"X\",1:No_of_Variables),x[[j]])     term_no <- term_no+1   } } All_Models<-All_Models[-c(5:7)] names(All_Models)<-paste0(\"Model_\",1:length(All_Models))"},{"path":"/articles/Logistic_Regression.html","id":"apriori-probabilities-are-equal","dir":"Articles","previous_headings":"Understanding the skin segmentation data > Model robust or average subsampling","what":"Apriori probabilities are equal","title":"Logistic Regression : Model robust and misspecification","text":"Consider \\(Q=5\\) model equal priori probability (.e \\(\\alpha_q=1/5,q=1,\\ldots,5\\)). code implementation scenario. Mean squared error models equal apriori order e Model 1 5 across subsampling methods comparison.","code":"All_Covariates<-colnames(Original_Data_ModelRobust)[-1] # A- and L-optimality model robust subsampling for logistic regression NeEDS4BigData::modelRobustLogSub(r1=300,r2=rep_k,                                  Y=as.matrix(Original_Data_ModelRobust[,1]),                                  X=as.matrix(Original_Data_ModelRobust[,-1]),                                  N=N,Alpha=rep(1/length(All_Models),length(All_Models)),                                  All_Combinations = All_Models,                                  All_Covariates = All_Covariates)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelRobust<-Results$Beta_Estimates  # Mean squared error and their respective plots for all five models MSE_Beta_MR<-list(); plot_list_MR<-list() for (i in 1:length(All_Models)) {   glm(Y~.-1,data=Original_Data_ModelRobust[,c(\"Y\",All_Models[[i]])],       family=\"binomial\")->All_Results   All_Beta<-coefficients(All_Results)    matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),          nrow = nrow(Final_Beta_modelRobust[[i]]),          ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta    MSE_Beta_MR[[i]]<-data.frame(\"Method\"=Final_Beta_modelRobust[[i]]$Method,                                \"Sample\"=Final_Beta_modelRobust[[i]]$r2,                                \"MSE\"=rowSums((All_Beta -                                                Final_Beta_modelRobust[[i]][,-c(1,2)])^2))    ggplot(MSE_Beta_MR[[i]] |> dplyr::group_by(Method,Sample) |>           dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),          aes(x=factor(Sample),y=MSE,color=Method,group=Method,              linetype=Method,shape=Method))+     geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"MSE\")+     scale_color_manual(values = Method_Colour)+     ggtitle(paste0(\"Model \",i))+     scale_linetype_manual(values=Method_Line_Types)+     scale_shape_manual(values = Method_Shape_Types)+     theme_bw()+guides(colour = guide_legend(nrow = 2))+     Theme_special()->plot_list_MR[[i]] }  ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = \"auto\",           common.legend = TRUE,legend = \"bottom\")"},{"path":"/articles/Logistic_Regression.html","id":"main-effects-model-is-potentially-misspecified","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Main effects model is potentially misspecified","title":"Logistic Regression : Model robust and misspecification","text":"final third scenario comparison sampling method assumption main effects model potentially misspecified \\(\\)- \\(L\\)-optimality subsampling method. sampling method accounts potential model misspecification take scaling factor \\(\\alpha=10\\). scenario one two number simulations sample sizes stay . compare mean squared error estimated model parameters, however assume model potentially misspecified asymptotic approximation mean squared error predictions calculated well. code implementation. AMSE potentially misspecified main effects model across sampling methods comparison.","code":"# Define the sampling methods and their respective colours, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"RLmAMSE\",                 \"RLmAMSE Log Odds 10\",\"RLmAMSE Power 10\") Method_Colour<-c(\"red\",\"darkred\",\"yellowgreen\",\"green\",\"springgreen4\") Method_Shape_Types<-c(rep(8,2),rep(4,3)) Method_Line_Types<-c(rep(\"twodash\",2),rep(\"dotted\",3))  # For the big data fit the main effects model and estimate the contamination interaction_terms <- combn(colnames(Original_Data[,-1])[-1],2,                            FUN=function(x)paste(x,collapse=\"*\")) as.formula(paste(\"Y ~ \",                  paste(paste0(\"s(X\",1:ncol(Original_Data[,-c(1,2)]),\")\"),                        collapse=\"+\"),\"+\",paste(paste0(\"s(\",interaction_terms,\")\"),                                                collapse=\" + \")))->my_formula  glm(Y~.-1,data=Original_Data,family=\"binomial\")->Results beta.prop<-coefficients(Results) Xbeta_Final<-as.vector(as.matrix(Original_Data[,-1])%*%beta.prop) fit_GAM<-gam::gam(formula = my_formula,data=Original_Data,family=\"binomial\") Xbeta_GAM<-gam::predict.Gam(fit_GAM,newdata = Original_Data[,-1]) f_estimate<-Xbeta_GAM - Xbeta_Final  # A- and L-optimality and RLmAMSE model misspecified sampling for logistic regression  NeEDS4BigData::modelMissLogSub(r1=300,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,Alpha=10, Beta_Estimate_Full = beta.prop,                                F_Estimate_Full = f_estimate)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelMiss<-Results$Beta_Estimates Final_AMSE_modelMiss<-Results$AMSE_Estimates  matrix(rep(beta.prop,by=nrow(Final_Beta_modelMiss)),nrow = nrow(Final_Beta_modelMiss),        ncol = ncol(Final_Beta_modelMiss[,-c(1,2)]),byrow = TRUE)->All_Beta  # Plots for the mean squared error of the model parameter estimates  # and the AMSE for the main effects model  MSE_Beta_modelMiss<-data.frame(\"Method\"=Final_Beta_modelMiss$Method,                                \"Sample\"=Final_Beta_modelMiss$r2,                                \"MSE\"=rowSums((All_Beta -                                                Final_Beta_modelMiss[,-c(1,2)])^2))   ggplot(MSE_Beta_modelMiss |> dplyr::group_by(Method,Sample) |>         dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),        aes(x=factor(Sample),y=MSE,color=Method,group=Method,            linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Colour)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  ggplot(Final_AMSE_modelMiss |> dplyr::group_by(r2,Method) |>         dplyr::summarise(meanAMSE=mean(AMSE), .groups = 'drop'),        aes(x=factor(r2),y=meanAMSE,color=Method,group=Method,            linetype=Method,shape=Method)) +   geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"Mean AMSE\")+   scale_color_manual(values = Method_Colour)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  # ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\", #           legend = \"bottom\",common.legend = TRUE) p2"},{"path":[]},{"path":"/articles/Poisson_Regression.html","id":"understanding-the-bike-sharing-data","dir":"Articles","previous_headings":"","what":"Understanding the bike sharing data","title":"Poisson Regression : Model robust and misspecification","text":"Fanaee-T Gama (2013) collected data understand bike sharing demands rental return process. data contains \\(4\\) columns \\(17,379\\) observations, first column response variable rest covariates. consider covariates ) temperature (\\(x_1\\)), b) humidity (\\(x_2\\)) c) windspeed (\\(x_3\\)) model response, number bikes rented hourly. covariates scaled mean zero variance one. Given data analysed two different scenarios, model robust average subsampling methods assuming set models can describe data. sampling method assuming main effects model potentially misspecified. First observations bike sharing data.","code":"# Selecting 100% of the big data and prepare it Original_Data<-cbind(Bike_sharing[,1],1,Bike_sharing[,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Original_Data) %>%    kable(format = \"html\",         caption = \"First few observations of the bike sharing data.\") # Setting the sample sizes N<-nrow(Original_Data); M<-250; k<-seq(6,18,by=3)*100; rep_k<-rep(k,each=M)"},{"path":"/articles/Poisson_Regression.html","id":"model-robust-or-average-subsampling","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Model robust or average subsampling","title":"Poisson Regression : Model robust and misspecification","text":"method \\(\\)- \\(L\\)-optimality model robust average subsampling (Mahendran, Thompson, McGree 2023) compared \\(\\)- \\(L\\)-optimality subsampling (Ai et al. 2021; Yao Wang 2021) method. five different models considered 1) main effects model (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3\\)), 2-4) main effects model squared term covariate (\\(X^2_1 / X^2_2 / X^2_3\\)) 5) main effects model squared terms (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3+\\beta_4X^2_1+\\beta_5X^2_2+\\beta_6X^2_3\\)). model \\(j\\) mean squared error model parameters \\(MSE_l(\\tilde{\\beta}_k,\\hat{\\beta})=\\frac{1}{MJ} \\sum_{=1}^M \\sum_{j=1}^J (\\tilde{\\beta}_{k,j} - \\hat{\\beta}_j)^2\\) calculated \\(M=100\\) simulations across sample sizes \\(k=(600,\\ldots,1500)\\) initial sample size \\(r1=300\\). , \\(l\\)-th model \\(\\tilde{\\beta}_k\\) estimated model parameters sample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data, \\(j\\) index model parameter.","code":"# Define the subsampling methods and their respective colours, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MR\",\"L-Optimality MR\") Method_Colour<-c(\"red\",\"darkred\",\"green\",\"springgreen4\") Method_Shape_Types<-c(rep(8,2),rep(4,2)) Method_Line_Types<-c(rep(\"twodash\",2),rep(\"dotted\",2))  # Preparing the data for the model average method with squared terms No_of_Variables<-ncol(Original_Data[,-c(1,2)]) Squared_Terms<-paste0(\"X\",1:No_of_Variables,\"^2\") term_no <- 2 All_Models <- list(c(\"X0\",paste0(\"X\",1:No_of_Variables)))  Original_Data_ModelRobust<-cbind(Original_Data,Original_Data[,-c(1,2)]^2) colnames(Original_Data_ModelRobust)<-c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables),                                        paste0(\"X\",1:No_of_Variables,\"^2\"))  for (i in 1:No_of_Variables) {   x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))   for(j in 1:length(x))   {     All_Models[[term_no]] <- c(\"X0\",paste0(\"X\",1:No_of_Variables),x[[j]])     term_no <- term_no+1   } } All_Models<-All_Models[-c(5:7)] names(All_Models)<-paste0(\"Model_\",1:length(All_Models))"},{"path":"/articles/Poisson_Regression.html","id":"apriori-probabilities-are-equal","dir":"Articles","previous_headings":"Understanding the bike sharing data > Model robust or average subsampling","what":"Apriori probabilities are equal","title":"Poisson Regression : Model robust and misspecification","text":"Consider \\(Q=5\\) model equal priori probability (.e \\(\\alpha_q=1/5,q=1,\\ldots,5\\)). code implementation scenario. Mean squared error models equal apriori order e Model 1 5 across subsampling methods comparison.","code":"All_Covariates<-colnames(Original_Data_ModelRobust)[-1] # A- and L-optimality model robust subsampling for poisson regression NeEDS4BigData::modelRobustPoiSub(r1=300,r2=rep_k,                                  Y=as.matrix(Original_Data_ModelRobust[,1]),                                  X=as.matrix(Original_Data_ModelRobust[,-1]),                                  N=N,Alpha=rep(1/length(All_Models),length(All_Models)),                                  All_Combinations = All_Models,                                  All_Covariates = All_Covariates)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelRobust<-Results$Beta_Estimates  # Mean squared error and their respective plots for all five models MSE_Beta_MR<-list(); plot_list_MR<-list() for (i in 1:length(All_Models)) {   glm(Y~.-1,data=Original_Data_ModelRobust[,c(\"Y\",All_Models[[i]])],       family=\"poisson\")->All_Results   All_Beta<-coefficients(All_Results)    matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),          nrow = nrow(Final_Beta_modelRobust[[i]]),          ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta    MSE_Beta_MR[[i]]<-data.frame(\"Method\"=Final_Beta_modelRobust[[i]]$Method,                                \"Sample\"=Final_Beta_modelRobust[[i]]$r2,                                \"MSE\"=rowSums((All_Beta -                                                Final_Beta_modelRobust[[i]][,-c(1,2)])^2))    ggplot(MSE_Beta_MR[[i]] |> dplyr::group_by(Method,Sample) |>           dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),          aes(x=factor(Sample),y=MSE,color=Method,group=Method,              linetype=Method,shape=Method))+     geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"MSE\")+     scale_color_manual(values = Method_Colour)+     scale_linetype_manual(values=Method_Line_Types)+     scale_shape_manual(values = Method_Shape_Types)+     theme_bw()+guides(colour = guide_legend(nrow = 2))+     Theme_special()->plot_list_MR[[i]] }  ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = \"auto\",           legend = \"bottom\",common.legend = TRUE)"},{"path":"/articles/Poisson_Regression.html","id":"main-effects-model-is-potentially-misspecified","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Main effects model is potentially misspecified","title":"Poisson Regression : Model robust and misspecification","text":"final third scenario comparison sampling method assumption main effects model potentially misspecified \\(\\)- \\(L\\)-optimality subsampling method. sampling method accounts potential model misspecification take scaling factor \\(\\alpha=10\\). scenario one two number simulations sample sizes stay . compare mean squared error estimated model parameters, however assume model potentially misspecified asymptotic approximation mean squared error predictions calculated well. code implementation. AMSE potentially misspecified main effects model across sampling methods comparison.","code":"# Define the sampling methods and their respective colors, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"RLmAMSE\",                 \"RLmAMSE Log Odds 10\",\"RLmAMSE Power 10\") Method_Colour<-c(\"red\",\"darkred\",\"yellowgreen\",\"green\",\"springgreen4\") Method_Shape_Types<-c(rep(8,2),rep(4,3)) Method_Line_Types<-c(rep(\"twodash\",2),rep(\"dotted\",3))  # For the big data fit the main effects model and estimate the contamination interaction_terms <- combn(colnames(Original_Data[,-1])[-1],2,                            FUN=function(x)paste(x,collapse=\"*\")) as.formula(paste(\"Y~\",                  paste(paste0(\"s(X\",1:ncol(Original_Data[,-c(1,2)]),\")\"),                        collapse=\"+\"),\"+\",paste(paste0(\"s(\",interaction_terms,\")\"),                                                collapse=\" + \")))->my_formula glm(Y~.-1,data=Original_Data,family=\"poisson\")->Results beta.prop<-coefficients(Results) Xbeta_Final<-as.vector(as.matrix(Original_Data[,-1])%*%beta.prop) fit_GAM<-gam::gam(formula = my_formula,data=Original_Data,family=\"poisson\") Xbeta_GAM<-gam::predict.Gam(fit_GAM,newdata = Original_Data[,-1]) f_estimate<-Xbeta_GAM - Xbeta_Final  # A- and L-optimality and RLmAMSE model misspecified sampling for poisson regression  NeEDS4BigData::modelMissPoiSub(r1=300,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,Alpha=10, Beta_Estimate_Full = beta.prop,                                F_Estimate_Full = f_estimate)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelMiss<-Results$Beta_Estimates Final_AMSE_modelMiss<-Results$AMSE_Estimates  matrix(rep(beta.prop,by=nrow(Final_Beta_modelMiss)),nrow = nrow(Final_Beta_modelMiss),        ncol = ncol(Final_Beta_modelMiss[,-c(1,2)]),byrow = TRUE)->All_Beta  # Plots for the mean squared error of the model parameter estimates  # and the AMSE for the main effects model  MSE_Beta_modelMiss<-data.frame(\"Method\"=Final_Beta_modelMiss$Method,                                \"Sample\"=Final_Beta_modelMiss$r2,                                \"MSE\"=rowSums((All_Beta -                                                Final_Beta_modelMiss[,-c(1,2)])^2))   ggplot(MSE_Beta_modelMiss |> dplyr::group_by(Method,Sample) |>         dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),        aes(x=factor(Sample),y=MSE,color=Method,group=Method,            linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Colour)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  ggplot(Final_AMSE_modelMiss |> dplyr::group_by(r2,Method) |>           dplyr::summarise(meanAMSE=mean(AMSE), .groups = 'drop'),        aes(x=factor(r2),y=meanAMSE,color=Method,group=Method,linetype=Method,shape=Method)) +   geom_point()+geom_line()+xlab(\"Sample size\")+ylab(\"Mean AMSE\")+   scale_color_manual(values = Method_Colour)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  # ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\", #           common.legend = TRUE,legend = \"bottom\") p2"},{"path":[]},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Amalan Mahendran. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mahendran (2024). NeEDS4BigData: New Experimental Design Based Subsampling Methods Big Data. R package version 1.0.0, https://amalan-constat.github.io/NeEDS4BigData/index.html, https://github.com/Amalan-ConStat/NeEDS4BigData.","code":"@Manual{,   title = {NeEDS4BigData: New Experimental Design Based Subsampling Methods for Big Data},   author = {Amalan Mahendran},   year = {2024},   note = {R package version 1.0.0, https://amalan-constat.github.io/NeEDS4BigData/index.html},   url = {https://github.com/Amalan-ConStat/NeEDS4BigData}, }"},{"path":"/CONDUCT.html","id":null,"dir":"","previous_headings":"","what":"Contributor Code of Conduct","title":"Contributor Code of Conduct","text":"contributors maintainers project, pledge respect people contribute reporting issues, posting feature requests, updating documentation, submitting pull requests patches, activities. committed making participation project harassment-free experience everyone, regardless level experience, gender, gender identity expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion. Examples unacceptable behavior participants include use sexual language imagery, derogatory comments personal attacks, trolling, public private harassment, insults, unprofessional conduct. Project maintainers right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct. Project maintainers follow Code Conduct may removed project team. Instances abusive, harassing, otherwise unacceptable behavior may reported opening issue contacting one project maintainers. Code Conduct adapted Contributor Covenant (http:contributor-covenant.org), version 1.0.0, available http://contributor-covenant.org/version/1/0/0/","code":""},{"path":"/index.html","id":"needs4bigdata-","dir":"","previous_headings":"","what":"NeEDS4BigData - An R package for sampling methods","title":"NeEDS4BigData - An R package for sampling methods","text":"R package “NeEDS4BigData” provides approaches implement sampling methods analyse big data.","code":""},{"path":"/index.html","id":"what-is-needs4bigdata-an-abbreviation-for","dir":"","previous_headings":"","what":"What is “NeEDS4BigData” an abbreviation for?","title":"NeEDS4BigData - An R package for sampling methods","text":"New Experimental Design based Sampling methods Big Data.","code":""},{"path":"/index.html","id":"how-to-engage-with-needs4bigdata-the-first-time-","dir":"","previous_headings":"","what":"How to engage with “NeEDS4BigData” the first time ?","title":"NeEDS4BigData - An R package for sampling methods","text":"","code":"## Installing the package from GitHub devtools::install_github(\"Amalan-ConStat/NeEDS4BigData\")  ## Installing the package from CRAN install.packages(\"NeEDS4BigData\")"},{"path":"/index.html","id":"sampling-methods","dir":"","previous_headings":"","what":"Sampling Methods","title":"NeEDS4BigData - An R package for sampling methods","text":"- L-optimality based subsampling GLMs. -optimality based subsampling Gaussian Linear Models. Leverage sampling GLMs. Local case control sampling logistic regression. -optimality based sampling measurement constraints GLMs. Model robust subsampling method GLMs. Sampling method GLMs model potentially misspecified. seven methods described following articles Introduction - explains need sampling methods. Linear Regression - Model based sampling. Linear Regression - Model robust misspecification. Logistic Regression - Model based sampling. Logistic Regression - Model robust misspecification. Poisson Regression - Model based sampling. Poisson Regression - Model robust misspecification. 2, 4 6 assume main effects model can describe data. 3, 5 7 first consider several models can describe big data, later assume given main effects model misspecified. conditions 2 − 7 explore sampling three given big data sets.","code":""},{"path":[]},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 NeEDS4BigData authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":null,"dir":"Reference","previous_headings":"","what":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"Using function sample big data linear, logistic Poisson regression describe data. Sampling probabilities obtained based - L- optimality criteria.","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"","code":"ALoptimalGLMSub(r1,r2,Y,X,N,family)"},{"path":"/reference/ALoptimalGLMSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data family character value \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"output ALoptimalGLMSub gives list Beta_Estimates estimated model parameters data.frame subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon data.frame subsampling (valid linear regression) Utility_Estimates estimated log scaled Information variance estimated model parameters Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Sample_L-Optimality list indexes initial optimal samples obtained based L-Optimality criteria Sampling_Probability matrix calculated sampling probabilities - L- optimality criteria","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"Two stage subsampling algorithm big data Generalised Linear Models (linear, logistic Poisson regression). First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters sampling probabilities evaluated - L-optimality criteria. estimated sampling probabilities optimal sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. character value provided family three types error message produced.","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"Wang H, Zhu R, Ma P (2018). “Optimal subsampling large sample logistic regression.” Journal American Statistical Association, 113(522), 829--844.  Ai M, Yu J, Zhang H, Wang H (2021). “Optimal subsampling algorithms big data regressions.” Statistica Sinica, 31(2), 749--772.  Yao Y, Wang H (2021). “review optimal subsampling methods massive datasets.” Journal Data Science, 19(1), 151--172.","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A- and L-optimality criteria based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"linear\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(600,50); Original_Data<-Full_Data$Complete_Data;  ALoptimalGLMSub(r1 = r1, r2 = r2,                 Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                 X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                 family = \"linear\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results)   Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"logistic\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(600,50); Original_Data<-Full_Data$Complete_Data;  ALoptimalGLMSub(r1 = r1, r2 = r2,                 Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                 X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                 family = \"logistic\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results)   Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"poisson\" Full_Data<-GenGLMdata(Dist,NULL,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(600,50); Original_Data<-Full_Data$Complete_Data;  ALoptimalGLMSub(r1 = r1, r2 = r2,                 Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                 X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                 family = \"poisson\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results)"},{"path":"/reference/AoptimalGauLMSub.html","id":null,"dir":"Reference","previous_headings":"","what":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"Using function sample big data Gaussian linear regression models describe data. Sampling probabilities obtained based -optimality criteria.","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"","code":"AoptimalGauLMSub(r1,r2,Y,X,N)"},{"path":"/reference/AoptimalGauLMSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"output AoptimalGauLMSub gives list Beta_Estimates estimated model parameters data.frame subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon data.frame subsampling Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Sampling_Probability matrix calculated sampling probabilities -optimality criteria","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"Two stage subsampling algorithm big data Gaussian Linear Model. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters sampling probabilities evaluated -optimality criteria. estimated sampling probabilities optimal sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced.","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"Lee J, Schifano ED, Wang H (2021). “Fast optimal subsampling probability approximation generalized linear models.” Econometrics Statistics.","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A-optimality criteria based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"linear\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,12),50); Original_Data<-Full_Data$Complete_Data;  AoptimalGauLMSub(r1 = r1, r2 = r2,                  Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),                  N = nrow(Original_Data))->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results)"},{"path":"/reference/AoptimalMCGLMSub.html","id":null,"dir":"Reference","previous_headings":"","what":"A-optimality criteria based sampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","title":"A-optimality criteria based sampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"Using function sample big data linear, logistic Poisson regression describe data response \\(y\\) partially unavailable. Sampling probabilities obtained based -optimality criteria.","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A-optimality criteria based sampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"","code":"AoptimalMCGLMSub(r1,r2,Y,X,N,family)"},{"path":"/reference/AoptimalMCGLMSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A-optimality criteria based sampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data family character value \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A-optimality criteria based sampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"output AoptimalMCGLMSub gives list Beta_Estimates estimated model parameters data.frame sampling Variance_Epsilon_Estimates matrix estimated variance epsilon data.frame sampling (valid linear regression) Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Sampling_Probability matrix calculated sampling probabilities -optimality criteria","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A-optimality criteria based sampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"Two stage sampling algorithm big data Generalised Linear Models (linear, logistic Poisson regression) response available sampling probability evaluation. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters sampling probabilities evaluated -optimality criteria. estimated sampling probabilities optimal sample size \\(r_2 \\ge r_1\\) obtained. Finally, optimal sample used model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. character value provided family three types error message produced.","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"A-optimality criteria based sampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"Zhang T, Ning Y, Ruppert D (2021). “Optimal sampling generalized linear models measurement constraints.” Journal Computational Graphical Statistics, 30(1), 106--114.","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A-optimality criteria based sampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"linear\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,12),50); Original_Data<-Full_Data$Complete_Data;  AoptimalMCGLMSub(r1 = r1, r2 = r2,                  Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  family = \"linear\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results)   Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"logistic\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,12),50); Original_Data<-Full_Data$Complete_Data;  AoptimalMCGLMSub(r1 = r1, r2 = r2,                  Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  family = \"logistic\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results)   Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"poisson\" Full_Data<-GenGLMdata(Dist,NULL,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,12),50); Original_Data<-Full_Data$Complete_Data;  AoptimalMCGLMSub(r1 = r1, r2 = r2,                  Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  family = \"poisson\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results)"},{"path":"/reference/Bike_sharing.html","id":null,"dir":"Reference","previous_headings":"","what":"Bike sharing data — Bike_sharing","title":"Bike sharing data — Bike_sharing","text":"Fanaee-T (2013) collected data understand bike sharing demands rental return process. data total contains 17,379 observations consider covariates temperature, humidity windspeed model response, number bikes rented hourly.","code":""},{"path":"/reference/Bike_sharing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bike sharing data — Bike_sharing","text":"","code":"Bike_sharing"},{"path":"/reference/Bike_sharing.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Bike sharing data — Bike_sharing","text":"data frame 4 columns 17,379 rows. Rented_Bikes Number bikes rented hourly Temperature Hourly temperature Humidity Hourly humidity Windspeed Hourly windspeed","code":""},{"path":"/reference/Bike_sharing.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Bike sharing data — Bike_sharing","text":"Extracted Fanaee-T H (2013) Bike Sharing. UCI Machine Learning Repository. Available : doi:10.24432/C5W894","code":""},{"path":"/reference/Bike_sharing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bike sharing data — Bike_sharing","text":"","code":"nrow(Bike_sharing) #> [1] 17379"},{"path":"/reference/Electric_consumption.html","id":null,"dir":"Reference","previous_headings":"","what":"Electric consumption data — Electric_consumption","title":"Electric consumption data — Electric_consumption","text":"Hebrail Berard (2012) described data contains 2,049,280 completed measurements house located Sceaux, France December 2006 November 2010. log scale minute-averaged current intensity selected response covariates active electrical energy (watt-hour) kitchen, laundry room, electric water-heater air-conditioner.","code":""},{"path":"/reference/Electric_consumption.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Electric consumption data — Electric_consumption","text":"","code":"Electric_consumption"},{"path":"/reference/Electric_consumption.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Electric consumption data — Electric_consumption","text":"data frame 4 columns 2,049,280 rows. Intensity Minute-averaged current intensity EE_Kitchen Active electrical energy (watt-hour) kitchen EE_Laundry Active electrical energy (watt-hour) laundry room EE_WH_AC Active electrical energy (watt-hour) electric water-heater air-conditioner","code":""},{"path":"/reference/Electric_consumption.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Electric consumption data — Electric_consumption","text":"Extracted Hebrail G, Berard (2012) Individual Household Electric Power Consumption. UCI Machine Learning Repository. Available : doi:10.24432/C58K54","code":""},{"path":"/reference/Electric_consumption.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Electric consumption data — Electric_consumption","text":"","code":"nrow(Electric_consumption) #> [1] 2049280"},{"path":"/reference/GenGLMdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate data for Generalised Linear Models — GenGLMdata","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"Function simulate big data linear, logistic Poisson regression sampling. Covariate data X Normal Uniform distribution linear regression. Covariate data X Exponential Normal Uniform distribution logistic regression. Covariate data X Normal Uniform distribution Poisson regression.","code":""},{"path":"/reference/GenGLMdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"","code":"GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,family)"},{"path":"/reference/GenGLMdata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"Dist character value distribution \"Normal\" \"Uniform \"Exponential\" Dist_Par list parameters distribution generate data covariate X No_Of_Var number variables Beta vector model parameters, including intercept N big data size family character vector \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/GenGLMdata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"output GenGLMData gives list Basic list outputs based inputs Beta Estimates models Complete_Data matrix Y X","code":""},{"path":"/reference/GenGLMdata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"Big data Generalised Linear Models generated \"linear\", \"logistic\" \"poisson\" regression types. limited covariate data generation linear regression normal uniform distribution, logistic regression exponential, normal uniform Poisson regression normal uniform distribution.","code":""},{"path":"/reference/GenGLMdata.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"Lee Y, Nelder JA (1996). “Hierarchical generalized linear models.” Journal Royal Statistical Society Series B: Statistical Methodology, 58(4), 619--656.","code":""},{"path":"/reference/GenGLMdata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"linear\" Results<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"logistic\" Results<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"poisson\" Results<-GenGLMdata(Dist,NULL,No_Of_Var,Beta,N,Family)"},{"path":"/reference/GenModelMissGLMdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"Function simulate big data Generalised Linear Models model misspecification scenario misspecification type.","code":""},{"path":"/reference/GenModelMissGLMdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"","code":"GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon,N,MisspecificationType,family)"},{"path":"/reference/GenModelMissGLMdata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"No_Of_Var number variables Beta vector model parameters, including intercept Var_Epsilon variance value residuals N big data size MisspecificationType character vector referring different types misspecification family character vector \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/GenModelMissGLMdata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"output GenModelMissGLMdata gives list N big data size Beta list outputs(real estimated) beta values Variance_Epsilon list outputs(real estimated) variance epsilon Xbeta list outputs(real estimated) linear predictor f list outputs(real estimated) misspecification Real_Full_Data matrix Y,X f(x) Full_Data matrix Y X","code":""},{"path":"/reference/GenModelMissGLMdata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"Big data Generalised Linear Models generated \"linear\", \"logistic\" \"poisson\" regression types model misspecification. limited covariate data generation uniform distribution limits \\((-1,1)\\). Different type misspecifications \"Type 1\", \"Type 2 Squared\", \"Type 2 Interaction\", \"Type 3 Squared\" \"Type 3 Interaction\".","code":""},{"path":"/reference/GenModelMissGLMdata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"","code":"No_Of_Var<-2; Beta<-c(-1,2,2,1); Var_Epsilon<-0.5; N<-10000; MisspecificationType <- \"Type 2 Squared\"; family <- \"linear\"  Results<-GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon,N,MisspecificationType,family)  No_Of_Var<-2; Beta<-c(-1,2,2,1); N<-10000; MisspecificationType <- \"Type 2 Squared\"; family <- \"logistic\"  Results<-GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon=NULL,N,MisspecificationType,family)  No_Of_Var<-2; Beta<-c(-1,2,2,1); N<-10000; MisspecificationType <- \"Type 2 Squared\"; family <- \"poisson\"  Results<-GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon=NULL,N,MisspecificationType,family)"},{"path":"/reference/GenModelRobustGLMdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","title":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","text":"Function simulate big data linear, logistic Poisson regression model robust scenario set models. Covariate data X Normal Uniform distribution linear regression. Covariate data X Exponential Normal Uniform distribution logistic regression. Covariate data X Normal Uniform distribution Poisson regression.","code":""},{"path":"/reference/GenModelRobustGLMdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","text":"","code":"GenModelRobustGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,All_Models,family)"},{"path":"/reference/GenModelRobustGLMdata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","text":"Dist character value distribution \"Normal\" \"Uniform Dist_Par list parameters distribution generate data covariate X No_Of_Var number variables Beta vector model parameters, including intercept N big data size All_Models list contains possible models family character vector \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/GenModelRobustGLMdata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","text":"output GenModelRobustGLMdata gives list Basic list outputs based inputs Beta Estimates models Complete_Data matrix Y,X X^2","code":""},{"path":"/reference/GenModelRobustGLMdata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","text":"Big data Generalised Linear Models generated \"linear\", \"logistic\" \"poisson\" regression types. limited covariate data generation linear regression normal uniform distribution, logistic regression exponential, normal uniform Poisson regression normal uniform distribution. given real model data generated data modelled All_Models.","code":""},{"path":"/reference/GenModelRobustGLMdata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1,2); N<-10000 All_Models<-list(Real_Model=c(\"X0\",\"X1\",\"X2\",\"X1^2\"),                  Assumed_Model_1=c(\"X0\",\"X1\",\"X2\"),                  Assumed_Model_2=c(\"X0\",\"X1\",\"X2\",\"X2^2\"),                  Assumed_Model_3=c(\"X0\",\"X1\",\"X2\",\"X1^2\",\"X2^2\")) family<-\"linear\"  Results<-GenModelRobustGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,All_Models,family)  Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1,2); N<-10000 All_Models<-list(Real_Model=c(\"X0\",\"X1\",\"X2\",\"X1^2\"),                  Assumed_Model_1=c(\"X0\",\"X1\",\"X2\"),                  Assumed_Model_2=c(\"X0\",\"X1\",\"X2\",\"X2^2\"),                  Assumed_Model_3=c(\"X0\",\"X1\",\"X2\",\"X1^2\",\"X2^2\")) family = \"logistic\"  Results<-GenModelRobustGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,All_Models,family) #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred  Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,2,1,2); N<-10000 All_Models<-list(Real_Model=c(\"X0\",\"X1\",\"X2\",\"X1^2\"),                  Assumed_Model_1=c(\"X0\",\"X1\",\"X2\"),                  Assumed_Model_2=c(\"X0\",\"X1\",\"X2\",\"X2^2\"),                  Assumed_Model_3=c(\"X0\",\"X1\",\"X2\",\"X1^2\",\"X2^2\")) family = \"poisson\"  Results<-GenModelRobustGLMdata(Dist,Dist_Par=NULL,No_Of_Var,Beta,N,All_Models,family) #> Warning: glm.fit: algorithm did not converge #> Warning: glm.fit: fitted rates numerically 0 occurred #> Warning: glm.fit: fitted rates numerically 0 occurred #> Warning: glm.fit: algorithm did not converge"},{"path":"/reference/LCCsampling.html","id":null,"dir":"Reference","previous_headings":"","what":"Local case control sampling for logistic regression — LCCsampling","title":"Local case control sampling for logistic regression — LCCsampling","text":"Using function sample big data logistic regression describe data. Sampling probabilities obtained based local case control method.","code":""},{"path":"/reference/LCCsampling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Local case control sampling for logistic regression — LCCsampling","text":"","code":"LCCsampling(r1,r2,Y,X,N)"},{"path":"/reference/LCCsampling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Local case control sampling for logistic regression — LCCsampling","text":"r1 sample size initial random sampling r2 sample size local case control sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data","code":""},{"path":"/reference/LCCsampling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Local case control sampling for logistic regression — LCCsampling","text":"output LCCsampling gives list Beta_Estimates estimated model parameters data.frame sampling Utility_Estimates estimated log scaled Information variance estimated model parameters Sample_LCC_Sampling list indexes initial optimal samples obtained based local case control sampling Sampling_Probability vector calculated sampling probabilities local case control sampling","code":""},{"path":"/reference/LCCsampling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Local case control sampling for logistic regression — LCCsampling","text":"Two stage sampling algorithm big data logistic regression. First obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters sampling probabilities evaluated local case control. estimated sampling probabilities optimal sample size \\(r_2 \\ge r_1\\) obtained. Finally, optimal sample used model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced.","code":""},{"path":"/reference/LCCsampling.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Local case control sampling for logistic regression — LCCsampling","text":"Fithian W, Hastie T (2015). “Local case-control sampling: Efficient subsampling imbalanced data sets.” Quality control applied statistics, 60(3), 187--190.","code":""},{"path":"/reference/LCCsampling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Local case control sampling for logistic regression — LCCsampling","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"logistic\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,9,12),50); Original_Data<-Full_Data$Complete_Data;  LCCsampling(r1 = r1, r2 = r2, Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),             X = as.matrix(Original_Data[,-1]),             N = nrow(Original_Data))->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results)"},{"path":"/reference/LeverageSampling.html","id":null,"dir":"Reference","previous_headings":"","what":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"Using function sample big data linear, logistic Poisson regression describe data. Sampling probabilities obtained based basic shrinkage leverage method.","code":""},{"path":"/reference/LeverageSampling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"","code":"LeverageSampling(r,Y,X,N,alpha,family)"},{"path":"/reference/LeverageSampling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"r sample size Y response data Y X covariate data X matrix covariates (first column intercept) N size big data alpha shrinkage factor 0 1 family character vector \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/LeverageSampling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"output LeverageSampling gives list Beta_Estimates estimated model parameters data.frame sampling Variance_Epsilon_Estimates matrix estimated variance epsilon data.frame sampling (valid linear regression) Sample_Basic_Leverage list indexes optimal samples obtained based basic leverage Sample_Shrinkage_Leverage list indexes optimal samples obtained based shrinkage leverage Sampling_Probability matrix calculated sampling probabilities basic shrinkage leverage","code":""},{"path":"/reference/LeverageSampling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"Leverage sampling algorithm big data Generalised Linear Models (linear, logistic Poisson regression). First obtain random sample size \\(min(r)/2\\) estimate model parameters. Using estimated parameters leverage scores evaluated leverage sampling. estimated leverage scores sample size \\(r\\) obtained. Finally, sample size \\(r\\) used model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. \\(0 < \\alpha < 1\\) satisfied error message produced. character vector provided family three types error message produced.","code":""},{"path":"/reference/LeverageSampling.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"Ma P, Mahoney M, Yu B (2014). “statistical perspective algorithmic leveraging.” International conference machine learning, 91--99. PMLR.  Ma P, Sun X (2015). “Leveraging big data regression.” Wiley Interdisciplinary Reviews: Computational Statistics, 7(1), 70--76.","code":""},{"path":"/reference/LeverageSampling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"linear\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r<-rep(100*c(6,10),50); Original_Data<-Full_Data$Complete_Data;  LeverageSampling(r = r, Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  alpha = 0.95,                  family = \"linear\")->Results #> Basic and shrinkage leverage probabilities calculated. #> Sampling completed.  plot_Beta(Results)   Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"logistic\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r<-rep(100*c(6,10),50); Original_Data<-Full_Data$Complete_Data;  LeverageSampling(r = r, Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  alpha = 0.95,                  family = \"logistic\")->Results #> Basic and shrinkage leverage probabilities calculated. #> Sampling completed.  plot_Beta(Results)   Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,0.5,0.5); N<-5000; Family<-\"poisson\" Full_Data<-GenGLMdata(Dist,NULL,No_Of_Var,Beta,N,Family)  r<-rep(100*c(6,10),50); Original_Data<-Full_Data$Complete_Data;  LeverageSampling(r = r, Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  alpha = 0.95,                  family = \"poisson\")->Results #> Basic and shrinkage leverage probabilities calculated. #> Sampling completed.  plot_Beta(Results)"},{"path":"/reference/modelMissLinSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Sampling under linear regression for a potentially misspecified model — modelMissLinSub","title":"Sampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"Using function sample big data linear regression potentially misspecified model. Sampling probabilities obtained based - L- optimality criteria RLmAMSE (Reduction Loss minimizing Average Mean Squared Error).","code":""},{"path":"/reference/modelMissLinSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"","code":"modelMissLinSub(r1,r2,Y,X,N,Alpha,Var_GAM_Full,Var_Full,F_Estimate_Full)"},{"path":"/reference/modelMissLinSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha scaling factor using Log Odds Power functions magnify probabilities Var_GAM_Full estimate Var_Epsilon fitting GAM model Var_Full estimate Var_Epsilon fitting linear regression model F_Estimate_Full estimate f difference linear predictor GAM linear model","code":""},{"path":"/reference/modelMissLinSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"output modelMissLinSub gives list Beta_Estimates estimated model parameters sampling Variance_Epsilon_Estimates matrix estimated variance epsilon sampling AMSE_Estimates matrix estimated AMSE values sampling Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Sample_L-Optimality list indexes initial optimal samples obtained based L-Optimality criteria Sample_RLmAMSE list indexes optimal samples obtained based obtained based RLmAMSE Sample_RLmAMSE_Log_Odds list indexes optimal samples obtained based RLmAMSE Log Odds function Sample_RLmAMSE_Power list indexes optimal samples obtained based RLmAMSE Power function Sampling_Probability matrix calculated sampling probabilities","code":""},{"path":"/reference/modelMissLinSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"Two stage sampling algorithm big data linear regression potential model misspecification. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters sampling probabilities evaluated -, L-optimality criteria, RLmAMSE enhanced RLmAMSE (log-odds power) sampling methods. estimated sampling probabilities sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated - L-optimality, RLmAMSE enhanced RLmAMSE (log-odds power) optimal sample used. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\),F_estimate_Full aligned error message produced. \\(\\alpha > 1\\) scaling factor satisfied error message produced.","code":""},{"path":"/reference/modelMissLinSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"Adewale AJ, Wiens DP (2009). “Robust designs misspecified logistic models.” Journal Statistical Planning Inference, 139(1), 3--15.  Adewale AJ, Xu X (2010). “Robust designs generalized linear models possible overdispersion misspecified link functions.” Computational statistics & data analysis, 54(4), 875--890.","code":""},{"path":"/reference/modelMissLinSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sampling under linear regression for a potentially misspecified model — modelMissLinSub","text":"","code":"No_Of_Var<-2; Beta<-c(-1,2,2,1); Var_Epsilon<-0.5; N<-10000; MisspecificationType <- \"Type 2 Squared\"; family <- \"linear\"  Full_Data<-GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon,N,MisspecificationType,family)  r1<-300; r2<-rep(100*c(6,9),50); Original_Data<-Full_Data$Full_Data;  # cl <- parallel::makeCluster(4) # doParallel::registerDoParallel(cl) if (FALSE) { Results<-modelMissLinSub(r1 = r1, r2 = r2,                          Y = as.matrix(Original_Data[,1]),                          X = as.matrix(Original_Data[,-1]),                          N = Full_Data$N,                          Alpha = 10 ,                          Var_GAM_Full = Full_Data$Variance_Epsilon$Real_GAM,                          Var_Full = Full_Data$Variance_Epsilon$Estimate,                          F_Estimate_Full = Full_Data$f$Real_GAM)  # parallel::stopCluster(cl)  plot_Beta(Results) plot_AMSE(Results) }"},{"path":"/reference/modelMissLogSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Sampling under logistic regression for a potentially misspecified model — modelMissLogSub","title":"Sampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"Using function sample big data logistic regression potentially misspecified model. Sampling probabilities obtained based - L- optimality criteria RLmAMSE (Reduction Loss minimizing Average Mean Squared Error).","code":""},{"path":"/reference/modelMissLogSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"","code":"modelMissLogSub(r1,r2,Y,X,N,Alpha,Beta_Estimate_Full,F_Estimate_Full)"},{"path":"/reference/modelMissLogSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha scaling factor using Log Odds Power functions magnify probabilities Beta_Estimate_Full estimate Beta fitting logistic model F_Estimate_Full estimate f difference linear predictor GAM logistic model","code":""},{"path":"/reference/modelMissLogSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"output modelMissLogSub gives list Beta_Estimates estimated model parameters sampling AMSE_Estimates matrix estimated AMSE values sampling Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Sample_L-Optimality list indexes initial optimal samples obtained based L-Optimality criteria Sample_RLmAMSE list indexes optimal samples obtained based RLmAMSE Sample_RLmAMSE_Log_Odds list indexes optimal samples obtained based RLmAMSE Log Odds function Sample_RLmAMSE_Power list indexes optimal samples obtained based RLmAMSE Power function Sampling_Probability matrix calculated sampling probabilities","code":""},{"path":"/reference/modelMissLogSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"Two stage sampling algorithm big data logistic regression potential model misspecification. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters sampling probabilities evaluated -, L-optimality criteria, RLmAMSE enhanced RLmAMSE(log-odds power) sampling methods. estimated sampling probabilities sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated - L-optimality, RLmAMSE enhanced RLmAMSE (log-odds power) optimal sample used. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\),F_estimate_Full aligned error message produced. \\(\\alpha > 1\\) scaling vector satisfied error message produced.","code":""},{"path":"/reference/modelMissLogSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"Adewale AJ, Wiens DP (2009). “Robust designs misspecified logistic models.” Journal Statistical Planning Inference, 139(1), 3--15.  Adewale AJ, Xu X (2010). “Robust designs generalized linear models possible overdispersion misspecified link functions.” Computational statistics & data analysis, 54(4), 875--890.","code":""},{"path":"/reference/modelMissLogSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sampling under logistic regression for a potentially misspecified model — modelMissLogSub","text":"","code":"No_Of_Var<-2; Beta<-c(-1,2,2,1); N<-10000; MisspecificationType <- \"Type 2 Squared\"; family <- \"logistic\"  Full_Data<-GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon=NULL,N,MisspecificationType,family)  r1<-300; r2<-rep(100*c(6,9),50); Original_Data<-Full_Data$Full_Data;  # cl <- parallel::makeCluster(4) # doParallel::registerDoParallel(cl) if (FALSE) { Results<-modelMissLogSub(r1 = r1, r2 = r2,                          Y = as.matrix(Original_Data[,1]),                          X = as.matrix(Original_Data[,-1]),                          N = Full_Data$N,                          Alpha = 10,                          Beta_Estimate_Full = Full_Data$Beta$Estimate,                          F_Estimate_Full = Full_Data$f$Real_GAM)  # parallel::stopCluster(cl)  plot_Beta(Results) plot_AMSE(Results) }"},{"path":"/reference/modelMissPoiSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Sampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","title":"Sampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"Using function sample big data Poisson regression potentially misspecified model. Sampling probabilities obtained based - L- optimality criteria RLmAMSE (Reduction Loss minimizing Average Mean Squared Error).","code":""},{"path":"/reference/modelMissPoiSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"","code":"modelMissPoiSub(r1,r2,Y,X,N,Alpha,Beta_Estimate_Full,F_Estimate_Full)"},{"path":"/reference/modelMissPoiSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha scaling factor using Log Odds Power functions magnify probabilities Beta_Estimate_Full estimate Beta fitting Poisson model F_Estimate_Full estimate f difference linear predictor GAM Poisson model","code":""},{"path":"/reference/modelMissPoiSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"output modelMissPoiSub gives list Beta_Estimates estimated model parameters sampling AMSE_Estimates matrix estimated AMSE values sampling Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Sample_L-Optimality list indexes initial optimal samples obtained based L-Optimality criteria Sample_RLmAMSE list indexes optimal samples obtained based RLmAMSE Sample_RLmAMSE_Log_Odds list indexes optimal samples obtained based RLmAMSE Log Odds function Sample_RLmAMSE_Power list indexes optimal samples obtained based RLmAMSE Power function Sampling_Probability matrix calculated sampling probabilities","code":""},{"path":"/reference/modelMissPoiSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"Two stage sampling algorithm big data Poisson regression potential model misspecification. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters sampling probabilities evaluated -, L-optimality criteria, RLmAMSE enhanced RLmAMSE (log-odds power) sampling methods. estimated sampling probabilities sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated - L-optimality, RLmAMSE enhanced RLmAMSE (log-odds power) optimal sample used. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\),F_estimate_Full aligned error message produced. \\(\\alpha > 1\\) scaling vector satisfied error message produced.","code":""},{"path":"/reference/modelMissPoiSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"Adewale AJ, Wiens DP (2009). “Robust designs misspecified logistic models.” Journal Statistical Planning Inference, 139(1), 3--15.  Adewale AJ, Xu X (2010). “Robust designs generalized linear models possible overdispersion misspecified link functions.” Computational statistics & data analysis, 54(4), 875--890.","code":""},{"path":"/reference/modelMissPoiSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sampling under Poisson regression for a potentially misspecified model — modelMissPoiSub","text":"","code":"No_Of_Var<-2; Beta<-c(-1,2,2,1); N<-10000; MisspecificationType <- \"Type 2 Squared\"; family <- \"poisson\"  Full_Data<-GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon=NULL,N,MisspecificationType,family)  r1<-300; r2<-rep(100*c(6,9),50); Original_Data<-Full_Data$Full_Data;  # cl <- parallel::makeCluster(4) # doParallel::registerDoParallel(cl) if (FALSE) { Results<-modelMissPoiSub(r1 = r1, r2 = r2,                          Y = as.matrix(Original_Data[,1]),                          X = as.matrix(Original_Data[,-1]),                          N = Full_Data$N,                          Alpha = 10,                          Beta_Estimate_Full = Full_Data$Beta$Estimate,                          F_Estimate_Full = Full_Data$f$Real_GAM)  # parallel::stopCluster(cl)  plot_Beta(Results) plot_AMSE(Results) }"},{"path":"/reference/modelRobustLinSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"Using function sample big data linear regression one model describe data. Sampling probabilities obtained based - L- optimality criteria.","code":""},{"path":"/reference/modelRobustLinSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"","code":"modelRobustLinSub(r1,r2,Y,X,N,Alpha,All_Combinations,All_Covariates)"},{"path":"/reference/modelRobustLinSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha vector alpha values used obtain model robust sampling probabilities All_Combinations list possible models can describe data All_Covariates covariates models","code":""},{"path":"/reference/modelRobustLinSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"output modelRobustLinSub gives list Beta_Estimates estimated model parameters model list subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon model subsampling Sample_A-Optimality list indexes initial optimal samples obtained based -Optimality criteria Sample_A-Optimality_MR list indexes initial model robust optimal samples obtained based -Optimality criteria Sample_L-Optimality list indexes initial optimal samples obtained based L-Optimality criteria Sample_L-Optimality_MR list indexes initial model robust optimal samples obtained based L-Optimality criteria Sampling_Probability matrix calculated sampling probabilities - L- optimality criteria","code":""},{"path":"/reference/modelRobustLinSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"Two stage subsampling algorithm big data linear regression multiple models can describe big data. First stage obtain random sample size \\(r_1\\) estimate model parameters models. Using estimated parameters sampling probabilities evaluated -, L-optimality criteria model averaging -, L-optimality subsampling methods. estimated sampling probabilities sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated models. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. \\(0 < \\alpha < 1\\) priori probabilities satisfied error message produced.","code":""},{"path":"/reference/modelRobustLinSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"Mahendran , Thompson H, McGree JM (2023). “model robust subsampling approach Generalised Linear Models big data settings.” Statistical Papers, 64(4), 1137--1157.","code":""},{"path":"/reference/modelRobustLinSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model robust optimal subsampling for A- and L- optimality criteria under linear regression — modelRobustLinSub","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1,2); N<-10000 All_Models<-list(Real_Model=c(\"X0\",\"X1\",\"X2\",\"X1^2\"),                  Assumed_Model_1=c(\"X0\",\"X1\",\"X2\"),                  Assumed_Model_2=c(\"X0\",\"X1\",\"X2\",\"X2^2\"),                  Assumed_Model_3=c(\"X0\",\"X1\",\"X2\",\"X1^2\",\"X2^2\")) family = \"linear\"  Full_Data<-GenModelRobustGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,All_Models,family)  r1<-300; r2<-rep(100*c(6,12),50); Original_Data<-Full_Data$Complete_Data;  modelRobustLinSub(r1 = r1, r2 = r2,                   Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                   X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                   Alpha = rep(1/length(All_Models),length(All_Models)),                   All_Combinations = All_Models,                   All_Covariates = colnames(Original_Data)[-1])->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  Beta_Plots<-plot_Beta(Results)"},{"path":"/reference/modelRobustLogSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"Using function sample big data logistic regression one model describe data. Sampling probabilities obtained based - L- optimality criteria.","code":""},{"path":"/reference/modelRobustLogSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"","code":"modelRobustLogSub(r1,r2,Y,X,N,Alpha,All_Combinations,All_Covariates)"},{"path":"/reference/modelRobustLogSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha vector alpha values used obtain model robust sampling probabilities All_Combinations list possible models can describe data All_Covariates covariates models","code":""},{"path":"/reference/modelRobustLogSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"output modelRobustLinSub gives list Beta_Data estimated model parameters model list subsampling Utility_Data estimated Variance Information model parameters subsampling Sample_L-optimality list indexes initial optimal samples obtained based L-optimality criteria Sample_L-optimality_MR list indexes initial model robust optimal samples obtained based L-optimality criteria Sample_A-optimality list indexes initial optimal samples obtained based -optimality criteria Sample_A-optimality_MR list indexes initial model robust optimal samples obtained based -optimality criteria Sampling_Probability matrix calculated sampling probabilities - L- optimality criteria","code":""},{"path":"/reference/modelRobustLogSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"Two stage subsampling algorithm big data logistic regression multiple models can describe big data. First stage obtain random sample size \\(r_1\\) estimate model parameters models. Using estimated parameters sampling probabilities evaluated -, L-optimality criteria model averaging -, L-optimality subsampling methods. estimated sampling probabilities sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated models. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. \\(0 < \\alpha < 1\\) priori probabilities satisfied error message produced.","code":""},{"path":"/reference/modelRobustLogSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"Mahendran , Thompson H, McGree JM (2023). “model robust subsampling approach Generalised Linear Models big data settings.” Statistical Papers, 64(4), 1137--1157.","code":""},{"path":"/reference/modelRobustLogSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model robust optimal subsampling for A- and L- optimality criteria under logistic regression — modelRobustLogSub","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1,2); N<-10000 All_Models<-list(Real_Model=c(\"X0\",\"X1\",\"X2\",\"X1^2\"),                  Assumed_Model_1=c(\"X0\",\"X1\",\"X2\"),                  Assumed_Model_2=c(\"X0\",\"X1\",\"X2\",\"X2^2\"),                  Assumed_Model_3=c(\"X0\",\"X1\",\"X2\",\"X1^2\",\"X2^2\")) family = \"logistic\"  Full_Data<-GenModelRobustGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,All_Models,family) #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred  r1<-300; r2<-rep(100*c(6,9,12),50); Original_Data<-Full_Data$Complete_Data;  modelRobustLogSub(r1 = r1, r2 = r2,                   Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                   X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                   Alpha = rep(1/length(All_Models),length(All_Models)),                   All_Combinations = All_Models,                   All_Covariates = colnames(Original_Data)[-1])->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  Beta_Plots<-plot_Beta(Results)"},{"path":"/reference/modelRobustPoiSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"Using function sample big data Poisson regression one model describe data. Sampling probabilities obtained based - L- optimality criteria.","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"","code":"modelRobustPoiSub(r1,r2,Y,X,N,Alpha,All_Combinations,All_Covariates)"},{"path":"/reference/modelRobustPoiSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"r1 sample size initial random sampling r2 sample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha vector alpha values used obtain model robust sampling probabilities All_Combinations list possible models can describe data All_Covariates covariates models","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"output modelRobustLinSub gives list Beta_Data estimated model parameters model list subsampling Utility_Data estimated Variance Information model parameters subsampling Sample_L-optimality list indexes initial optimal samples obtained based L-optimality criteria Sample_L-optimality_MR list indexes initial model robust optimal samples obtained based L-optimality criteria Sample_A-optimality list indexes initial optimal samples obtained based -optimality criteria Sample_A-optimality_MR list indexes initial model robust optimal samples obtained based -optimality criteria Sampling_Probability matrix calculated sampling probabilities - L- optimality criteria","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"Two stage subsampling algorithm big data Poisson regression multiple models can describe big data. First stage obtain random sample size \\(r_1\\) estimate model parameters models. Using estimated parameters sampling probabilities evaluated -, L-optimality criteria model averaging -, L-optimality subsampling methods. estimated sampling probabilities sample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated models. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. \\(0 < \\alpha < 1\\) priori probabilities satisfied error message produced.","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"Mahendran , Thompson H, McGree JM (2023). “model robust subsampling approach Generalised Linear Models big data settings.” Statistical Papers, 64(4), 1137--1157.","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model robust optimal subsampling for A- and L- optimality criteria under Poisson regression — modelRobustPoiSub","text":"","code":"Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,0.25,0.1); N<-10000 All_Models<-list(Real_Model=c(\"X0\",\"X1\",\"X2\"),                  Assumed_Model_1=c(\"X0\",\"X1\",\"X2\",\"X1^2\"),                  Assumed_Model_2=c(\"X0\",\"X1\",\"X2\",\"X2^2\"),                  Assumed_Model_3=c(\"X0\",\"X1\",\"X2\",\"X1^2\",\"X2^2\")) family = \"poisson\"  Full_Data<-GenModelRobustGLMdata(Dist,Dist_Par=NULL,No_Of_Var,Beta,N,All_Models,family)  r1<-300; r2<-rep(1200,50); Original_Data<-Full_Data$Complete_Data;  modelRobustPoiSub(r1 = r1, r2 = r2,                   Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                   X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                   Alpha = rep(1/length(All_Models),length(All_Models)),                   All_Combinations = All_Models,                   All_Covariates = colnames(Original_Data)[-1])->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  Beta_Plots<-plot_Beta(Results)"},{"path":"/reference/plot_AMSE.html","id":null,"dir":"Reference","previous_headings":"","what":"Plotting AMSE outputs for the samples under model misspecification — plot_AMSE","title":"Plotting AMSE outputs for the samples under model misspecification — plot_AMSE","text":"using sampling methods potential model misspecification obtain respective AMSE values predictions. summarised plots .","code":""},{"path":"/reference/plot_AMSE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plotting AMSE outputs for the samples under model misspecification — plot_AMSE","text":"","code":"plot_AMSE(object)"},{"path":"/reference/plot_AMSE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plotting AMSE outputs for the samples under model misspecification — plot_AMSE","text":"object object sampling sampling function potential model misspecification","code":""},{"path":"/reference/plot_AMSE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plotting AMSE outputs for the samples under model misspecification — plot_AMSE","text":"output faceted ggplot result","code":""},{"path":"/reference/plot_AMSE.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plotting AMSE outputs for the samples under model misspecification — plot_AMSE","text":"- L-optimality criteria RLmAMSE sampling Generalised Linear Models potential model misspecification facets variance bias^2 AMSE values.","code":""},{"path":"/reference/plot_Beta.html","id":null,"dir":"Reference","previous_headings":"","what":"Plotting model parameter outputs after sampling — plot_Beta","title":"Plotting model parameter outputs after sampling — plot_Beta","text":"using sampling methods mostly obtain estimated model parameter estimates. , summarised histogram plots.","code":""},{"path":"/reference/plot_Beta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plotting model parameter outputs after sampling — plot_Beta","text":"","code":"plot_Beta(object)"},{"path":"/reference/plot_Beta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plotting model parameter outputs after sampling — plot_Beta","text":"object object sampling sampling functions","code":""},{"path":"/reference/plot_Beta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plotting model parameter outputs after sampling — plot_Beta","text":"output faceted ggplot result","code":""},{"path":"/reference/plot_Beta.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plotting model parameter outputs after sampling — plot_Beta","text":"local case control sampling facets sample sizes beta values. leverage sampling facets sample sizes beta values. - L-optimality criteria subsampling Generalised Linear Models facets sample sizes beta values. -optimality criteria subsampling Gaussian Linear Models facets sample sizes beta values. -optimality criteria sampling Generalised Linear Models response variable inclusive facets sample sizes beta values. - L-optimality criteria subsampling Generalised Linear Models multiple models can describe data facets sample sizes beta values. - L-optimality criteria LmAMSE sampling Generalised Linear Models potential model misspecification facets sample sizes beta values.","code":""},{"path":"/reference/plot_LmAMSE.html","id":null,"dir":"Reference","previous_headings":"","what":"Plotting loss function or mAMSE outputs for the subsamples under model misspecification — plot_LmAMSE","title":"Plotting loss function or mAMSE outputs for the subsamples under model misspecification — plot_LmAMSE","text":"using subsampling methods potential model misspecification obtain respective loss mAMSE values. summarised plots .","code":""},{"path":"/reference/plot_LmAMSE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plotting loss function or mAMSE outputs for the subsamples under model misspecification — plot_LmAMSE","text":"","code":"plot_LmAMSE(object)"},{"path":"/reference/plot_LmAMSE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plotting loss function or mAMSE outputs for the subsamples under model misspecification — plot_LmAMSE","text":"object object subsampling subsampling function potential model misspecification","code":""},{"path":"/reference/plot_LmAMSE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plotting loss function or mAMSE outputs for the subsamples under model misspecification — plot_LmAMSE","text":"output faceted ggplot result","code":""},{"path":"/reference/plot_LmAMSE.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plotting loss function or mAMSE outputs for the subsamples under model misspecification — plot_LmAMSE","text":"- L-optimality criterion LmAMSE subsampling Generalised Linear Models potential model misspecification facets variance bias^2 mAMSE values.","code":""},{"path":"/reference/plot_Utility.html","id":null,"dir":"Reference","previous_headings":"","what":"Plotting A- and D-optimality for model parameter (Beta) outputs of the subsamples — plot_Utility","title":"Plotting A- and D-optimality for model parameter (Beta) outputs of the subsamples — plot_Utility","text":"using subsampling methods mostly obtain estimated model parameter estimates. estimates can obtain respective - D-optimality values summarised plots . noted - D-optimality represents model parameter's tr(Variance) log(det(Information)), respectively.","code":""},{"path":"/reference/plot_Utility.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plotting A- and D-optimality for model parameter (Beta) outputs of the subsamples — plot_Utility","text":"","code":"plot_Utility(object)"},{"path":"/reference/plot_Utility.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plotting A- and D-optimality for model parameter (Beta) outputs of the subsamples — plot_Utility","text":"object object subsampling subsampling functions","code":""},{"path":"/reference/plot_Utility.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plotting A- and D-optimality for model parameter (Beta) outputs of the subsamples — plot_Utility","text":"output faceted ggplot result","code":""},{"path":"/reference/plot_Utility.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plotting A- and D-optimality for model parameter (Beta) outputs of the subsamples — plot_Utility","text":"- L-optimality criterion subsampling Generalised Linear Models facets model parameter variance information. - L-optimality criterion subsampling Generalised Linear Models multiple models can describe data facets model parameter variance information.","code":""},{"path":"/reference/Skin_segmentation.html","id":null,"dir":"Reference","previous_headings":"","what":"Skin segmentation data — Skin_segmentation","title":"Skin segmentation data — Skin_segmentation","text":"Rajen Abhinav (2012) addressed challenge detecting skin-like regions images component intricate process facial recognition. achieve goal, curated “Skin segmentation” data set, comprising RGB (R-red, G-green, B-blue) values randomly selected pixels N = 245,057 facial images, including 50,859 skin samples 194,198 nonskin samples, spanning diverse age groups, racial backgrounds, genders.","code":""},{"path":"/reference/Skin_segmentation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Skin segmentation data — Skin_segmentation","text":"","code":"Skin_segmentation"},{"path":"/reference/Skin_segmentation.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Skin segmentation data — Skin_segmentation","text":"data frame 4 columns 245,057 rows. Skin_presence Skin presence randomly selected pixels Red Red values randomly selected pixels Green Green values randomly selected pixels Blue Blue values randomly selected pixels","code":""},{"path":"/reference/Skin_segmentation.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Skin segmentation data — Skin_segmentation","text":"Extracted Rajen B, Abhinav D (2012) Skin segmentation. UCI Machine Learning Repository. Available : doi:10.24432/C5T30C","code":""},{"path":"/reference/Skin_segmentation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Skin segmentation data — Skin_segmentation","text":"","code":"nrow(Skin_segmentation) #> [1] 245057"},{"path":"/news/index.html","id":"needs4bigdata-100","dir":"Changelog","previous_headings":"","what":"NeEDS4BigData 1.0.0","title":"NeEDS4BigData 1.0.0","text":"Initial CRAN submission.","code":""}]
