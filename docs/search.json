[{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"understanding-the-electric-consumption-data","dir":"Articles","previous_headings":"","what":"Understanding the electric consumption data","title":"Basic Sampling for Linear Regression","text":"data contains \\(4\\) columns \\(2,049,280\\) observations, first column response variable rest covariates, however use \\(5\\%\\) data explanation. response \\(y\\) log scaled intensity, covariates active electrical energy ) kitchen (\\(X_1\\)), b) laundry room (\\(X_2\\)) c) water-heater air-conditioner (\\(X_3\\)). covariates scaled mean \\(\\mu=0\\) variance \\(\\sigma^2=1\\). given data subsampling methods implemented assuming main effects model can describe data. First five observations electric consumption data. focus assuming main effects model intercept can appropriately describe big data. Based model methods random sampling, leverage sampling, \\(\\)-optimality sampling Gaussian Linear Model, \\(\\)- \\(L\\)-optimality subsampling \\(\\)-optimality subsampling response constraint implemented big data. obtained subsamples respective model parameter estimates \\(M=100\\) simulations across different subsample sizes \\(k=(600,\\ldots,1500)\\). set initial subsample size \\(r1=300\\) methods requires random sample. final subsamples respective model parameters estimated. estimated model parameters compared estimated model parameters full big data mean squared error \\(MSE(\\tilde{\\beta}_k,\\hat{\\beta})=\\sum_{=1}^M (\\tilde{\\beta}_k - \\hat{\\beta})^2/M\\). , \\(\\tilde{\\beta}_k\\) estimated model parameters subsample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data. code implementing subsampling methods.","code":"indexes<-sample(1:nrow(Electric_consumption),nrow(Electric_consumption)*0.05) Original_Data<-cbind(Electric_consumption[indexes,1],1,Electric_consumption[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Original_Data) %>%    kable(format = \"html\",         caption = \"First five observations of the electric consumption data.\") # Setting the subsample sizes N<-nrow(Original_Data); M<-200; k<-c(6:15)*100; rep_k<-rep(k,each=M) # define colors, shapes, line types for the methods Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality GauLM\",\"A-Optimality MC\",                 \"Shrinkage Leverage\",\"Basic Leverage\",\"Unweighted Leverage\",\"Random Sampling\") Method_Color<-c(\"green\",\"green4\",\"palegreen\",\"springgreen4\",                 \"red\",\"darkred\",\"maroon\",\"black\") Method_Shape_Types<-c(rep(8,4),rep(4,3),16) Method_Line_Types<-c(rep(\"twodash\",4),rep(\"dotted\",3),\"solid\")"},{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"random-sampling","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Random sampling","title":"Basic Sampling for Linear Regression","text":"","code":"# Random sampling Final_Beta_RS<-matrix(nrow = length(k)*M,ncol = ncol(Original_Data[,-1])+1) for (i in 1:length(rep_k)) {   Temp_Data<-Original_Data[sample(1:N,rep_k[i]),]   lm(Y~.-1,data=Temp_Data)->Results   Final_Beta_RS[i,]<-c(rep_k[i],coefficients(Results))   if(i==length(rep_k)){print(\"All simulations completed for random sampling\")} } ## [1] \"All simulations completed for random sampling\" Final_Beta_RS<-cbind.data.frame(\"Random Sampling\",Final_Beta_RS) colnames(Final_Beta_RS)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"leverage-sampling","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Leverage sampling","title":"Basic Sampling for Linear Regression","text":"","code":"# Leverage sampling ## we set the shrinkage value of alpha=0.9 NeEDS4BigData::LeverageSampling(r=rep_k,Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),N=N,                                 alpha = 0.9,family = \"linear\")->Results ## Basic and shrinkage leverage probabilities calculated. ## Subsampling completed. Final_Beta_LS<-Results$Beta_Estimates colnames(Final_Beta_LS)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"a-optimality-subsampling-for-gaussian-linear-model","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"A-optimality subsampling for Gaussian Linear Model","title":"Basic Sampling for Linear Regression","text":"","code":"# A-optimality subsampling for Gaussian Linear Model NeEDS4BigData::AoptimalGauLMSub(r1=300,r2=,rep_k,                                 Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),N=N)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_AoptGauLM<-Results$Beta_Estimates Final_Beta_AoptGauLM$Method<-rep(\"A-Optimality GauLM\",nrow(Final_Beta_AoptGauLM)) colnames(Final_Beta_AoptGauLM)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"a--and-l-optimality-subsampling","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"A- and L-optimality subsampling","title":"Basic Sampling for Linear Regression","text":"","code":"# A- and L-optimality subsampling for GLM NeEDS4BigData::ALoptimalGLMSub(r1=300,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,family = \"linear\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_ALoptGLM<-Results$Beta_Estimates colnames(Final_Beta_ALoptGLM)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"a-optimality-subsampling-with-measurement-constraints","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"A-optimality subsampling with measurement constraints","title":"Basic Sampling for Linear Regression","text":"","code":"# A-optimality subsampling for without response NeEDS4BigData::AoptimalMCGLMSub(r1=300,r2=rep_k,                                 Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),                                 N=N,family=\"linear\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_AoptMCGLM<-Results$Beta_Estimates Final_Beta_AoptMCGLM$Method<-rep(\"A-Optimality MC\",nrow(Final_Beta_AoptMCGLM)) colnames(Final_Beta_AoptMCGLM)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Lin_Reg.html","id":"summary","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Summary","title":"Basic Sampling for Linear Regression","text":"mean squared error plotted . Mean squared error ) subsampling methods b) without unweighted leverage sampling.","code":"# Summarising Results of Scenario one Final_Beta<-rbind(Final_Beta_RS,Final_Beta_LS,                   Final_Beta_AoptGauLM,Final_Beta_ALoptGLM,                   Final_Beta_AoptMCGLM)  # Obtaining the model parameter estimates for the full big data model lm(Y~.-1,data = Original_Data)->All_Results coefficients(All_Results)->All_Beta matrix(rep(All_Beta,by=nrow(Final_Beta)),nrow = nrow(Final_Beta),        ncol = ncol(Final_Beta[,-c(1,2)]),byrow = TRUE)->All_Beta  remove(Final_Beta_RS,Final_Beta_LS,Final_Beta_AoptGauLM,Final_Beta_ALoptGLM,        Final_Beta_AoptMCGLM,Temp_Data,Results,All_Results) # Obtain the mean squared error for the model parameter estimates MSE_Beta<-data.frame(\"Method\"=Final_Beta$Method,                      \"Subsample\"=Final_Beta$Subsample,                      \"MSE\"=rowSums((All_Beta - Final_Beta[,-c(1,2)])^2))   MSE_Beta$Method<-factor(MSE_Beta$Method,levels = Method_Names,labels = Method_Names)  # Plot for the mean squared error with all methods ggplot(MSE_Beta |> dplyr::group_by(Method,Subsample) |>           dplyr::summarise(MSE=mean(MSE),.groups ='drop'),        aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Color)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  # Plot for the mean squared error with all methods except unweighted leverage ggplot(MSE_Beta[MSE_Beta$Method != \"Unweighted Leverage\",] |>           dplyr::group_by(Method,Subsample) |>           dplyr::summarise(MSE=mean(MSE),.groups ='drop'),        aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Color[-7])+   scale_linetype_manual(values=Method_Line_Types[-7])+   scale_shape_manual(values = Method_Shape_Types[-7])+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"understanding-the-skin-segmentation-data","dir":"Articles","previous_headings":"","what":"Understanding the skin segmentation data","title":"Basic Sampling for Logistic Regression","text":"data contains \\(4\\) columns \\(245,057\\) observations, first column response variable rest covariates, however use \\(25\\%\\) data explanation. Aim logistic regression model classify images skin based ) Red, b) Green c) Blue colour data. Skin presence denoted one skin absence denoted zero. colour vector scaled mean zero variance one (initial range 0−255). given data subsampling methods implemented assuming main effects model can describe data. First five observations skin segmentation data. focus assuming main effects model intercept can appropriately describe big data. Based model methods random sampling, leverage sampling, local case control sampling, \\(\\)- \\(L\\)-optimality subsampling \\(\\)-optimality subsampling response constraint implemented big data. obtained subsamples respective model parameter estimates \\(M=100\\) simulations across different subsample sizes \\(k=(600,\\ldots,1500)\\). set initial subsample size \\(r1=300\\) methods requires random sample. final subsamples respective model parameters estimated. estimated model parameters compared estimated model parameters full big data mean squared error \\(MSE(\\tilde{\\beta}_k,\\hat{\\beta})=\\sum_{=1}^M (\\tilde{\\beta}_k - \\hat{\\beta})^2/M\\). , \\(\\tilde{\\beta}_k\\) estimated model parameters subsample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data. code implementation.","code":"indexes<-sample(1:nrow(Skin_segmentation),nrow(Skin_segmentation)*0.25) Original_Data<-cbind(Skin_segmentation[indexes,1],1,Skin_segmentation[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Original_Data) %>%    kable(format = \"html\",         caption = \"First five observations of the skin segmentation data.\") # Setting the subsample sizes N<-nrow(Original_Data); M<-200; k<-c(6:15)*100; rep_k<-rep(k,each=M) # define colors, shapes, line types for the methods Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MC\",\"Local case control sampling\",                 \"Shrinkage Leverage\",\"Basic Leverage\",\"Unweighted Leverage\",\"Random sampling\") Method_Color<-c(\"green\",\"green4\",\"palegreen\",                 \"maroon\",\"red\",\"darkred\",\"firebrick\",\"black\") Method_Shape_Types<-c(rep(8,3),rep(4,4),16) Method_Line_Types<-c(rep(\"twodash\",3),rep(\"dotted\",4),\"solid\")"},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"random-sampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Random sampling","title":"Basic Sampling for Logistic Regression","text":"","code":"# Random sampling Final_Beta_RS<-matrix(nrow = length(k)*M,ncol = ncol(Original_Data[,-1])+1) for (i in 1:length(rep_k)) {   Temp_Data<-Original_Data[sample(1:N,rep_k[i]),]   glm(Y~.-1,data=Temp_Data,family=\"binomial\")->Results   Final_Beta_RS[i,]<-c(rep_k[i],coefficients(Results))   if(i==length(rep_k)){print(\"All simulations completed for random sampling\")} } ## [1] \"All simulations completed for random sampling\" Final_Beta_RS<-cbind.data.frame(\"Random sampling\",Final_Beta_RS) colnames(Final_Beta_RS)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"leverage-sampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Leverage sampling","title":"Basic Sampling for Logistic Regression","text":"","code":"# Leverage sampling ## we set the shrinkage value of alpha=0.9 NeEDS4BigData::LeverageSampling(r=rep_k,Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),N=N,                                 alpha = 0.9,family = \"logistic\")->Results ## Basic and shrinkage leverage probabilities calculated. ## Subsampling completed. Final_Beta_LS<-Results$Beta_Estimates colnames(Final_Beta_LS)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"local-case-control-sampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Local case control sampling","title":"Basic Sampling for Logistic Regression","text":"","code":"# Local case control sampling NeEDS4BigData::LCCsampling(r1=300,r2=rep_k,                            Y=as.matrix(Original_Data[,1]),                            X=as.matrix(Original_Data[,-1]),                            N=N)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_LCCS<-Results$Beta_Estimates Final_Beta_LCCS$Method<-rep(\"Local case control sampling\",nrow(Final_Beta_LCCS)) colnames(Final_Beta_LCCS)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"a--and-l-optimality-subsampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"A- and L-optimality subsampling","title":"Basic Sampling for Logistic Regression","text":"","code":"# A- and L-optimality subsampling for GLM  NeEDS4BigData::ALoptimalGLMSub(r1=300,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,family = \"logistic\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_ALoptGLM<-Results$Beta_Estimates colnames(Final_Beta_ALoptGLM)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"a-optimality-subsampling-under-measurement-constraints","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"A-optimality subsampling under measurement constraints","title":"Basic Sampling for Logistic Regression","text":"","code":"# A-optimality subsampling for without response NeEDS4BigData::AoptimalMCGLMSub(r1=300,r2=rep_k,                                 Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),                                 N=N,family=\"logistic\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_AoptMCGLM<-Results$Beta_Estimates Final_Beta_AoptMCGLM$Method<-rep(\"A-Optimality MC\",nrow(Final_Beta_AoptMCGLM)) colnames(Final_Beta_AoptMCGLM)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Log_Reg.html","id":"summary","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Summary","title":"Basic Sampling for Logistic Regression","text":"mean squared error plotted . Mean squared error ) subsampling methods b) without unweighted leverage sampling.","code":"# Summarising Results of Scenario one Final_Beta<-rbind(Final_Beta_RS,Final_Beta_LS,Final_Beta_LCCS,                   Final_Beta_ALoptGLM,Final_Beta_AoptMCGLM)  # Obtaining the model parameter estimates for the full big data model glm(Y~.-1,data = Original_Data,family=\"binomial\")->All_Results coefficients(All_Results)->All_Beta matrix(rep(All_Beta,by=nrow(Final_Beta)),nrow = nrow(Final_Beta),        ncol = ncol(Final_Beta[,-c(1,2)]),byrow = TRUE)->All_Beta  remove(Final_Beta_RS,Final_Beta_LS,Final_Beta_LCCS,Final_Beta_ALoptGLM,        Final_Beta_AoptMCGLM,Temp_Data,Results,All_Results) # Obtain the mean squared error for the model parameter estimates MSE_Beta<-data.frame(\"Method\"=Final_Beta$Method,                      \"Subsample\"=Final_Beta$Subsample,                      \"MSE\"=rowSums((All_Beta - Final_Beta[,-c(1,2)])^2))   MSE_Beta$Method<-factor(MSE_Beta$Method,levels = Method_Names,labels = Method_Names)  # Plot for the mean squared error with all methods ggplot(MSE_Beta |> dplyr::group_by(Method,Subsample) |>           dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),        aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Color)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  # Plot for the mean squared error with all methods except unweighted leverage ggplot(MSE_Beta[!(MSE_Beta$Method %in% c(\"Unweighted Leverage\",                                          \"Shrinkage Leverage\",\"Basic Leverage\")),] |>           dplyr::group_by(Method,Subsample) |>           dplyr::summarise(MSE=mean(MSE),.groups = 'drop'),        aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Color[-7])+   scale_linetype_manual(values=Method_Line_Types[-7])+   scale_shape_manual(values = Method_Shape_Types[-7])+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":"/articles/Basic_Sampling_Poi_Reg.html","id":"understanding-the-bike-sharing-data","dir":"Articles","previous_headings":"","what":"Understanding the bike sharing data","title":"Basic Sampling for Poisson Regression","text":"data contains \\(4\\) columns \\(17,379\\) observations, first column response variable rest covariates. consider covariates ) temperature (\\(x_1\\)), b) humidity (\\(x_2\\)) c) windspeed (\\(x_3\\)) model response, number bikes rented hourly. covariates scaled mean \\(\\mu=0\\) variance \\(\\sigma^2=1\\). given data subsampling methods implemented assuming main effects model can describe data. First five observations bike sharing data. focus assuming main effects model intercept can appropriately describe big data. Based model methods random sampling, leverage sampling, \\(\\)- \\(L\\)-optimality subsampling \\(\\)-optimality subsampling response constraint implemented big data. obtained subsamples respective model parameter estimates \\(M=100\\) simulations across different subsample sizes \\(k=(600,\\ldots,1500)\\). set initial subsample size \\(r1=300\\) methods requires random sample. final subsamples respective model parameters estimated. estimated model parameters compared estimated model parameters full big data mean squared error \\(MSE(\\tilde{\\beta}_k,\\hat{\\beta})=\\sum_{=1}^M (\\tilde{\\beta}_k - \\hat{\\beta})^2/M\\). , \\(\\tilde{\\beta}_k\\) estimated model parameters subsample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data. code implementing subsampling methods.","code":"Original_Data<-cbind(Bike_sharing[,1],1,Bike_sharing[,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Original_Data) %>%    kable(format = \"html\",         caption = \"First five observations of the bike sharing data.\") # Setting the subsample sizes N<-nrow(Original_Data); M<-200; k<-c(6:15)*100; rep_k<-rep(k,each=M) # define colors, shapes, line types for the methods Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MC\",                 \"Shrinkage Leverage\",\"Basic Leverage\",\"Unweighted Leverage\",\"Random Sampling\") Method_Color<-c(\"green\",\"green4\",\"palegreen\",                 \"red\",\"darkred\",\"maroon\",\"black\") Method_Shape_Types<-c(rep(8,3),rep(4,3),16) Method_Line_Types<-c(rep(\"twodash\",3),rep(\"dotted\",3),\"solid\")"},{"path":"/articles/Basic_Sampling_Poi_Reg.html","id":"random-sampling","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Random sampling","title":"Basic Sampling for Poisson Regression","text":"","code":"# Random sampling Final_Beta_RS<-matrix(nrow = length(k)*M,ncol = ncol(Original_Data[,-1])+1) for (i in 1:length(rep_k)) {   Temp_Data<-Original_Data[sample(1:N,rep_k[i]),]   glm(Y~.-1,data=Temp_Data,family=\"poisson\")->Results   Final_Beta_RS[i,]<-c(rep_k[i],coefficients(Results))   if(i==length(rep_k)){print(\"All simulations completed for random sampling\")} } ## [1] \"All simulations completed for random sampling\" Final_Beta_RS<-cbind.data.frame(\"Random Sampling\",Final_Beta_RS) colnames(Final_Beta_RS)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Poi_Reg.html","id":"leverage-sampling","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Leverage sampling","title":"Basic Sampling for Poisson Regression","text":"","code":"# Leverage sampling  ## we set the shrinkage value of alpha=0.9 NeEDS4BigData::LeverageSampling(r=rep_k,Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),N=N,                                 alpha = 0.9,family = \"poisson\")->Results ## Basic and shrinkage leverage probabilities calculated. ## Subsampling completed. Final_Beta_LS<-Results$Beta_Estimates colnames(Final_Beta_LS)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Poi_Reg.html","id":"a--and-l-optimality-subsampling","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"A- and L-optimality subsampling","title":"Basic Sampling for Poisson Regression","text":"","code":"# A- and L-optimality subsampling for GLM NeEDS4BigData::ALoptimalGLMSub(r1=300,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,family = \"poisson\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_ALoptGLM<-Results$Beta_Estimates colnames(Final_Beta_ALoptGLM)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Poi_Reg.html","id":"a-optimality-subsampling-with-measurement-constraints","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"A-optimality subsampling with measurement constraints","title":"Basic Sampling for Poisson Regression","text":"","code":"# A-optimality subsampling without response NeEDS4BigData::AoptimalMCGLMSub(r1=300,r2=rep_k,                                 Y=as.matrix(Original_Data[,1]),                                 X=as.matrix(Original_Data[,-1]),                                 N=N,family=\"poisson\")->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_AoptMCGLM<-Results$Beta_Estimates Final_Beta_AoptMCGLM$Method<-rep(\"A-Optimality MC\",nrow(Final_Beta_AoptMCGLM)) colnames(Final_Beta_AoptMCGLM)<-c(\"Method\",\"Subsample\",paste0(\"Beta\",0:3))"},{"path":"/articles/Basic_Sampling_Poi_Reg.html","id":"summary","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Summary","title":"Basic Sampling for Poisson Regression","text":"mean squared error plotted . Mean squared error ) subsampling methods b) without unweighted leverage sampling.","code":"# Summarising Results of Scenario one Final_Beta<-rbind(Final_Beta_RS,Final_Beta_LS,                   Final_Beta_ALoptGLM,Final_Beta_AoptMCGLM)  # Obtaining the model parameter estimates for the full big data model glm(Y~.-1,data = Original_Data,family=\"poisson\")->All_Results coefficients(All_Results)->All_Beta matrix(rep(All_Beta,by=nrow(Final_Beta)),nrow = nrow(Final_Beta),        ncol = ncol(Final_Beta[,-c(1,2)]),byrow = TRUE)->All_Beta  remove(Final_Beta_RS,Final_Beta_LS,Final_Beta_ALoptGLM,        Final_Beta_AoptMCGLM,Temp_Data,Results,All_Results) # Obtain the mean squared error for the model parameter estimates MSE_Beta<-data.frame(\"Method\"=Final_Beta$Method,                      \"Subsample\"=Final_Beta$Subsample,                      \"MSE\"=rowSums((All_Beta - Final_Beta[,-c(1,2)])^2))   MSE_Beta$Method<-factor(MSE_Beta$Method,levels = Method_Names,labels = Method_Names)  # Plot for the mean squared error with all methods ggplot(MSE_Beta |> dplyr::group_by(Method,Subsample) |>           dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),        aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Color)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  # Plot for the mean squared error with all methods except unweighted leverage ggplot(MSE_Beta[MSE_Beta$Method != \"Unweighted Leverage\",] |>           dplyr::group_by(Method,Subsample) |>           dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),        aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Color[-7])+   scale_linetype_manual(values=Method_Line_Types[-7])+   scale_shape_manual(values = Method_Shape_Types[-7])+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  #save(Final_Beta,All_Beta,MSE_Beta,p1,p2,file=\"Poisson_Regression_Scenario_1.RData\") #load(\"Poisson_Regression_Scenario_1.RData\") ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\")"},{"path":[]},{"path":[]},{"path":"/articles/Linear_Regression.html","id":"understanding-the-electric-consumption-data","dir":"Articles","previous_headings":"","what":"Understanding the electric consumption data","title":"Linear Regression : Model robust or misspecification","text":"data contains \\(4\\) columns \\(2,049,280\\) observations, first column response variable rest covariates, however use \\(5\\%\\) data explanation. response \\(y\\) log scaled intensity, covariates active electrical energy ) kitchen (\\(X_1\\)), b) laundry room (\\(X_2\\)) c) water-heater air-conditioner (\\(X_3\\)). covariates scaled mean \\(\\mu=0\\) variance \\(\\sigma^2=1\\). Given data analysed two different scenarios, model robust averaging subsampling methods assuming set models can describe data. subsampling method assuming main effects model potentially misspecified. First five observations electric consumption data.","code":"indexes<-sample(1:nrow(Electric_consumption),nrow(Electric_consumption)*0.05) Original_Data<-cbind(Electric_consumption[indexes,1],1,Electric_consumption[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Original_Data) %>%    kable(format = \"html\",         caption = \"First five observations of the electric consumption data.\") # Setting the subsample sizes N<-nrow(Original_Data); M<-200; k<-c(6:15)*100; rep_k<-rep(k,each=M)"},{"path":"/articles/Linear_Regression.html","id":"model-robust-or-model-averaging-subsampling","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Model robust or model averaging subsampling","title":"Linear Regression : Model robust or misspecification","text":"method \\(\\)- \\(L\\)-optimality model robust average subsampling compared \\(\\)- \\(L\\)-optimality subsampling method. five different models considered 1) main effects model (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3\\)), 2-4) main effects model squared term covariate (\\(X^2_1 / X^2_2 / X^2_3\\)) 5) main effects model squared terms (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3+\\beta_4X^2_1+\\beta_5X^2_2+\\beta_6X^2_3\\)). model \\(j\\) mean squared error model parameters \\(MSE_j(\\tilde{\\beta}_k,\\hat{\\beta})=\\sum_{=1}^M (\\tilde{\\beta}_k - \\hat{\\beta})^2/M\\) calculated \\(M=100\\) simulations across subsample sizes \\(k=(600,\\ldots,1500)\\) initial subsample size \\(r1=300\\). , \\(j\\)-th model \\(\\tilde{\\beta}_k\\) estimated model parameters subsample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data.","code":"# Define the subsampling methods and their respective colors, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MR\",\"L-Optimality MR\") Method_Color<-c(\"red\",\"darkred\",\"palegreen\",\"springgreen4\") Method_Shape_Types<-c(rep(8,2),rep(4,2)) Method_Line_Types<-c(rep(\"twodash\",2),rep(\"dotted\",2))  # Preparing the data for the model average method with squared terms No_of_Variables<-ncol(Original_Data[,-c(1,2)]) Squared_Terms<-paste0(\"X\",1:No_of_Variables,\"^2\") term_no <- 2 All_Models <- list(c(\"X0\",paste0(\"X\",1:No_of_Variables)))  Original_Data_ModelRobust<-cbind(Original_Data,Original_Data[,-c(1,2)]^2) colnames(Original_Data_ModelRobust)<-c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables),                                        paste0(\"X\",1:No_of_Variables,\"^2\"))  for (i in 1:No_of_Variables) {   x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))   for(j in 1:length(x))   {     All_Models[[term_no]] <- c(\"X0\",paste0(\"X\",1:No_of_Variables),x[[j]])     term_no <- term_no+1   } } All_Models<-All_Models[-c(5:7)] names(All_Models)<-paste0(\"Model_\",1:length(All_Models))"},{"path":"/articles/Linear_Regression.html","id":"apriori-probabilities-are-equal","dir":"Articles","previous_headings":"Understanding the electric consumption data > Model robust or model averaging subsampling","what":"Apriori probabilities are equal","title":"Linear Regression : Model robust or misspecification","text":"Consider \\(Q=5\\) model equal priori probability (.e \\(\\alpha_q=1/5,q=1,\\ldots,5\\)). code implementation scenario. Mean squared error models equal apriori order e Model 1 5 across subsampling methods comparison.","code":"# A- and L-optimality model robust subsampling for linear regression NeEDS4BigData::modelRobustLinSub(r1=300,r2=rep_k,                                  Y=as.matrix(Original_Data_ModelRobust[,1]),                                  X=as.matrix(Original_Data_ModelRobust[,-1]),                                  N=N,Alpha=rep(1/length(All_Models),length(All_Models)),                                  All_Combinations = All_Models,                                  All_Covariates = colnames(Original_Data_ModelRobust)[-1])->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelRobust<-Results$Beta_Estimates  # Mean squared error and their respective plots for all five models MSE_Beta_MR<-list(); plot_list_MR<-list() for (i in 1:length(All_Models)) {   lm(Y~.-1,data=Original_Data_ModelRobust[,c(\"Y\",All_Models[[i]])])->All_Results   All_Beta<-coefficients(All_Results)      matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),          nrow = nrow(Final_Beta_modelRobust[[i]]),          ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta      MSE_Beta_MR[[i]]<-data.frame(\"Method\"=Final_Beta_modelRobust[[i]]$Method,                                \"Subsample\"=Final_Beta_modelRobust[[i]]$r2,                                \"MSE\"=rowSums((All_Beta - Final_Beta_modelRobust[[i]][,-c(1,2)])^2))      ggplot(MSE_Beta_MR[[i]] |> dplyr::group_by(Method,Subsample) |>            dplyr::summarise(MSE=mean(MSE),.groups ='drop'),          aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+     geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+     scale_color_manual(values = Method_Color)+     ggtitle(paste0(\"Model \",i))+     scale_linetype_manual(values=Method_Line_Types)+     scale_shape_manual(values = Method_Shape_Types)+     theme_bw()+guides(colour = guide_legend(nrow = 2))+Theme_special()->plot_list_MR[[i]] }  ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = \"auto\",           common.legend = TRUE,legend = \"bottom\")"},{"path":"/articles/Linear_Regression.html","id":"apriori-probabilities-are-unequal","dir":"Articles","previous_headings":"Understanding the electric consumption data > Model robust or model averaging subsampling","what":"Apriori probabilities are unequal","title":"Linear Regression : Model robust or misspecification","text":"Consider \\(Q=5\\) model different priori probability based number model parameters (Scott Berger 2010) (.e., \\(\\alpha_q = \\Big(7 {6 \\choose b_q}\\Big)^{−1}\\) \\(b_q\\) number model parameters \\(q\\)-th model excluding intercept). code implementation scenario. Mean squared error models unequal apriori order e Model 1 5 across subsampling methods comparison.","code":"UnEqual_Apriori<-NULL for (i in 1:length(All_Models))  {   UnEqual_Apriori[i]<-(7*choose(6,length(All_Models[[i]])-1))^(-1) }  UnEqual_Apriori<-UnEqual_Apriori/sum(UnEqual_Apriori)  # A- and L-optimality model robust subsampling for linear regression NeEDS4BigData::modelRobustLinSub(r1=300,r2=rep_k,                                  Y=as.matrix(Original_Data_ModelRobust[,1]),                                  X=as.matrix(Original_Data_ModelRobust[,-1]),                                  N=N,Alpha=UnEqual_Apriori,                                  All_Combinations = All_Models,                                  All_Covariates = colnames(Original_Data_ModelRobust)[-1])->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelRobust<-Results$Beta_Estimates  # Mean squared error and their respective plots for all five models MSE_Beta_MR<-list(); plot_list_MR<-list() for (i in 1:length(All_Models)) {   lm(Y~.-1,data=Original_Data_ModelRobust[,c(\"Y\",All_Models[[i]])])->All_Results   All_Beta<-coefficients(All_Results)      matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),          nrow = nrow(Final_Beta_modelRobust[[i]]),          ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta      MSE_Beta_MR[[i]]<-data.frame(\"Method\"=Final_Beta_modelRobust[[i]]$Method,                                \"Subsample\"=Final_Beta_modelRobust[[i]]$r2,                                \"MSE\"=rowSums((All_Beta - Final_Beta_modelRobust[[i]][,-c(1,2)])^2))      ggplot(MSE_Beta_MR[[i]] |> dplyr::group_by(Method,Subsample) |>             dplyr::summarise(MSE=mean(MSE),.groups ='drop'),          aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+     geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+     scale_color_manual(values = Method_Color)+     ggtitle(paste0(\"Model \",i))+     scale_linetype_manual(values=Method_Line_Types)+     scale_shape_manual(values = Method_Shape_Types)+     theme_bw()+guides(colour = guide_legend(nrow = 2))+Theme_special()->plot_list_MR[[i]] }  ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = \"auto\",           common.legend = TRUE,legend = \"bottom\")"},{"path":"/articles/Linear_Regression.html","id":"main-effects-model-is-potentially-misspecified","dir":"Articles","previous_headings":"Understanding the electric consumption data","what":"Main effects model is potentially misspecified","title":"Linear Regression : Model robust or misspecification","text":"final scenario comparison subsampling method assumption main effects model potentially misspecified \\(\\)- \\(L\\)-optimality subsampling method. subsampling method accounts potential model misspecification take scaling factor \\(\\alpha=10\\). scenarios number simulations subsample sizes stay . compare mean squared error estimated model parameters, however assume model potentially misspecified asymptotic approximation mean squared error predictions calculated well. code implementation. Average loss potentially misspecified main effects model across subsampling methods comparison.","code":"# Define the subsampling methods and their respective colors, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"LmAMSE\",\"LmAMSE Log Odds 10\",\"LmAMSE Power 10\") Method_Color<-c(\"red\",\"darkred\",\"palegreen\",\"green\",\"springgreen4\") Method_Shape_Types<-c(rep(8,2),rep(4,3)) Method_Line_Types<-c(rep(\"twodash\",2),rep(\"dotted\",3))  # For the big data fit the main effects model and estimate the contamination interaction_terms <- combn(colnames(Original_Data[,-1])[-1],2,FUN=function(x) paste(x,collapse=\"*\")) my_formula<-as.formula(paste(\"Y ~ \",                              paste(paste0(\"s(X\",1:ncol(Original_Data[,-c(1,2)]),\")\"),collapse=\"+\"),                              \"+\",paste(paste0(\"s(\",interaction_terms,\")\"),collapse=\"+\")))  lm(Y~.-1,data=Original_Data)->Results beta.prop<-coefficients(Results) Xbeta_Final<-as.vector(as.matrix(Original_Data[,-1])%*%beta.prop) Var.prop<-sum((Original_Data$Y-Xbeta_Final)^2)/N fit_GAM<-gam::gam(formula = my_formula,data=Original_Data) Xbeta_GAM<-gam::predict.Gam(fit_GAM,newdata = Original_Data[,-1]) f_estimate<-Xbeta_GAM - Xbeta_Final Var_GAM.prop<-sum((Original_Data[,1]-Xbeta_GAM)^2)/N  # A- and L-optimality and LmAMSE model misspecified subsampling for linear regression  NeEDS4BigData::modelMissLinSub(r1=300,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,Alpha=10,Var_GAM_Full = Var_GAM.prop,                                Var_Full=Var.prop,F_Estimate_Full = f_estimate)->Results ## Warning: executing %dopar% sequentially: no parallel backend registered ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelMiss<-Results$Beta_Estimates Final_Loss_modelMiss<-Results$Loss_Estimates  lm(Y~.-1,data = Original_Data)->All_Results coefficients(All_Results)->All_Beta matrix(rep(All_Beta,by=nrow(Final_Beta_modelMiss)),nrow = nrow(Final_Beta_modelMiss),        ncol = ncol(Final_Beta_modelMiss[,-c(1,2)]),byrow = TRUE)->All_Beta  # Plots for the mean squared error of the model parameter estimates  # and the loss for the main effects model  MSE_Beta_modelMiss<-data.frame(\"Method\"=Final_Beta_modelMiss$Method,                                \"Subsample\"=Final_Beta_modelMiss$r2,                                \"MSE\"=rowSums((All_Beta - Final_Beta_modelMiss[,-c(1,2)])^2))  ggplot(MSE_Beta_modelMiss |> dplyr::group_by(Method,Subsample) |>           dplyr::summarise(MSE=mean(MSE),.groups ='drop'),        aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Color)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  ggplot(Final_Loss_modelMiss |> dplyr::group_by(r2,Method) |>           dplyr::summarise(meanLoss=mean(Loss),.groups ='drop'),        aes(x=factor(r2),y=meanLoss,color=Method,group=Method,linetype=Method,shape=Method)) +   geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"Mean Loss\")+   scale_color_manual(values = Method_Color)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  p2 #ggarrange(p1,p2,nrow = 1,ncol = 2,labels = \"auto\")"},{"path":[]},{"path":[]},{"path":"/articles/Logistic_Regression.html","id":"understanding-the-skin-segmentation-data","dir":"Articles","previous_headings":"","what":"Understanding the skin segmentation data","title":"Logistic Regression : Model robust or misspecification","text":"data contains \\(4\\) columns \\(245,057\\) observations, first column response variable rest covariates, however use \\(25\\%\\) data explanation. Aim logistic regression model classify images skin based ) Red, b) Green c) Blue colour data. Skin presence denoted one skin absence denoted zero. colour vector scaled mean zero variance one (initial range 0−255). Given data analysed two different scenarios, model robust averaging subsampling methods assuming set models can describe data. subsampling method assuming main effects model potentially misspecified. First five observations skin segmentation data.","code":"indexes<-sample(1:nrow(Skin_segmentation),nrow(Skin_segmentation)*0.25) Original_Data<-cbind(Skin_segmentation[indexes,1],1,Skin_segmentation[indexes,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Original_Data) %>%    kable(format = \"html\",         caption = \"First five observations of the skin segmentation data.\") # Setting the subsample sizes N<-nrow(Original_Data); M<-200; k<-c(6:15)*100; rep_k<-rep(k,each=M)"},{"path":"/articles/Logistic_Regression.html","id":"model-robust-or-model-averaging-subsampling","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Model robust or model averaging subsampling","title":"Logistic Regression : Model robust or misspecification","text":"method \\(\\)- \\(L\\)-optimality model robust average subsampling compared \\(\\)- \\(L\\)-optimality subsampling method. five different models considered 1) main effects model (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3\\)), 2-4) main effects model squared term covariate (\\(X^2_1 / X^2_2 / X^2_3\\)) 5) main effects model squared terms (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3+\\beta_4X^2_1+\\beta_5X^2_2+\\beta_6X^2_3\\)). model \\(j\\) mean squared error model parameters \\(MSE_j(\\tilde{\\beta}_k,\\hat{\\beta})=\\sum_{=1}^M (\\tilde{\\beta}_k - \\hat{\\beta})^2/M\\) calculated \\(M=100\\) simulations across subsample sizes \\(k=(600,\\ldots,1500)\\) initial subsample size \\(r1=300\\). , \\(j\\)-th model \\(\\tilde{\\beta}_k\\) estimated model parameters subsample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data.","code":"# Define the subsampling methods and their respective colors, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MR\",\"L-Optimality MR\") Method_Color<-c(\"red\",\"darkred\",\"palegreen\",\"springgreen4\") Method_Shape_Types<-c(rep(8,2),rep(4,2)) Method_Line_Types<-c(rep(\"twodash\",2),rep(\"dotted\",2))  # Preparing the data for the model average method with squared terms No_of_Variables<-ncol(Original_Data[,-c(1,2)]) Squared_Terms<-paste0(\"X\",1:No_of_Variables,\"^2\") term_no <- 2 All_Models <- list(c(\"X0\",paste0(\"X\",1:No_of_Variables)))  Original_Data_ModelRobust<-cbind(Original_Data,Original_Data[,-c(1,2)]^2) colnames(Original_Data_ModelRobust)<-c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables),                                        paste0(\"X\",1:No_of_Variables,\"^2\"))  for (i in 1:No_of_Variables) {   x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))   for(j in 1:length(x))   {     All_Models[[term_no]] <- c(\"X0\",paste0(\"X\",1:No_of_Variables),x[[j]])     term_no <- term_no+1   } } All_Models<-All_Models[-c(5:7)] names(All_Models)<-paste0(\"Model_\",1:length(All_Models))"},{"path":"/articles/Logistic_Regression.html","id":"apriori-probabilities-are-equal","dir":"Articles","previous_headings":"Understanding the skin segmentation data > Model robust or model averaging subsampling","what":"Apriori probabilities are equal","title":"Logistic Regression : Model robust or misspecification","text":"Consider \\(Q=5\\) model equal priori probability (.e \\(\\alpha_q=1/5,q=1,\\ldots,5\\)). code implementation scenario. Mean squared error models equal apriori order e Model 1 5 across subsampling methods comparison.","code":"# A- and L-optimality model robust subsampling for logistic regression NeEDS4BigData::modelRobustLogSub(r1=300,r2=rep_k,                                  Y=as.matrix(Original_Data_ModelRobust[,1]),                                  X=as.matrix(Original_Data_ModelRobust[,-1]),                                  N=N,Alpha=rep(1/length(All_Models),length(All_Models)),                                  All_Combinations = All_Models,                                  All_Covariates = colnames(Original_Data_ModelRobust)[-1])->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelRobust<-Results$Beta_Estimates  # Mean squared error and their respective plots for all five models MSE_Beta_MR<-list(); plot_list_MR<-list() for (i in 1:length(All_Models)) {   glm(Y~.-1,data=Original_Data_ModelRobust[,c(\"Y\",All_Models[[i]])],family=\"binomial\")->All_Results   All_Beta<-coefficients(All_Results)    matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),          nrow = nrow(Final_Beta_modelRobust[[i]]),          ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta    MSE_Beta_MR[[i]]<-data.frame(\"Method\"=Final_Beta_modelRobust[[i]]$Method,                                \"Subsample\"=Final_Beta_modelRobust[[i]]$r2,                                \"MSE\"=rowSums((All_Beta - Final_Beta_modelRobust[[i]][,-c(1,2)])^2))    ggplot(MSE_Beta_MR[[i]] |> dplyr::group_by(Method,Subsample) |>             dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),          aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+     geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+     scale_color_manual(values = Method_Color)+     ggtitle(paste0(\"Model \",i))+     scale_linetype_manual(values=Method_Line_Types)+     scale_shape_manual(values = Method_Shape_Types)+     theme_bw()+guides(colour = guide_legend(nrow = 2))+Theme_special()->plot_list_MR[[i]] }  ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = \"auto\",           common.legend = TRUE,legend = \"bottom\")"},{"path":"/articles/Logistic_Regression.html","id":"apriori-probabilities-are-unequal","dir":"Articles","previous_headings":"Understanding the skin segmentation data > Model robust or model averaging subsampling","what":"Apriori probabilities are unequal","title":"Logistic Regression : Model robust or misspecification","text":"Consider \\(Q=5\\) model different priori probability based number model parameters (Scott Berger 2010) (.e., \\(\\alpha_q = \\Big(7 {6 \\choose b_q}\\Big)^{−1}\\) \\(b_q\\) number model parameters \\(q\\)-th model excluding intercept). code implementation scenario. Mean squared error models unequal apriori order e Model 1 5 across subsampling methods comparison.","code":"UnEqual_Apriori<-NULL for (i in 1:length(All_Models))  {   UnEqual_Apriori[i]<-(7*choose(6,length(All_Models[[i]])-1))^(-1) }  UnEqual_Apriori<-UnEqual_Apriori/sum(UnEqual_Apriori)  # A- and L-optimality model robust subsampling for logistic regression NeEDS4BigData::modelRobustLogSub(r1=300,r2=rep_k,                                  Y=as.matrix(Original_Data_ModelRobust[,1]),                                  X=as.matrix(Original_Data_ModelRobust[,-1]),                                  N=N,Alpha=UnEqual_Apriori,                                  All_Combinations = All_Models,                                  All_Covariates = colnames(Original_Data_ModelRobust)[-1])->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelRobust<-Results$Beta_Estimates  # Mean squared error and their respective plots for all five models MSE_Beta_MR<-list(); plot_list_MR<-list() for (i in 1:length(All_Models)) {   glm(Y~.-1,data=Original_Data_ModelRobust[,c(\"Y\",All_Models[[i]])],       family=\"binomial\")->All_Results   All_Beta<-coefficients(All_Results)      matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),          nrow = nrow(Final_Beta_modelRobust[[i]]),          ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta      MSE_Beta_MR[[i]]<-data.frame(\"Method\"=Final_Beta_modelRobust[[i]]$Method,                                \"Subsample\"=Final_Beta_modelRobust[[i]]$r2,                                \"MSE\"=rowSums((All_Beta - Final_Beta_modelRobust[[i]][,-c(1,2)])^2))      ggplot(MSE_Beta_MR[[i]] |> dplyr::group_by(Method,Subsample) |>             dplyr::summarise(MSE=mean(MSE),.groups ='drop'),          aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+     geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+     scale_color_manual(values = Method_Color)+     ggtitle(paste0(\"Model \",i))+     scale_linetype_manual(values=Method_Line_Types)+     scale_shape_manual(values = Method_Shape_Types)+     theme_bw()+guides(colour = guide_legend(nrow = 2))+Theme_special()->plot_list_MR[[i]] }  ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = \"auto\",           common.legend = TRUE,legend = \"bottom\")"},{"path":"/articles/Logistic_Regression.html","id":"main-effects-model-is-potentially-misspecified","dir":"Articles","previous_headings":"Understanding the skin segmentation data","what":"Main effects model is potentially misspecified","title":"Logistic Regression : Model robust or misspecification","text":"final third scenario comparison subsampling method assumption main effects model potentially misspecified \\(\\)- \\(L\\)-optimality subsampling method. subsampling method accounts potential model misspecification take scaling factor \\(\\alpha=10\\). scenario one two number simulations subsample sizes stay . compare mean squared error estimated model parameters, however assume model potentially misspecified asymptotic approximation mean squared error predictions calculated well. code implementation. Average loss potentially misspecified main effects model across subsampling methods comparison.","code":"# Define the subsampling methods and their respective colors, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"LmAMSE\",\"LmAMSE Log Odds 10\",\"LmAMSE Power 10\") Method_Color<-c(\"red\",\"darkred\",\"palegreen\",\"green\",\"springgreen4\") Method_Shape_Types<-c(rep(8,2),rep(4,3)) Method_Line_Types<-c(rep(\"twodash\",2),rep(\"dotted\",3))  # For the big data fit the main effects model and estimate the contamination interaction_terms <- combn(colnames(Original_Data[,-1])[-1],2,FUN=function(x)paste(x,collapse=\"*\")) my_formula<-as.formula(paste(\"Y ~ \",                              paste(paste0(\"s(X\",1:ncol(Original_Data[,-c(1,2)]),\")\"),collapse=\"+\"),                              \"+\",paste(paste0(\"s(\",interaction_terms,\")\"),collapse=\" + \")))  glm(Y~.-1,data=Original_Data,family=\"binomial\")->Results beta.prop<-coefficients(Results) Xbeta_Final<-as.vector(as.matrix(Original_Data[,-1])%*%beta.prop) fit_GAM<-gam::gam(formula = my_formula,data=Original_Data,family=\"binomial\") Xbeta_GAM<-gam::predict.Gam(fit_GAM,newdata = Original_Data[,-1]) f_estimate<-Xbeta_GAM - Xbeta_Final  # A- and L-optimality and LmAMSE model misspecified subsampling for logistic regression  NeEDS4BigData::modelMissLogSub(r1=300,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,Alpha=10, Beta_Estimate_Full = beta.prop,                                F_Estimate_Full = f_estimate)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelMiss<-Results$Beta_Estimates Final_Loss_modelMiss<-Results$Loss_Estimates  matrix(rep(beta.prop,by=nrow(Final_Beta_modelMiss)),nrow = nrow(Final_Beta_modelMiss),        ncol = ncol(Final_Beta_modelMiss[,-c(1,2)]),byrow = TRUE)->All_Beta  # Plots for the mean squared error of the model parameter estimates  # and the loss for the main effects model  MSE_Beta_modelMiss<-data.frame(\"Method\"=Final_Beta_modelMiss$Method,                                \"Subsample\"=Final_Beta_modelMiss$r2,                                \"MSE\"=rowSums((All_Beta - Final_Beta_modelMiss[,-c(1,2)])^2))   ggplot(MSE_Beta_modelMiss |> dplyr::group_by(Method,Subsample) |>           dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),        aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Color)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  ggplot(Final_Loss_modelMiss |> dplyr::group_by(r2,Method) |>           dplyr::summarise(meanLoss=mean(Loss), .groups = 'drop'),        aes(x=factor(r2),y=meanLoss,color=Method,group=Method,linetype=Method,shape=Method)) +   geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"Mean Loss\")+   scale_color_manual(values = Method_Color)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  # ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\", #           legend = \"bottom\",common.legend = TRUE) p2"},{"path":[]},{"path":[]},{"path":"/articles/Poisson_Regression.html","id":"understanding-the-bike-sharing-data","dir":"Articles","previous_headings":"","what":"Understanding the bike sharing data","title":"Poisson Regression : Model robust or misspecification","text":"data contains \\(4\\) columns \\(17,379\\) observations, first column response variable rest covariates. consider covariates ) temperature (\\(x_1\\)), b) humidity (\\(x_2\\)) c) windspeed (\\(x_3\\)) model response, number bikes rented hourly. covariates scaled mean \\(\\mu=0\\) variance \\(\\sigma^2=1\\). Given data analysed two different scenarios, model robust averaging subsampling methods assuming set models can describe data. subsampling method assuming main effects model potentially misspecified. First five observations bike sharing data.","code":"Original_Data<-cbind(Bike_sharing[,1],1,Bike_sharing[,-1]) colnames(Original_Data)<-c(\"Y\",paste0(\"X\",0:ncol(Original_Data[,-c(1,2)])))  # Scaling the covariate data for (j in 3:5) {   Original_Data[,j]<-scale(Original_Data[,j]) } head(Original_Data) %>%    kable(format = \"html\",         caption = \"First five observations of the bike sharing data.\") # Setting the subsample sizes N<-nrow(Original_Data); M<-200; k<-c(6:15)*100; rep_k<-rep(k,each=M)"},{"path":"/articles/Poisson_Regression.html","id":"model-robust-or-model-averaging-subsampling","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Model robust or model averaging subsampling","title":"Poisson Regression : Model robust or misspecification","text":"method \\(\\)- \\(L\\)-optimality model robust average subsampling compared \\(\\)- \\(L\\)-optimality subsampling method. five different models considered 1) main effects model (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3\\)), 2-4) main effects model squared term covariate (\\(X^2_1 / X^2_2 / X^2_3\\)) 5) main effects model squared terms (\\(\\beta_0+\\beta_1X_1+\\beta_2X_2+\\beta_3X_3+\\beta_4X^2_1+\\beta_5X^2_2+\\beta_6X^2_3\\)). model \\(j\\) mean squared error model parameters \\(MSE_j(\\tilde{\\beta}_k,\\hat{\\beta})=\\sum_{=1}^M (\\tilde{\\beta}_k - \\hat{\\beta})^2/M\\) calculated \\(M=100\\) simulations across subsample sizes \\(k=(600,\\ldots,1500)\\) initial subsample size \\(r1=300\\). , \\(j\\)-th model \\(\\tilde{\\beta}_k\\) estimated model parameters subsample size \\(k\\) \\(\\hat{\\beta}\\) estimated model parameters full big data.","code":"# Define the subsampling methods and their respective colors, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"A-Optimality MR\",\"L-Optimality MR\") Method_Color<-c(\"red\",\"darkred\",\"palegreen\",\"springgreen4\") Method_Shape_Types<-c(rep(8,2),rep(4,2)) Method_Line_Types<-c(rep(\"twodash\",2),rep(\"dotted\",2))  # Preparing the data for the model average method with squared terms No_of_Variables<-ncol(Original_Data[,-c(1,2)]) Squared_Terms<-paste0(\"X\",1:No_of_Variables,\"^2\") term_no <- 2 All_Models <- list(c(\"X0\",paste0(\"X\",1:No_of_Variables)))  Original_Data_ModelRobust<-cbind(Original_Data,Original_Data[,-c(1,2)]^2) colnames(Original_Data_ModelRobust)<-c(\"Y\",\"X0\",paste0(\"X\",1:No_of_Variables),                                        paste0(\"X\",1:No_of_Variables,\"^2\"))  for (i in 1:No_of_Variables) {   x <- as.vector(combn(Squared_Terms,i,simplify = FALSE))   for(j in 1:length(x))   {     All_Models[[term_no]] <- c(\"X0\",paste0(\"X\",1:No_of_Variables),x[[j]])     term_no <- term_no+1   } } All_Models<-All_Models[-c(5:7)] names(All_Models)<-paste0(\"Model_\",1:length(All_Models))"},{"path":"/articles/Poisson_Regression.html","id":"apriori-probabilities-are-equal","dir":"Articles","previous_headings":"Understanding the bike sharing data > Model robust or model averaging subsampling","what":"Apriori probabilities are equal","title":"Poisson Regression : Model robust or misspecification","text":"Consider \\(Q=5\\) model equal priori probability (.e \\(\\alpha_q=1/5,q=1,\\ldots,5\\)). code implementation scenario. Mean squared error models equal apriori order e Model 1 5 across subsampling methods comparison.","code":"# A- and L-optimality model robust subsampling for poisson regression NeEDS4BigData::modelRobustPoiSub(r1=300,r2=rep_k,                                  Y=as.matrix(Original_Data_ModelRobust[,1]),                                  X=as.matrix(Original_Data_ModelRobust[,-1]),                                  N=N,Alpha=rep(1/length(All_Models),length(All_Models)),                                  All_Combinations = All_Models,                                  All_Covariates = colnames(Original_Data_ModelRobust)[-1])->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelRobust<-Results$Beta_Estimates  # Mean squared error and their respective plots for all five models MSE_Beta_MR<-list(); plot_list_MR<-list() for (i in 1:length(All_Models)) {   glm(Y~.-1,data=Original_Data_ModelRobust[,c(\"Y\",All_Models[[i]])],family=\"poisson\")->All_Results   All_Beta<-coefficients(All_Results)    matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),          nrow = nrow(Final_Beta_modelRobust[[i]]),          ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta    MSE_Beta_MR[[i]]<-data.frame(\"Method\"=Final_Beta_modelRobust[[i]]$Method,                                \"Subsample\"=Final_Beta_modelRobust[[i]]$r2,                                \"MSE\"=rowSums((All_Beta - Final_Beta_modelRobust[[i]][,-c(1,2)])^2))    ggplot(MSE_Beta_MR[[i]] |> dplyr::group_by(Method,Subsample) |>             dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),          aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+     geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+     scale_color_manual(values = Method_Color)+     scale_linetype_manual(values=Method_Line_Types)+     scale_shape_manual(values = Method_Shape_Types)+     theme_bw()+guides(colour = guide_legend(nrow = 2))+Theme_special()->plot_list_MR[[i]] }  ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = \"auto\",           legend = \"bottom\",common.legend = TRUE)"},{"path":"/articles/Poisson_Regression.html","id":"apriori-probabilities-are-unequal","dir":"Articles","previous_headings":"Understanding the bike sharing data > Model robust or model averaging subsampling","what":"Apriori probabilities are unequal","title":"Poisson Regression : Model robust or misspecification","text":"Consider \\(Q=5\\) model different priori probability based number model parameters (Scott Berger 2010) (.e., \\(\\alpha_q = \\Big(7 {6 \\choose b_q}\\Big)^{−1}\\) \\(b_q\\) number model parameters \\(q\\)-th model excluding intercept). code implementation scenario. Mean squared error models unequal apriori order e Model 1 5 across subsampling methods comparison.","code":"UnEqual_Apriori<-NULL for (i in 1:length(All_Models))  {   UnEqual_Apriori[i]<-(7*choose(6,length(All_Models[[i]])-1))^(-1) }  UnEqual_Apriori<-UnEqual_Apriori/sum(UnEqual_Apriori)  # A- and L-optimality model robust subsampling for poisson regression NeEDS4BigData::modelRobustPoiSub(r1=300,r2=rep_k,                                  Y=as.matrix(Original_Data_ModelRobust[,1]),                                  X=as.matrix(Original_Data_ModelRobust[,-1]),                                  N=N,Alpha=UnEqual_Apriori,                                  All_Combinations = All_Models,                                  All_Covariates = colnames(Original_Data_ModelRobust)[-1])->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelRobust<-Results$Beta_Estimates  # Mean squared error and their respective plots for all five models MSE_Beta_MR<-list(); plot_list_MR<-list() for (i in 1:length(All_Models)) {   glm(Y~.-1,data=Original_Data_ModelRobust[,c(\"Y\",All_Models[[i]])],       family = \"poisson\")->All_Results   All_Beta<-coefficients(All_Results)      matrix(rep(All_Beta,by=nrow(Final_Beta_modelRobust[[i]])),          nrow = nrow(Final_Beta_modelRobust[[i]]),          ncol = ncol(Final_Beta_modelRobust[[i]][,-c(1,2)]),byrow = TRUE)->All_Beta      MSE_Beta_MR[[i]]<-data.frame(\"Method\"=Final_Beta_modelRobust[[i]]$Method,                                \"Subsample\"=Final_Beta_modelRobust[[i]]$r2,                                \"MSE\"=rowSums((All_Beta - Final_Beta_modelRobust[[i]][,-c(1,2)])^2))      ggplot(MSE_Beta_MR[[i]] |> dplyr::group_by(Method,Subsample) |>             dplyr::summarise(MSE=mean(MSE),.groups ='drop'),          aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+     geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+     scale_color_manual(values = Method_Color)+     ggtitle(paste0(\"Model \",i))+     scale_linetype_manual(values=Method_Line_Types)+     scale_shape_manual(values = Method_Shape_Types)+     theme_bw()+guides(colour = guide_legend(nrow = 2))+Theme_special()->plot_list_MR[[i]] }  ggarrange(plotlist = plot_list_MR,nrow = 3,ncol = 2,labels = \"auto\",           common.legend = TRUE,legend = \"bottom\")"},{"path":"/articles/Poisson_Regression.html","id":"main-effects-model-is-potentially-misspecified","dir":"Articles","previous_headings":"Understanding the bike sharing data","what":"Main effects model is potentially misspecified","title":"Poisson Regression : Model robust or misspecification","text":"final third scenario comparison subsampling method assumption main effects model potentially misspecified \\(\\)- \\(L\\)-optimality subsampling method. subsampling method accounts potential model misspecification take scaling factor \\(\\alpha=10\\). scenario one two number simulations subsample sizes stay . compare mean squared error estimated model parameters, however assume model potentially misspecified asymptotic approximation mean squared error predictions calculated well. code implementation. Average loss potentially misspecified main effects model across subsampling methods comparison.","code":"# Define the subsampling methods and their respective colors, shapes and line types Method_Names<-c(\"A-Optimality\",\"L-Optimality\",\"LmAMSE\",\"LmAMSE Log Odds 10\",\"LmAMSE Power 10\") Method_Color<-c(\"red\",\"darkred\",\"palegreen\",\"green\",\"springgreen4\") Method_Shape_Types<-c(rep(8,2),rep(4,3)) Method_Line_Types<-c(rep(\"twodash\",2),rep(\"dotted\",3))  # For the big data fit the main effects model and estimate the contamination interaction_terms <- combn(colnames(Original_Data[,-1])[-1],2,FUN=function(x)paste(x,collapse=\"*\")) my_formula<-as.formula(paste(\"Y~\",paste(paste0(\"s(X\",1:ncol(Original_Data[,-c(1,2)]),\")\"),collapse=\"+\"),                              \"+\",paste(paste0(\"s(\",interaction_terms,\")\"),collapse=\" + \"))) glm(Y~.-1,data=Original_Data,family=\"poisson\")->Results beta.prop<-coefficients(Results) Xbeta_Final<-as.vector(as.matrix(Original_Data[,-1])%*%beta.prop) fit_GAM<-gam::gam(formula = my_formula,data=Original_Data,family=\"poisson\") Xbeta_GAM<-gam::predict.Gam(fit_GAM,newdata = Original_Data[,-1]) f_estimate<-Xbeta_GAM - Xbeta_Final  # A- and L-optimality and LmAMSE model misspecified subsampling for poisson regression  NeEDS4BigData::modelMissPoiSub(r1=300,r2=rep_k,                                Y=as.matrix(Original_Data[,1]),                                X=as.matrix(Original_Data[,-1]),                                N=N,Alpha=10, Beta_Estimate_Full = beta.prop,                                F_Estimate_Full = f_estimate)->Results ## Step 1 of the algorithm completed. ## Step 2 of the algorithm completed. Final_Beta_modelMiss<-Results$Beta_Estimates Final_Loss_modelMiss<-Results$Loss_Estimates  matrix(rep(beta.prop,by=nrow(Final_Beta_modelMiss)),nrow = nrow(Final_Beta_modelMiss),        ncol = ncol(Final_Beta_modelMiss[,-c(1,2)]),byrow = TRUE)->All_Beta  # Plots for the mean squared error of the model parameter estimates  # and the loss for the main effects model  MSE_Beta_modelMiss<-data.frame(\"Method\"=Final_Beta_modelMiss$Method,                                \"Subsample\"=Final_Beta_modelMiss$r2,                                \"MSE\"=rowSums((All_Beta - Final_Beta_modelMiss[,-c(1,2)])^2))   ggplot(MSE_Beta_modelMiss |> dplyr::group_by(Method,Subsample) |>           dplyr::summarise(MSE=mean(MSE), .groups = 'drop'),        aes(x=factor(Subsample),y=MSE,color=Method,group=Method,linetype=Method,shape=Method))+   geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"MSE\")+   scale_color_manual(values = Method_Color)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p1  ggplot(Final_Loss_modelMiss |> dplyr::group_by(r2,Method) |>           dplyr::summarise(meanLoss=mean(Loss), .groups = 'drop'),        aes(x=factor(r2),y=meanLoss,color=Method,group=Method,linetype=Method,shape=Method)) +   geom_point()+geom_line()+xlab(\"Subsample size\")+ylab(\"Mean Loss\")+   scale_color_manual(values = Method_Color)+   scale_linetype_manual(values=Method_Line_Types)+   scale_shape_manual(values = Method_Shape_Types)+   theme_bw()+guides(colour = guide_legend(nrow = 3))+Theme_special()->p2  # ggarrange(p1,p2,nrow = 2,ncol = 1,labels = \"auto\", #           common.legend = TRUE,legend = \"bottom\") p2"},{"path":[]},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Amalan Mahendran. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mahendran (2024). NeEDS4BigData: New Experimental Design based Subsampling Methods Big Data. R package version 1.0.0, https://amalan-constat.github.io/NeEDS4BigData/index.html, https://github.com/Amalan-ConStat/NeEDS4BigData.","code":"@Manual{,   title = {NeEDS4BigData: New Experimental Design based Subsampling Methods for Big Data},   author = {Amalan Mahendran},   year = {2024},   note = {R package version 1.0.0, https://amalan-constat.github.io/NeEDS4BigData/index.html},   url = {https://github.com/Amalan-ConStat/NeEDS4BigData}, }"},{"path":"/CONDUCT.html","id":null,"dir":"","previous_headings":"","what":"Contributor Code of Conduct","title":"Contributor Code of Conduct","text":"contributors maintainers project, pledge respect people contribute reporting issues, posting feature requests, updating documentation, submitting pull requests patches, activities. committed making participation project harassment-free experience everyone, regardless level experience, gender, gender identity expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion. Examples unacceptable behavior participants include use sexual language imagery, derogatory comments personal attacks, trolling, public private harassment, insults, unprofessional conduct. Project maintainers right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct. Project maintainers follow Code Conduct may removed project team. Instances abusive, harassing, otherwise unacceptable behavior may reported opening issue contacting one project maintainers. Code Conduct adapted Contributor Covenant (http:contributor-covenant.org), version 1.0.0, available http://contributor-covenant.org/version/1/0/0/","code":""},{"path":"/index.html","id":"needs4bigdata-","dir":"","previous_headings":"","what":"New Experimental Design based Subsampling Methods for Big Data","title":"New Experimental Design based Subsampling Methods for Big Data","text":"R package “NeEDS4BigData” holds subsampling methods can implemented big data. continuously increasing big data Big Data: Survey.","code":""},{"path":"/index.html","id":"how-did-the-name-needs4bigdata-came-through-","dir":"","previous_headings":"","what":"How did the name “NeEDS4BigData” came through ?","title":"New Experimental Design based Subsampling Methods for Big Data","text":"New Experimental Design based Subsampling methods Big Data.","code":""},{"path":"/index.html","id":"how-to-engage-with-needs4bigdata-the-first-time-","dir":"","previous_headings":"","what":"How to engage with “NeEDS4BigData” the first time ?","title":"New Experimental Design based Subsampling Methods for Big Data","text":"","code":"## Installing the package from GitHub devtools::install_github(\"Amalan-ConStat/NeEDS4BigData\")  ## Installing the package from CRAN install.packages(\"NeEDS4BigData\")"},{"path":"/index.html","id":"what-does-needs4bigdata-","dir":"","previous_headings":"","what":"What does “NeEDS4BigData” ?","title":"New Experimental Design based Subsampling Methods for Big Data","text":"Algorithms implemented Generalised Linear Models (GLMs) big data obtain informative subsamples.","code":""},{"path":"/index.html","id":"subsampling-methods","dir":"","previous_headings":"","what":"Subsampling Methods","title":"New Experimental Design based Subsampling Methods for Big Data","text":"- L-optimality based subsampling GLMs. -optimality based subsampling Gaussian Linear Models. Leverage sampling GLMs. Local case control sampling logistic regression. -optimality based subsampling measurement constraints GLMs. Model Robust subsampling method GLMs. Subsampling method GLMs model potentially misspecified. explain methods detail 4 articles written , Introduction - explains need subsampling methods. Linear Regression - implementing subsampling methods linear regression 3 scenarios. Logistic Regression - implementing subsampling methods logistic regression 3 scenarios. Poisson Regression - implementing subsampling methods Poisson regression 3 scenarios. 3 scenarios mentioned , First scenario - assume main effects can explicitly describe big data. Second scenario - assume set models can appropriately describe big data. Third scenario - main effects model potentially misspecified.","code":""},{"path":[]},{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 NeEDS4BigData authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":null,"dir":"Reference","previous_headings":"","what":"A- and L-optimality criterion based subsampling under Generalised Linear Models — ALoptimalGLMSub","title":"A- and L-optimality criterion based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"Using function subsample big data linear, logistic Poisson regression describe data. Subsampling probabilities obtained based - L- optimality criterions.","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A- and L-optimality criterion based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"","code":"ALoptimalGLMSub(r1,r2,Y,X,N,family)"},{"path":"/reference/ALoptimalGLMSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A- and L-optimality criterion based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"r1 subsample size initial random sampling r2 subsample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data family character value \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A- and L-optimality criterion based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"output ALoptimalGLMSub gives list Beta_Estimates estimated model parameters data.frame subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon data.frame subsampling (valid linear regression) Utility_Estimates estimated log scaled Information variance estimated model parameters Sample_A-Optimality list indexes initial optimal subsamples obtained based -Optimality criterion Sample_L-Optimality list indexes initial optimal subsamples obtained based L-Optimality criterion Subsampling_Probability matrix calculated subsampling probabilities - L- optimality criterion","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A- and L-optimality criterion based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"Two stage subsampling algorithm big data Generalised Linear Models (linear, logistic Poisson regression). First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters subsampling probabilities evaluated - L-optimality criterion. estimated subsampling probabilities optimal subsample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. character value provided family three types error message produced.","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"A- and L-optimality criterion based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"Wang H, Zhu R, Ma P (2018). “Optimal subsampling large sample logistic regression.” Journal American Statistical Association, 113(522), 829--844.  Ai M, Yu J, Zhang H, Wang H (2021). “Optimal subsampling algorithms big data regressions.” Statistica Sinica, 31(2), 749--772.  Yao Y, Wang H (2021). “review optimal subsampling methods massive datasets.” Journal Data Science, 19(1), 151--172.","code":""},{"path":"/reference/ALoptimalGLMSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A- and L-optimality criterion based subsampling under Generalised Linear Models — ALoptimalGLMSub","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"linear\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-600; Original_Data<-Full_Data$Complete_Data;  ALoptimalGLMSub(r1 = r1, r2 = r2,                 Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                 X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                 family = \"linear\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  #plot_Utility(Results)  Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"logistic\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-600; Original_Data<-Full_Data$Complete_Data;  ALoptimalGLMSub(r1 = r1, r2 = r2,                 Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                 X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                 family = \"logistic\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  plot_Utility(Results) #> `geom_line()`: Each group consists of only one observation. #> ℹ Do you need to adjust the group aesthetic? #> `geom_line()`: Each group consists of only one observation. #> ℹ Do you need to adjust the group aesthetic?   Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"poisson\" Full_Data<-GenGLMdata(Dist,NULL,No_Of_Var,Beta,N,Family)  r1<-300; r2<-600; Original_Data<-Full_Data$Complete_Data;  ALoptimalGLMSub(r1 = r1, r2 = r2,                 Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                 X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                 family = \"poisson\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  plot_Utility(Results) #> `geom_line()`: Each group consists of only one observation. #> ℹ Do you need to adjust the group aesthetic? #> `geom_line()`: Each group consists of only one observation. #> ℹ Do you need to adjust the group aesthetic?"},{"path":"/reference/AoptimalGauLMSub.html","id":null,"dir":"Reference","previous_headings":"","what":"A-optimality criterion based subsampling under Gaussian Linear Models — AoptimalGauLMSub","title":"A-optimality criterion based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"Using function subsample big data Gaussian linear regression models describe data. Subsampling probabilities obtained based -optimality criterions.","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A-optimality criterion based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"","code":"AoptimalGauLMSub(r1,r2,Y,X,N)"},{"path":"/reference/AoptimalGauLMSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A-optimality criterion based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"r1 subsample size initial random sampling r2 subsample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A-optimality criterion based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"output AoptimalGauLMSub gives list Beta_Estimates estimated model parameters data.frame subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon data.frame subsampling Sample_A-Optimality list indexes initial optimal subsamples obtained based -Optimality criterion Subsampling_Probability matrix calculated subsampling probabilities -optimality criterion","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A-optimality criterion based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"Two stage subsampling algorithm big data Gaussian Linear Model. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters subsampling probabilities evaluated -optimality criterion. estimated subsampling probabilities optimal subsample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced.","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"A-optimality criterion based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"Lee J, Schifano ED, Wang H (2021). “Fast optimal subsampling probability approximation generalized linear models.” Econometrics Statistics.","code":""},{"path":"/reference/AoptimalGauLMSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A-optimality criterion based subsampling under Gaussian Linear Models — AoptimalGauLMSub","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"linear\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,9,12),50); Original_Data<-Full_Data$Complete_Data;  AoptimalGauLMSub(r1 = r1, r2 = r2,                  Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),                  N = nrow(Original_Data))->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"/reference/AoptimalMCGLMSub.html","id":null,"dir":"Reference","previous_headings":"","what":"A-optimality criterion based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","title":"A-optimality criterion based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"Using function subsample big data linear, logistic Poisson regression describe data response \\(y\\) partially unavailable. Subsampling probabilities obtained based -optimality criterion.","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A-optimality criterion based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"","code":"AoptimalMCGLMSub(r1,r2,Y,X,N,family)"},{"path":"/reference/AoptimalMCGLMSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A-optimality criterion based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"r1 subsample size initial random sampling r2 subsample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data family character value \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A-optimality criterion based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"output AoptimalMCGLMSub gives list Beta_Estimates estimated model parameters data.frame subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon data.frame subsampling (valid linear regression) Sample_A-Optimality list indexes initial optimal subsamples obtained based -Optimality criterion Subsampling_Probability matrix calculated subsampling probabilities -optimality criterion","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"A-optimality criterion based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"Two stage subsampling algorithm big data Generalised Linear Models (linear, logistic Poisson regression) response available subsampling probability evaluation. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters subsampling probabilities evaluated -optimality criterion. estimated subsampling probabilities optimal subsample size \\(r_2 \\ge r_1\\) obtained. Finally, optimal sample used model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. character value provided family three types error message produced.","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"A-optimality criterion based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"Zhang T, Ning Y, Ruppert D (2021). “Optimal sampling generalized linear models measurement constraints.” Journal Computational Graphical Statistics, 30(1), 106--114.","code":""},{"path":"/reference/AoptimalMCGLMSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A-optimality criterion based subsampling under measurement constraints for Generalised Linear Models — AoptimalMCGLMSub","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"linear\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,9,12),50); Original_Data<-Full_Data$Complete_Data;  AoptimalMCGLMSub(r1 = r1, r2 = r2,                  Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  family = \"linear\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"logistic\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,9,12),50); Original_Data<-Full_Data$Complete_Data;  AoptimalMCGLMSub(r1 = r1, r2 = r2,                  Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  family = \"logistic\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"poisson\" Full_Data<-GenGLMdata(Dist,NULL,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,9,12),50); Original_Data<-Full_Data$Complete_Data;  AoptimalMCGLMSub(r1 = r1, r2 = r2,                  Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  family = \"poisson\")->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"/reference/Bike_sharing.html","id":null,"dir":"Reference","previous_headings":"","what":"Bike sharing data — Bike_sharing","title":"Bike sharing data — Bike_sharing","text":"Fanaee-T (2013) collected data understand bike sharing demands rental return process. data total contains 17,379 observations consider covariates temperature, humidity windspeed model response, number bikes rented hourly.","code":""},{"path":"/reference/Bike_sharing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bike sharing data — Bike_sharing","text":"","code":"Bike_sharing"},{"path":"/reference/Bike_sharing.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Bike sharing data — Bike_sharing","text":"data frame 4 columns 17,379 rows. Rented_Bikes Number bikes rented hourly Temperature Hourly temperature Humidity Hourly humidity Windspeed Hourly windspeed","code":""},{"path":"/reference/Bike_sharing.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Bike sharing data — Bike_sharing","text":"Extracted Fanaee-T H (2013) Bike Sharing. UCI Machine Learning Repository. Available : doi:10.24432/C5W894","code":""},{"path":"/reference/Bike_sharing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bike sharing data — Bike_sharing","text":"","code":"nrow(Bike_sharing) #> [1] 17379"},{"path":"/reference/Electric_consumption.html","id":null,"dir":"Reference","previous_headings":"","what":"Electric consumption data — Electric_consumption","title":"Electric consumption data — Electric_consumption","text":"Hebrail Berard (2012) described data contains 2,049,280 completed measurements house located Sceaux, France December 2006 November 2010. log scale minute-averaged current intensity selected response covariates active electrical energy (watt-hour) kitchen, laundry room, electric water-heater air-conditioner.","code":""},{"path":"/reference/Electric_consumption.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Electric consumption data — Electric_consumption","text":"","code":"Electric_consumption"},{"path":"/reference/Electric_consumption.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Electric consumption data — Electric_consumption","text":"data frame 4 columns 2,049,280 rows. Intensity Minute-averaged current intensity EE_Kitchen Active electrical energy (watt-hour) kitchen EE_Laundry Active electrical energy (watt-hour) laundry room EE_WH_AC Active electrical energy (watt-hour) electric water-heater air-conditioner","code":""},{"path":"/reference/Electric_consumption.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Electric consumption data — Electric_consumption","text":"Extracted Hebrail G, Berard (2012) Individual Household Electric Power Consumption. UCI Machine Learning Repository. Available : doi:10.24432/C58K54","code":""},{"path":"/reference/Electric_consumption.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Electric consumption data — Electric_consumption","text":"","code":"nrow(Electric_consumption) #> [1] 2049280"},{"path":"/reference/GenGLMdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate data for Generalised Linear Models — GenGLMdata","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"Function simulate big data linear, logistic Poisson regression subsampling. Covariate data X Normal Uniform distribution linear regression. Covariate data X Exponential Normal Uniform distribution logistic regression. Covariate data X Normal Uniform distribution Poisson regression.","code":""},{"path":"/reference/GenGLMdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"","code":"GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,family)"},{"path":"/reference/GenGLMdata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"Dist character value distribution \"Normal\" \"Uniform \"Exponential\" Dist_Par list parameters distribution generate data covariate X No_Of_Var number variables Beta vector model parameters, including intercept N big data size family character vector \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/GenGLMdata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"output GenGLMData gives list Basic list outputs based inputs Beta Estimates models Complete_Data matrix Y X","code":""},{"path":"/reference/GenGLMdata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"Big data Generalised Linear Models generated \"linear\", \"logistic\" \"poisson\" regression types. limited covariate data generation linear regression normal uniform distribution, logistic regression exponential, normal uniform Poisson regression normal uniform distribution.","code":""},{"path":"/reference/GenGLMdata.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"Nelder, J. ., & Wedderburn, R. W. (1972). Generalized linear models. Journal Royal Statistical Society Series : Statistics Society, 135(3), 370-384.","code":""},{"path":"/reference/GenGLMdata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate data for Generalised Linear Models — GenGLMdata","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"linear\" Results<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"logistic\" Results<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"poisson\" Results<-GenGLMdata(Dist,NULL,No_Of_Var,Beta,N,Family)"},{"path":"/reference/GenModelMissGLMdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"Function simulate big data Generalised Linear Models model misspecification scenario misspecification type.","code":""},{"path":"/reference/GenModelMissGLMdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"","code":"GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon,N,MisspecificationType,family)"},{"path":"/reference/GenModelMissGLMdata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"No_Of_Var number variables Beta vector model parameters, including intercept Var_Epsilon variance value residuals N big data size MisspecificationType character vector referring different types misspecification family character vector \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/GenModelMissGLMdata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"output GenModelMissGLMdata gives list N big data size Beta list outputs(real estimated) beta values Variance_Epsilon list outputs(real estimated) variance epsilon Xbeta list outputs(real estimated) linear predictor f list outputs(real estimated) misspecification Real_Full_Data matrix Y,X f(x) Full_Data matrix Y X","code":""},{"path":"/reference/GenModelMissGLMdata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"Big data Generalised Linear Models generated \"linear\", \"logistic\" \"poisson\" regression types model misspecification. limited covariate data generation uniform distribution limits \\((-1,1)\\). Different type misspecifications \"Type 1\", \"Type 2 Squared\", \"Type 2 Interaction\", \"Type 3 Squared\" \"Type 3 Interaction\".","code":""},{"path":"/reference/GenModelMissGLMdata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate data for Generalised Linear Models under model misspecification scenario — GenModelMissGLMdata","text":"","code":"No_Of_Var<-2; Beta<-c(-1,2,2,1); Var_Epsilon<-0.5; N<-10000; MisspecificationType <- \"Type 2 Squared\"; family <- \"linear\"  Results<-GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon,N,MisspecificationType,family)  No_Of_Var<-2; Beta<-c(-1,2,2,1); N<-10000; MisspecificationType <- \"Type 2 Squared\"; family <- \"logistic\"  Results<-GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon=NULL,N,MisspecificationType,family)  No_Of_Var<-2; Beta<-c(-1,2,2,1); N<-10000; MisspecificationType <- \"Type 2 Squared\"; family <- \"poisson\"  Results<-GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon=NULL,N,MisspecificationType,family)"},{"path":"/reference/GenModelRobustGLMdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","title":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","text":"Function simulate big data linear, logistic Poisson regression model robust scenario set models. Covariate data X Normal Uniform distribution linear regression. Covariate data X Exponential Normal Uniform distribution logistic regression. Covariate data X Normal Uniform distribution Poisson regression.","code":""},{"path":"/reference/GenModelRobustGLMdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","text":"","code":"GenModelRobustGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,All_Models,family)"},{"path":"/reference/GenModelRobustGLMdata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","text":"Dist character value distribution \"Normal\" \"Uniform Dist_Par list parameters distribution generate data covariate X No_Of_Var number variables Beta vector model parameters, including intercept N big data size All_Models list contains possible models family character vector \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/GenModelRobustGLMdata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","text":"output GenModelRobustGLMdata gives list Basic list outputs based inputs Beta Estimates models Complete_Data matrix Y,X X^2","code":""},{"path":"/reference/GenModelRobustGLMdata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","text":"Big data Generalised Linear Models generated \"linear\", \"logistic\" \"poisson\" regression types. limited covariate data generation linear regression normal uniform distribution, logistic regression exponential, normal uniform Poisson regression normal uniform distribution. given real model data generated data modelled All_Models.","code":""},{"path":"/reference/GenModelRobustGLMdata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate data for Generalised Linear Models under model robust scenario — GenModelRobustGLMdata","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1,2); N<-10000 All_Models<-list(Real_Model=c(\"X0\",\"X1\",\"X2\",\"X1^2\"),                  Assumed_Model_1=c(\"X0\",\"X1\",\"X2\"),                  Assumed_Model_2=c(\"X0\",\"X1\",\"X2\",\"X2^2\"),                  Assumed_Model_3=c(\"X0\",\"X1\",\"X2\",\"X1^2\",\"X2^2\")) family<-\"linear\"  Results<-GenModelRobustGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,All_Models,family)  Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1,2); N<-10000 All_Models<-list(Real_Model=c(\"X0\",\"X1\",\"X2\",\"X1^2\"),                  Assumed_Model_1=c(\"X0\",\"X1\",\"X2\"),                  Assumed_Model_2=c(\"X0\",\"X1\",\"X2\",\"X2^2\"),                  Assumed_Model_3=c(\"X0\",\"X1\",\"X2\",\"X1^2\",\"X2^2\")) family = \"logistic\"  Results<-GenModelRobustGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,All_Models,family) #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred  Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,2,1,2); N<-10000 All_Models<-list(Real_Model=c(\"X0\",\"X1\",\"X2\",\"X1^2\"),                  Assumed_Model_1=c(\"X0\",\"X1\",\"X2\"),                  Assumed_Model_2=c(\"X0\",\"X1\",\"X2\",\"X2^2\"),                  Assumed_Model_3=c(\"X0\",\"X1\",\"X2\",\"X1^2\",\"X2^2\")) family = \"poisson\"  Results<-GenModelRobustGLMdata(Dist,Dist_Par=NULL,No_Of_Var,Beta,N,All_Models,family)"},{"path":"/reference/LCCsampling.html","id":null,"dir":"Reference","previous_headings":"","what":"Local case control sampling for logistic regression — LCCsampling","title":"Local case control sampling for logistic regression — LCCsampling","text":"Using function subsample big data logistic regression describe data. Subsampling probabilities obtained based local case control method.","code":""},{"path":"/reference/LCCsampling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Local case control sampling for logistic regression — LCCsampling","text":"","code":"LCCsampling(r1,r2,Y,X,N)"},{"path":"/reference/LCCsampling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Local case control sampling for logistic regression — LCCsampling","text":"r1 subsample size initial random sampling r2 subsample size local case control sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data","code":""},{"path":"/reference/LCCsampling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Local case control sampling for logistic regression — LCCsampling","text":"output LCCsampling gives list Beta_Estimates estimated model parameters data.frame subsampling Utility_Estimates estimated log scaled Information variance estimated model parameters Sample_LCC_Sampling list indexes initial optimal subsamples obtained based local case control sampling Subsampling_Probability vector calculated subsampling probabilities local case control sampling","code":""},{"path":"/reference/LCCsampling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Local case control sampling for logistic regression — LCCsampling","text":"Two stage sampling algorithm big data logistic regression. First obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters sampling probabilities evaluated local case control. estimated sampling probabilities optimal sample size \\(r_2 \\ge r_1\\) obtained. Finally, optimal sample used model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced.","code":""},{"path":"/reference/LCCsampling.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Local case control sampling for logistic regression — LCCsampling","text":"Fithian W, Hastie T (2015). “Local case-control sampling: Efficient subsampling imbalanced data sets.” Quality control applied statistics, 60(3), 187--190.","code":""},{"path":"/reference/LCCsampling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Local case control sampling for logistic regression — LCCsampling","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-10000; Family<-\"logistic\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r1<-300; r2<-rep(100*c(6,9,12),50); Original_Data<-Full_Data$Complete_Data;  LCCsampling(r1 = r1, r2 = r2, Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),             X = as.matrix(Original_Data[,-1]),             N = nrow(Original_Data))->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  #plot_Utility(Results)"},{"path":"/reference/LeverageSampling.html","id":null,"dir":"Reference","previous_headings":"","what":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"Using function subsample big data linear, logistic Poisson regression describe data. Subsampling probabilities obtained based basic shrinkage leverage method.","code":""},{"path":"/reference/LeverageSampling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"","code":"LeverageSampling(r,Y,X,N,alpha,family)"},{"path":"/reference/LeverageSampling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"r subsample size Y response data Y X covariate data X matrix covariates (first column intercept) N size big data alpha shrinkage factor 0 1 family character vector \"linear\", \"logistic\" \"poisson\" regression Generalised Linear Models","code":""},{"path":"/reference/LeverageSampling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"output LeverageSampling gives list Beta_Estimates estimated model parameters data.frame subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon data.frame subsampling (valid linear regression) Sample_Basic_Leverage list indexes optimal subsamples obtained based basic leverage Sample_Shrinkage_Leverage list indexes optimal subsamples obtained based shrinkage leverage Subsampling_Probability matrix calculated subsampling probabilities basic shrinkage leverage","code":""},{"path":"/reference/LeverageSampling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"Leverage sampling algorithm big data Generalised Linear Models (linear, logistic Poisson regression). First obtain random sample size \\(min(r)/2\\) estimate model parameters. Using estimated parameters leverage scores evaluated leverage sampling. estimated leverage scores sample size \\(r\\) obtained. Finally, sample size \\(r\\) used model parameters estimated. NOTE : input parameters given domain conditions necessary error messages provided go . \\(r\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. \\(0 < \\alpha < 1\\) satisfied error message produced. character vector provided family three types error message produced.","code":""},{"path":"/reference/LeverageSampling.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"Ma P, Mahoney M, Yu B (2014). “statistical perspective algorithmic leveraging.” International conference machine learning, 91--99. PMLR.  Ma P, Sun X (2015). “Leveraging big data regression.” Wiley Interdisciplinary Reviews: Computational Statistics, 7(1), 70--76.","code":""},{"path":"/reference/LeverageSampling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Basic and shrinkage leverage sampling for Generalised Linear Models — LeverageSampling","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"linear\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r<-100*c(6,10); Original_Data<-Full_Data$Complete_Data;  LeverageSampling(r = r, Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  alpha = 0.95,                  family = \"linear\")->Results #> Basic and shrinkage leverage probabilities calculated. #> Subsampling completed.  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"logistic\" Full_Data<-GenGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,Family)  r<-100*c(6,10); Original_Data<-Full_Data$Complete_Data;  LeverageSampling(r = r, Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  alpha = 0.95,                  family = \"logistic\")->Results #> Basic and shrinkage leverage probabilities calculated. #> Subsampling completed.  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.   Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,2,1); N<-5000; Family<-\"poisson\" Full_Data<-GenGLMdata(Dist,NULL,No_Of_Var,Beta,N,Family)  r<-100*c(6,10); Original_Data<-Full_Data$Complete_Data;  LeverageSampling(r = r, Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                  X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                  alpha = 0.95,                  family = \"poisson\")->Results #> Basic and shrinkage leverage probabilities calculated. #> Subsampling completed.  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"/reference/modelMissLinSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Model misspecified subsampling under linear regression — modelMissLinSub","title":"Model misspecified subsampling under linear regression — modelMissLinSub","text":"Using function subsample big data linear regression assumed model describes data misspecified. Subsampling probabilities obtained based - L- optimality criterions LmAMSE (Loss mean asymptotic mean squared error).","code":""},{"path":"/reference/modelMissLinSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model misspecified subsampling under linear regression — modelMissLinSub","text":"","code":"modelMissLinSub(r1,r2,Y,X,N,Alpha,Var_GAM_Full,Var_Full,F_Estimate_Full)"},{"path":"/reference/modelMissLinSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model misspecified subsampling under linear regression — modelMissLinSub","text":"r1 subsample size initial random sampling r2 subsample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha scaling factor using Log Odds Power functions magnify probabilities Var_GAM_Full estimate Var_Epsilon fitting GAM model Var_Full estimate Var_Epsilon fitting linear regression model F_Estimate_Full estimate f difference linear predictor GAM linear model","code":""},{"path":"/reference/modelMissLinSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model misspecified subsampling under linear regression — modelMissLinSub","text":"output modelMissLinSub gives list Beta_Estimates estimated model parameters subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon subsampling Loss_Estimates matrix estimated LmAMSE values subsampling Sample_A-Optimality list indexes initial optimal subsamples obtained based -Optimality criterion Sample_L-Optimality list indexes initial optimal subsamples obtained based L-Optimality criterion Sample_LmAMSE list indexes optimal subsamples obtained based obtained based LmAMSE Sample_LmAMSE_Log_Odds list indexes optimal subsamples obtained based LmAMSE Log Odds function Sample_LmAMSE_Power list indexes optimal subsamples obtained based LmAMSE Power function Subsampling_Probability matrix calculated subsampling probabilities","code":""},{"path":"/reference/modelMissLinSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model misspecified subsampling under linear regression — modelMissLinSub","text":"Two stage subsampling algorithm big data linear regression potential model misspecification. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters subsampling probabilities evaluated -, L-optimality criterion, LmAMSE enhanced LmAMSE(log-odds power) subsampling methods. estimated subsampling probabilities subsample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated - L-optimality, LmAMSE enhanced LmAMSE (log-odds power) optimal subsample used. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\),F_estimate_Full aligned error message produced. \\(\\alpha > 1\\) scaling factor satisfied error message produced.","code":""},{"path":"/reference/modelMissLinSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Model misspecified subsampling under linear regression — modelMissLinSub","text":"Adewale AJ, Wiens DP (2009). “Robust designs misspecified logistic models.” Journal Statistical Planning Inference, 139(1), 3--15.  Adewale AJ, Xu X (2010). “Robust designs generalized linear models possible overdispersion misspecified link functions.” Computational statistics & data analysis, 54(4), 875--890.","code":""},{"path":"/reference/modelMissLinSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model misspecified subsampling under linear regression — modelMissLinSub","text":"","code":"No_Of_Var<-2; Beta<-c(-1,2,2,1); Var_Epsilon<-0.5; N<-10000; MisspecificationType <- \"Type 2 Squared\"; family <- \"linear\"  Full_Data<-GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon,N,MisspecificationType,family)  r1<-300; r2<-100*c(6,9); Original_Data<-Full_Data$Full_Data;  # cl <- parallel::makeCluster(4) # doParallel::registerDoParallel(cl)  Results<-modelMissLinSub(r1 = r1, r2 = r2,                          Y = as.matrix(Original_Data[,1]),                          X = as.matrix(Original_Data[,-1]),                          N = Full_Data$N,                          Alpha = 10 ,                          Var_GAM_Full = Full_Data$Variance_Epsilon$Real_GAM,                          Var_Full = Full_Data$Variance_Epsilon$Estimate,                          F_Estimate_Full = Full_Data$f$Real_GAM) #> Warning: executing %dopar% sequentially: no parallel backend registered #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  # parallel::stopCluster(cl)  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  plot_LmAMSE(Results)"},{"path":"/reference/modelMissLogSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Model misspecified subsampling under logistic regression — modelMissLogSub","title":"Model misspecified subsampling under logistic regression — modelMissLogSub","text":"Using function subsample big data logistic regression assumed model describes data misspecified. Subsampling probabilities obtained based - L- optimality criterions LmAMSE (Loss mean asymptotic mean squared error).","code":""},{"path":"/reference/modelMissLogSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model misspecified subsampling under logistic regression — modelMissLogSub","text":"","code":"modelMissLogSub(r1,r2,Y,X,N,Alpha,Beta_Estimate_Full,F_Estimate_Full)"},{"path":"/reference/modelMissLogSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model misspecified subsampling under logistic regression — modelMissLogSub","text":"r1 subsample size initial random sampling r2 subsample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha scaling factor using Log Odds Power functions magnify probabilities Beta_Estimate_Full estimate Beta fitting logistic model F_Estimate_Full estimate f difference linear predictor GAM logistic model","code":""},{"path":"/reference/modelMissLogSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model misspecified subsampling under logistic regression — modelMissLogSub","text":"output modelMissLogSub gives list Beta_Estimates estimated model parameters subsampling Loss_Estimates matrix estimated LmAMSE values subsampling Sample_A-Optimality list indexes initial optimal subsamples obtained based -Optimality criterion Sample_L-Optimality list indexes initial optimal subsamples obtained based L-Optimality criterion Sample_LmAMSE list indexes optimal subsamples obtained based LmAMSE Sample_LmAMSE_Log_Odds list indexes optimal subsamples obtained based LmAMSE Log Odds function Sample_LmAMSE_Power list indexes optimal subsamples obtained based LmAMSE Power function Subsampling_Probability matrix calculated subsampling probabilities","code":""},{"path":"/reference/modelMissLogSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model misspecified subsampling under logistic regression — modelMissLogSub","text":"Two stage subsampling algorithm big data logistic regression potential model misspecification. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters subsampling probabilities evaluated -, L-optimality criterion, LmAMSE enhanced LmAMSE(log-odds power) subsampling methods. estimated subsampling probabilities subsample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated - L-optimality, LmAMSE enhanced LmAMSE (log-odds power) optimal subsample used. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\),F_estimate_Full aligned error message produced. \\(\\alpha > 1\\) scaling vector satisfied error message produced.","code":""},{"path":"/reference/modelMissLogSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Model misspecified subsampling under logistic regression — modelMissLogSub","text":"Adewale AJ, Wiens DP (2009). “Robust designs misspecified logistic models.” Journal Statistical Planning Inference, 139(1), 3--15.  Adewale AJ, Xu X (2010). “Robust designs generalized linear models possible overdispersion misspecified link functions.” Computational statistics & data analysis, 54(4), 875--890.","code":""},{"path":"/reference/modelMissLogSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model misspecified subsampling under logistic regression — modelMissLogSub","text":"","code":"No_Of_Var<-2; Beta<-c(-1,2,2,1); N<-10000; MisspecificationType <- \"Type 2 Squared\"; family <- \"logistic\"  Full_Data<-GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon=NULL,N,MisspecificationType,family)  r1<-300; r2<-100*c(6,9); Original_Data<-Full_Data$Full_Data;  # cl <- parallel::makeCluster(4) # doParallel::registerDoParallel(cl)  Results<-modelMissLogSub(r1 = r1, r2 = r2,                          Y = as.matrix(Original_Data[,1]),                          X = as.matrix(Original_Data[,-1]),                          N = Full_Data$N,                          Alpha = 10,                          Beta_Estimate_Full = Full_Data$Beta$Estimate,                          F_Estimate_Full = Full_Data$f$Real_GAM) #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  # parallel::stopCluster(cl)  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  plot_LmAMSE(Results)"},{"path":"/reference/modelMissPoiSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Model misspecified subsampling under Poisson regression — modelMissPoiSub","title":"Model misspecified subsampling under Poisson regression — modelMissPoiSub","text":"Using function subsample big data Poisson regression assumed model describes data misspecified. Subsampling probabilities obtained based - L- optimality criterions LmAMSE (Loss mean asymptotic mean squared error).","code":""},{"path":"/reference/modelMissPoiSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model misspecified subsampling under Poisson regression — modelMissPoiSub","text":"","code":"modelMissPoiSub(r1,r2,Y,X,N,Alpha,Beta_Estimate_Full,F_Estimate_Full)"},{"path":"/reference/modelMissPoiSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model misspecified subsampling under Poisson regression — modelMissPoiSub","text":"r1 subsample size initial random sampling r2 subsample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha scaling factor using Log Odds Power functions magnify probabilities Beta_Estimate_Full estimate Beta fitting Poisson model F_Estimate_Full estimate f difference linear predictor GAM Poisson model","code":""},{"path":"/reference/modelMissPoiSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model misspecified subsampling under Poisson regression — modelMissPoiSub","text":"output modelMissPoiSub gives list Beta_Estimates estimated model parameters subsampling Loss_Estimates matrix estimated LmAMSE values subsampling Sample_A-Optimality list indexes initial optimal subsamples obtained based -Optimality criterion Sample_L-Optimality list indexes initial optimal subsamples obtained based L-Optimality criterion Sample_LmAMSE list indexes optimal subsamples obtained based LmAMSE Sample_LmAMSE_Log_Odds list indexes optimal subsamples obtained based LmAMSE Log Odds function Sample_LmAMSE_Power list indexes optimal subsamples obtained based LmAMSE Power function Subsampling_Probability matrix calculated subsampling probabilities","code":""},{"path":"/reference/modelMissPoiSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model misspecified subsampling under Poisson regression — modelMissPoiSub","text":"Two stage subsampling algorithm big data Poisson regression potential model misspecification. First stage obtain random sample size \\(r_1\\) estimate model parameters. Using estimated parameters subsampling probabilities evaluated -, L-optimality criterion, LmAMSE enhanced LmAMSE(log-odds power) subsampling methods. estimated subsampling probabilities subsample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated - L-optimality, LmAMSE enhanced LmAMSE (log-odds power) optimal subsample used. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\),F_estimate_Full aligned error message produced. \\(\\alpha > 1\\) scaling vector satisfied error message produced.","code":""},{"path":"/reference/modelMissPoiSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Model misspecified subsampling under Poisson regression — modelMissPoiSub","text":"Adewale AJ, Wiens DP (2009). “Robust designs misspecified logistic models.” Journal Statistical Planning Inference, 139(1), 3--15.  Adewale AJ, Xu X (2010). “Robust designs generalized linear models possible overdispersion misspecified link functions.” Computational statistics & data analysis, 54(4), 875--890.","code":""},{"path":"/reference/modelMissPoiSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model misspecified subsampling under Poisson regression — modelMissPoiSub","text":"","code":"No_Of_Var<-2; Beta<-c(-1,2,2,1); N<-10000; MisspecificationType <- \"Type 2 Squared\"; family <- \"poisson\"  Full_Data<-GenModelMissGLMdata(No_Of_Var,Beta,Var_Epsilon=NULL,N,MisspecificationType,family)  r1<-300; r2<-100*c(6,9); Original_Data<-Full_Data$Full_Data;  # cl <- parallel::makeCluster(4) # doParallel::registerDoParallel(cl)  Results<-modelMissPoiSub(r1 = r1, r2 = r2,                          Y = as.matrix(Original_Data[,1]),                          X = as.matrix(Original_Data[,-1]),                          N = Full_Data$N,                          Alpha = 10,                          Beta_Estimate_Full = Full_Data$Beta$Estimate,                          F_Estimate_Full = Full_Data$f$Real_GAM) #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  # parallel::stopCluster(cl)  plot_Beta(Results) #> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  plot_LmAMSE(Results)"},{"path":"/reference/modelRobustLinSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Model robust optimal subsampling for A- and L- optimality criterion under linear regression — modelRobustLinSub","title":"Model robust optimal subsampling for A- and L- optimality criterion under linear regression — modelRobustLinSub","text":"Using function subsample big data linear regression one model describe data. Subsampling probabilities obtained based - L- optimality criterions.","code":""},{"path":"/reference/modelRobustLinSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model robust optimal subsampling for A- and L- optimality criterion under linear regression — modelRobustLinSub","text":"","code":"modelRobustLinSub(r1,r2,Y,X,N,Alpha,All_Combinations,All_Covariates)"},{"path":"/reference/modelRobustLinSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model robust optimal subsampling for A- and L- optimality criterion under linear regression — modelRobustLinSub","text":"r1 subsample size initial random sampling r2 subsample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha vector alpha values used obtain model robust subsampling probabilities All_Combinations list possible models can describe data All_Covariates covariates models","code":""},{"path":"/reference/modelRobustLinSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model robust optimal subsampling for A- and L- optimality criterion under linear regression — modelRobustLinSub","text":"output modelRobustLinSub gives list Beta_Estimates estimated model parameters model list subsampling Variance_Epsilon_Estimates matrix estimated variance epsilon model subsampling Sample_A-Optimality list indexes initial optimal subsamples obtained based -Optimality criterion Sample_A-Optimality_MR list indexes initial model robust optimal subsamples obtained based -Optimality criterion Sample_L-Optimality list indexes initial optimal subsamples obtained based L-Optimality criterion Sample_L-Optimality_MR list indexes initial model robust optimal subsamples obtained based L-Optimality criterion Subsampling_Probability matrix calculated subsampling probabilities - L- optimality criterion","code":""},{"path":"/reference/modelRobustLinSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model robust optimal subsampling for A- and L- optimality criterion under linear regression — modelRobustLinSub","text":"Two stage subsampling algorithm big data linear regression multiple models can describe big data. First stage obtain random sample size \\(r_1\\) estimate model parameters models. Using estimated parameters subsampling probabilities evaluated -, L-optimality criterion model averaging -, L-optimality subsampling methods. estimated subsampling probabilities subsample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated models. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. \\(0 < \\alpha < 1\\) priori probabilities satisfied error message produced.","code":""},{"path":"/reference/modelRobustLinSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Model robust optimal subsampling for A- and L- optimality criterion under linear regression — modelRobustLinSub","text":"Mahendran , Thompson H, McGree JM (2023). “model robust subsampling approach Generalised Linear Models big data settings.” Statistical Papers, 64(4), 1137--1157.","code":""},{"path":"/reference/modelRobustLinSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model robust optimal subsampling for A- and L- optimality criterion under linear regression — modelRobustLinSub","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1,Error_Variance=0.5) No_Of_Var<-2; Beta<-c(-1,2,1,2); N<-10000 All_Models<-list(Real_Model=c(\"X0\",\"X1\",\"X2\",\"X1^2\"),                  Assumed_Model_1=c(\"X0\",\"X1\",\"X2\"),                  Assumed_Model_2=c(\"X0\",\"X1\",\"X2\",\"X2^2\"),                  Assumed_Model_3=c(\"X0\",\"X1\",\"X2\",\"X1^2\",\"X2^2\")) family = \"linear\"  Full_Data<-GenModelRobustGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,All_Models,family)  r1<-300; r2<-rep(100*c(6,9,12),25); Original_Data<-Full_Data$Complete_Data;  modelRobustLinSub(r1 = r1, r2 = r2,                   Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                   X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                   Alpha = rep(1/length(All_Models),length(All_Models)),                   All_Combinations = All_Models,                   All_Covariates = colnames(Original_Data)[-1])->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  Beta_Plots<-plot_Beta(Results) # Utility_Plots<-plot_Utility(Results)"},{"path":"/reference/modelRobustLogSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Model robust optimal subsampling for A- and L- optimality criterion under logistic regression — modelRobustLogSub","title":"Model robust optimal subsampling for A- and L- optimality criterion under logistic regression — modelRobustLogSub","text":"Using function subsample big data logistic regression one model describe data. Subsampling probabilities obtained based - L- optimality criterions.","code":""},{"path":"/reference/modelRobustLogSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model robust optimal subsampling for A- and L- optimality criterion under logistic regression — modelRobustLogSub","text":"","code":"modelRobustLogSub(r1,r2,Y,X,N,Alpha,All_Combinations,All_Covariates)"},{"path":"/reference/modelRobustLogSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model robust optimal subsampling for A- and L- optimality criterion under logistic regression — modelRobustLogSub","text":"r1 subsample size initial random sampling r2 subsample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha vector alpha values used obtain model robust subsampling probabilities All_Combinations list possible models can describe data All_Covariates covariates models","code":""},{"path":"/reference/modelRobustLogSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model robust optimal subsampling for A- and L- optimality criterion under logistic regression — modelRobustLogSub","text":"output modelRobustLinSub gives list Beta_Data estimated model parameters model list subsampling Utility_Data estimated Variance Information model parameters subsampling Sample_L-optimality list indexes initial optimal subsamples obtained based L-optimality criterion Sample_L-optimality_MR list indexes initial model robust optimal subsamples obtained based L-optimality criterion Sample_A-optimality list indexes initial optimal subsamples obtained based -optimality criterion Sample_A-optimality_MR list indexes initial model robust optimal subsamples obtained based -optimality criterion Subsampling_Probability matrix calculated subsampling probabilities - L- optimality criterion","code":""},{"path":"/reference/modelRobustLogSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model robust optimal subsampling for A- and L- optimality criterion under logistic regression — modelRobustLogSub","text":"Two stage subsampling algorithm big data logistic regression multiple models can describe big data. First stage obtain random sample size \\(r_1\\) estimate model parameters models. Using estimated parameters subsampling probabilities evaluated -, L-optimality criterion model averaging -, L-optimality subsampling methods. estimated subsampling probabilities subsample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated models. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. \\(0 < \\alpha < 1\\) priori probabilities satisfied error message produced.","code":""},{"path":"/reference/modelRobustLogSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Model robust optimal subsampling for A- and L- optimality criterion under logistic regression — modelRobustLogSub","text":"Mahendran , Thompson H, McGree JM (2023). “model robust subsampling approach Generalised Linear Models big data settings.” Statistical Papers, 64(4), 1137--1157.","code":""},{"path":"/reference/modelRobustLogSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model robust optimal subsampling for A- and L- optimality criterion under logistic regression — modelRobustLogSub","text":"","code":"Dist<-\"Normal\"; Dist_Par<-list(Mean=0,Variance=1) No_Of_Var<-2; Beta<-c(-1,2,1,2); N<-10000 All_Models<-list(Real_Model=c(\"X0\",\"X1\",\"X2\",\"X1^2\"),                  Assumed_Model_1=c(\"X0\",\"X1\",\"X2\"),                  Assumed_Model_2=c(\"X0\",\"X1\",\"X2\",\"X2^2\"),                  Assumed_Model_3=c(\"X0\",\"X1\",\"X2\",\"X1^2\",\"X2^2\")) family = \"logistic\"  Full_Data<-GenModelRobustGLMdata(Dist,Dist_Par,No_Of_Var,Beta,N,All_Models,family) #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred #> Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred  r1<-300; r2<-rep(100*c(6,9,12),25); Original_Data<-Full_Data$Complete_Data;  modelRobustLogSub(r1 = r1, r2 = r2,                   Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                   X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                   Alpha = rep(1/length(All_Models),length(All_Models)),                   All_Combinations = All_Models,                   All_Covariates = colnames(Original_Data)[-1])->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  Beta_Plots<-plot_Beta(Results) Utility_Plots<-plot_Utility(Results)"},{"path":"/reference/modelRobustPoiSub.html","id":null,"dir":"Reference","previous_headings":"","what":"Model robust optimal subsampling for A- and L- optimality criterion under Poisson regression — modelRobustPoiSub","title":"Model robust optimal subsampling for A- and L- optimality criterion under Poisson regression — modelRobustPoiSub","text":"Using function subsample big data Poisson regression one model describe data. Subsampling probabilities obtained based - L- optimality criterions.","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model robust optimal subsampling for A- and L- optimality criterion under Poisson regression — modelRobustPoiSub","text":"","code":"modelRobustPoiSub(r1,r2,Y,X,N,Alpha,All_Combinations,All_Covariates)"},{"path":"/reference/modelRobustPoiSub.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model robust optimal subsampling for A- and L- optimality criterion under Poisson regression — modelRobustPoiSub","text":"r1 subsample size initial random sampling r2 subsample size optimal sampling Y response data Y X covariate data X matrix covariates (first column intercept) N size big data Alpha vector alpha values used obtain model robust subsampling probabilities All_Combinations list possible models can describe data All_Covariates covariates models","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model robust optimal subsampling for A- and L- optimality criterion under Poisson regression — modelRobustPoiSub","text":"output modelRobustLinSub gives list Beta_Data estimated model parameters model list subsampling Utility_Data estimated Variance Information model parameters subsampling Sample_L-optimality list indexes initial optimal subsamples obtained based L-optimality criterion Sample_L-optimality_MR list indexes initial model robust optimal subsamples obtained based L-optimality criterion Sample_A-optimality list indexes initial optimal subsamples obtained based -optimality criterion Sample_A-optimality_MR list indexes initial model robust optimal subsamples obtained based -optimality criterion Subsampling_Probability matrix calculated subsampling probabilities - L- optimality criterion","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model robust optimal subsampling for A- and L- optimality criterion under Poisson regression — modelRobustPoiSub","text":"Two stage subsampling algorithm big data Poisson regression multiple models can describe big data. First stage obtain random sample size \\(r_1\\) estimate model parameters models. Using estimated parameters subsampling probabilities evaluated -, L-optimality criterion model averaging -, L-optimality subsampling methods. estimated subsampling probabilities subsample size \\(r_2 \\ge r_1\\) obtained. Finally, two samples combined model parameters estimated models. NOTE :  input parameters given domain conditions necessary error messages provided go . \\(r_2 \\ge r_1\\) satisfied error message produced. big data \\(X,Y\\) missing values error message produced. big data size \\(N\\) compared sizes \\(X,Y\\) aligned error message produced. \\(0 < \\alpha < 1\\) priori probabilities satisfied error message produced.","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Model robust optimal subsampling for A- and L- optimality criterion under Poisson regression — modelRobustPoiSub","text":"Mahendran , Thompson H, McGree JM (2023). “model robust subsampling approach Generalised Linear Models big data settings.” Statistical Papers, 64(4), 1137--1157.","code":""},{"path":"/reference/modelRobustPoiSub.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model robust optimal subsampling for A- and L- optimality criterion under Poisson regression — modelRobustPoiSub","text":"","code":"Dist<-\"Normal\"; No_Of_Var<-2; Beta<-c(-1,2,1,2); N<-10000 All_Models<-list(Real_Model=c(\"X0\",\"X1\",\"X2\",\"X1^2\"),                  Assumed_Model_1=c(\"X0\",\"X1\",\"X2\"),                  Assumed_Model_2=c(\"X0\",\"X1\",\"X2\",\"X2^2\"),                  Assumed_Model_3=c(\"X0\",\"X1\",\"X2\",\"X1^2\",\"X2^2\")) family = \"poisson\"  Full_Data<-GenModelRobustGLMdata(Dist,Dist_Par=NULL,No_Of_Var,Beta,N,All_Models,family)  r1<-300; r2<-rep(100*c(6,9,12),5); Original_Data<-Full_Data$Complete_Data;  modelRobustPoiSub(r1 = r1, r2 = r2,                   Y = as.matrix(Original_Data[,colnames(Original_Data) %in% c(\"Y\")]),                   X = as.matrix(Original_Data[,-1]),N = nrow(Original_Data),                   Alpha = rep(1/length(All_Models),length(All_Models)),                   All_Combinations = All_Models,                   All_Covariates = colnames(Original_Data)[-1])->Results #> Step 1 of the algorithm completed. #> Step 2 of the algorithm completed.  Beta_Plots<-plot_Beta(Results) Utility_Plots<-plot_Utility(Results)"},{"path":"/reference/plot_Beta.html","id":null,"dir":"Reference","previous_headings":"","what":"Plotting model parameter (Beta) outputs after subsampling — plot_Beta","title":"Plotting model parameter (Beta) outputs after subsampling — plot_Beta","text":"using subsampling methods mostly obtain estimated model parameter estimates. , summarised histogram plots.","code":""},{"path":"/reference/plot_Beta.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plotting model parameter (Beta) outputs after subsampling — plot_Beta","text":"","code":"plot_Beta(object)"},{"path":"/reference/plot_Beta.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plotting model parameter (Beta) outputs after subsampling — plot_Beta","text":"object object subsampling subsampling functions","code":""},{"path":"/reference/plot_Beta.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plotting model parameter (Beta) outputs after subsampling — plot_Beta","text":"output faceted ggplot result","code":""},{"path":"/reference/plot_Beta.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plotting model parameter (Beta) outputs after subsampling — plot_Beta","text":"local case control sampling facets subsample sizes beta values. leverage sampling facets subsample sizes beta values. - L-optimality criterion subsampling Generalised Linear Models facets subsample sizes beta values. -optimality criterion subsampling Gaussian Linear Models facets subsample sizes beta values. -optimality criterion subsampling Generalised Linear Models response variable inclusive facets subsample sizes beta values. - L-optimality criterion subsampling Generalised Linear Models multiple models can describe data facets subsample sizes beta values. - L-optimality criterion LmAMSE subsampling Generalised Linear Models potential model misspecification facets subsample sizes beta values.","code":""},{"path":"/reference/plot_LmAMSE.html","id":null,"dir":"Reference","previous_headings":"","what":"Plotting loss function or mAMSE outputs for the subsamples under model misspecification — plot_LmAMSE","title":"Plotting loss function or mAMSE outputs for the subsamples under model misspecification — plot_LmAMSE","text":"using subsampling methods potential model misspecification obtain respective loss mAMSE values. summarised plots .","code":""},{"path":"/reference/plot_LmAMSE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plotting loss function or mAMSE outputs for the subsamples under model misspecification — plot_LmAMSE","text":"","code":"plot_LmAMSE(object)"},{"path":"/reference/plot_LmAMSE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plotting loss function or mAMSE outputs for the subsamples under model misspecification — plot_LmAMSE","text":"object object subsampling subsampling function potential model misspecification","code":""},{"path":"/reference/plot_LmAMSE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plotting loss function or mAMSE outputs for the subsamples under model misspecification — plot_LmAMSE","text":"output faceted ggplot result","code":""},{"path":"/reference/plot_LmAMSE.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plotting loss function or mAMSE outputs for the subsamples under model misspecification — plot_LmAMSE","text":"- L-optimality criterion LmAMSE subsampling Generalised Linear Models potential model misspecification facets variance bias^2 mAMSE values.","code":""},{"path":"/reference/plot_Utility.html","id":null,"dir":"Reference","previous_headings":"","what":"Plotting A- and D-optimality for model parameter (Beta) outputs of the subsamples — plot_Utility","title":"Plotting A- and D-optimality for model parameter (Beta) outputs of the subsamples — plot_Utility","text":"using subsampling methods mostly obtain estimated model parameter estimates. estimates can obtain respective - D-optimality values summarised plots . noted - D-optimality represents model parameter's tr(Variance) log(det(Information)), respectively.","code":""},{"path":"/reference/plot_Utility.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plotting A- and D-optimality for model parameter (Beta) outputs of the subsamples — plot_Utility","text":"","code":"plot_Utility(object)"},{"path":"/reference/plot_Utility.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plotting A- and D-optimality for model parameter (Beta) outputs of the subsamples — plot_Utility","text":"object object subsampling subsampling functions","code":""},{"path":"/reference/plot_Utility.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plotting A- and D-optimality for model parameter (Beta) outputs of the subsamples — plot_Utility","text":"output faceted ggplot result","code":""},{"path":"/reference/plot_Utility.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plotting A- and D-optimality for model parameter (Beta) outputs of the subsamples — plot_Utility","text":"- L-optimality criterion subsampling Generalised Linear Models facets model parameter variance information. - L-optimality criterion subsampling Generalised Linear Models multiple models can describe data facets model parameter variance information.","code":""},{"path":"/reference/Skin_segmentation.html","id":null,"dir":"Reference","previous_headings":"","what":"Skin segmentation data — Skin_segmentation","title":"Skin segmentation data — Skin_segmentation","text":"Rajen Abhinav (2012) addressed challenge detecting skin-like regions images component intricate process facial recognition. achieve goal, curated “Skin segmentation” data set, comprising RGB (R-red, G-green, B-blue) values randomly selected pixels N = 245,057 facial images, including 50,859 skin samples 194,198 nonskin samples, spanning diverse age groups, racial backgrounds, genders.","code":""},{"path":"/reference/Skin_segmentation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Skin segmentation data — Skin_segmentation","text":"","code":"Skin_segmentation"},{"path":"/reference/Skin_segmentation.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Skin segmentation data — Skin_segmentation","text":"data frame 4 columns 245,057 rows. Skin_presence Skin presence randomly selected pixels Red Red values randomly selected pixels Green Green values randomly selected pixels Blue Blue values randomly selected pixels","code":""},{"path":"/reference/Skin_segmentation.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Skin segmentation data — Skin_segmentation","text":"Extracted Rajen B, Abhinav D (2012) Skin segmentation. UCI Machine Learning Repository. Available : doi:10.24432/C5T30C","code":""},{"path":"/reference/Skin_segmentation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Skin segmentation data — Skin_segmentation","text":"","code":"nrow(Skin_segmentation) #> [1] 245057"},{"path":"/news/index.html","id":"needs4bigdata-100","dir":"Changelog","previous_headings":"","what":"NeEDS4BigData 1.0.0","title":"NeEDS4BigData 1.0.0","text":"Initial CRAN submission.","code":""}]
